# Statistics in Ecology

As ecologists, we often are interested in testing hypotheses about some ecological relationship, prediction, or theory. Generally, our hypotheses may relate to some wild population, large ecosystem, or a broad-scale relationship (e.g. a predicted relationship from a theory; like allometric scaling or survival under different conditions, etc). However, it is typically impossible to survey the entire group of interest. In statistics, we define the entire group of interest as our "population" or the "population-level". Note that this may be slightly different than the ecological definition of population as a group of same-species organisms in an ecosystem. For the purposes of this article, I'll use "population" in the statistical sense.

Let's say we were interested in whether or not campus squirrels were fatter than squirrels from Harbison forest. To test this prediction, we'd need to collect some campus squirrels and some forest squirrels, get a body-fat composition analysis and compare them. Well, in this example, our "population-level" interest is all the squirrels in the forest and all the squirrels on campus. What specifically we are interested in is the difference between mean campus body fat and mean forest body fat. In statistical notation, this would be written as: $\mu_{c} - \mu_{f}$. Where c and f denote campus and forest. The term $\mu$ is common for population-level mean. Any metric of interest at the population-level, we can call a ***parameter.*** However, it would be impossible to capture every single squirrel. So we can't actually measure our parameter of interest. Instead we have to take a ***sample*** from the population. Let's say we collect 5 squirrels each from the forest and campus. Well now we could calculate the mean of our sample. The metric calculated at the sample-level, is referred to as a statistic. In our case that would be the sample mean of campus and forest squirrels (in statistics notation this is either $\bar{x}_c - \bar{x}_f$ or $\hat{\mu}_c - \hat{\mu}_f$). Well, how can we know if our statistics are actually representative of the true population parameter of interest? That is where inferential statistical tests enter the picture. At the most broad level, statistical analyses give us the ability to discuss how confident we are that our samples are representative of reality (the population-level parameters).

![](media/gen_stats-inferential-diagram.png)

In this document, I'll briefly summarize some basics of data types, how to plot them, and a basic tool kit for statistically testing your data. This is by no means an exhaustive list, but hopefully a good starting point. Additionally, in this article I'll try to provide a little bit more detail for those who are interested. Don't feel the need to deeply understand everything. However, please read through the whole article. A common issue in applying statistics to ecological data, is that ecologists fall into traps of how to correctly understand and interpret the results of a test. To avoid these traps, we need to understand the basics of common approaches, then why they might be misleading when interpreting them.

Like most things, learning how and when to apply a given test, comes with experience. So I hope you are able to refer back while working on independent projects.

Let's recap some basic terms:

Population - in a statistical sense, this is the entire group of interest (could be multiple, or less, biological populations)

Parameter - a metric which describes a feature at the population level

Sample - a sub-group of the population

Statistic - a metric describing the sample-level, which can be used to make inferences about the population.

# Types of Data

There are several types of data which we will encounter while processing ecological data. The type of data that we collect determines what types of statistical tests we may be able to use with our data.

Here's some of the basic types of data we may encounter:

**\
1. Categorical Data -** represent distinct categories or groups. These categories are often non-numeric and do not have a natural order. When discussing categorical data, we might also specify the number of levels it has.

-   **EX: Habitat Types**: Categorizing different habitat types, such as forest, grassland, or wetland. In that case, we might say there are three levels.

**2. Continuous Data -** take any numeric value. They are measured on a continuous scale and can have decimal values. Note that we may take count data and make it continuous by surveying a fixed area.

-   **Ex:**

    -   **Temperature**: Recording temperature in degrees Celsius or Fahrenheit.

    -   **Height of Trees**: Measuring tree heights in meters or centimeters.

    -   **Density of snails:** We might count the number of snails per square meter. This would be continuous

**3. Count Data -** the number of occurrences or items in a fixed area, time, or sample. Note that this is often converted to a continuous variable if standardizing across space or time

-   **Ex:**

    -   **Bird Counts**: Counting the number of birds in a specific area during a bird survey.

    -   **Insect Abundance**: Counting the number of insects on a plant.

**4. Ordinal Data** - categorical data with a natural order or ranking among categories. However, the intervals between categories may not be uniform.

-   **Ex:**

    -   **Behavioral Rankings**: Ranking animal behavior from "aggressive" to "docile."

    -   **Vegetation Health**: Assessing vegetation health as "poor," "fair," or "excellent."

**5. Time Series Data -** data are collected over successive time intervals. They are used to analyze trends and changes over time. These data are often continuous (although could be categorical time series). However, there as several specialized approaches to dealing with such data.

-   **Ex**:

    -   **Stream Flow**: Measuring daily stream flow rates over a year.

    -   **Climate Data**: Recording monthly temperature and precipitation data over several decades.

**6. Multivariate Data -** multiple variables measured for each sample or observation. They can include combinations of categorical, continuous, or count data.

-   **Ex:**

    -   **Community Composition**: Analyzing species composition in ecological communities with multiple species present.

    -   **Ecosystem Characteristics**: Measuring various ecosystem attributes, such as biodiversity, soil nutrients, and plant biomass, in a study area.

# Types of Variables

When analyzing ecological data, we typically are interested in relating how two different sets of data are related (or different).

Predictive variables - these are sometimes referred to as independent or explanatory variables. This is the data which you are using to explain a phenomena

Response variables - these are at times referred to as dependent variables. This is the data which is the phenonena/metric of interest.

Let's take the squirrel example above. In that case, our explanatory variable would be location (2 possible levels: campus or forest). Our response variable would be squirrel body fat composition. These would be categorical and continuous data respectively.

# Plotting Data

Plots should always offer a nice visual representation of your data. They should have some level of summary or trends visible through the image. Always make sure that your figure caption and axes titles

See this [example guide](./gen_R-guide-plotting.html)for a reference on what plots work well for what data types.

# The basic idea behind most analyses

Ecologists have relied on a suite of statistical tools to make inferences about their data. Traditionally, frequentist methods have dominated the landscape. At a broad level, this category of approach assumes that there is a fixed parameter among a population of interest. Then our sample, ideally unbiased and random, can be used to make inferences about the population, as discussed above.

At this point, you've likely implemented a variety of statistical tests. However, let's review some basic ideas behind these analyses.

### Null hypothesis significance testing

Many common approaches to analyzing data rely on the idea of null-hypothesis significance testing. These approaches set a null and an alternative hypothesis, then test how likely our data supports the ability to "reject" the null hypothesis. The null hypothesis, in most cases, is the base assumption that there is no effect, no difference, or no relationship (depending on the data being analyzed). Alternatively, the alternative hypothesis is that there is some effect, difference, or relationship. Effectively, what most analyses do is account for how large the sample size is, how big the sample variation is, then evaluate its ability to determine a conclusion about the population-level.

There is a lot of jargon in that, and it might be confusing when speaking so broadly. So let's take a look in our squirrel example:

We are interested in the body-fat composition of two populations of squirrels. Now we collected some sample of squirrels from each location and we want to compare them. In this case we'd probably use a t-test. The null hypothesis would be that there is no significant difference between our two squirrel populations ($\mu_c - \mu_f = 0$). The alternative hypothesis is that there is a significant difference in mean body fat between the two squirrels ( $\mu_c - \mu_f \neq 0$ ) .

We then would put all our sample data into a t-test to evaluate whether or not to "reject" our hypothesis. or in other words: if there is a significant difference.

### P-values

The p-value is the metric used to determine whether or not to reject a null hypothesis. There's a number of ways which I've heard the p-value explained. Which I'll share these explanations from what I think is the more technical to the more intuitive:

-   The proportion of times if you would collect data with that sample statistic, over repeated trails, if the population parameter was 0 (or another set value).

-   The probability of capturing a sample statistic which is different significantly different from the true population parameter.

-   The likelihood of rejecting the null hypothesis if it is actually true

-   The probability of you collecting the data you did, if the observed statistic (value, difference, slope, etc) was not real. For context: if you collected squirrels from the forest and campus and measured their body fat and found the difference in sample means was 100g (i'm guessing that would be a large number for a squirrel). If your p-value from a t-test comparing those groups was 0.001, that means the chance you would have observed a 100g difference or greater, if the true population difference was 0, is 0.1%.

All those explanations above say exactly the same thing, just in different ways. Regardless of which mantra speaks to you, the p-value is utilized to determine whether or not to (1) reject the null hypothesis and (2) state that your observed difference/effect is statistically significant. To make that determination, we compare the p-value to the alpha-value (p-value threshold).

Generally, p-value thresholds (referred to as the alpha value) are set at 0.05 (sometimes 0.01). All this means is that researchers are looking for a p-value smaller than that threshold to determine if their effect/difference is "significant". This could mean significantly different from another group or that there is a significant effect (different from 0 effect) depending on the specific analysis. The threshold is set to minimize our chance of making a type-I error.

-   Type-I error is essentially a false positive decision

-   Type-II error is essentially a false negative.

Which error is more important is a context-dependent situation. Yet generally statistical analyses are set-up to avoid false positives.

### Effect size:

A major pitfall with the common approaches of statistics is that researchers get hyper-focused on p-values. They seek to get a result with a low p-value otherwise they think their data are worthless. Yet, we are smarter than that. We shouldn't get trapped in the dogmatic perspective that this imaginary 0.05 threshold is all we can use to evaluate our data.

The effect size is really what we are interested in as researchers. Effect size varies by analysis but it is typically the value we are interested in. This could mean the average difference in a value between two groups, or it could be the strength of the relationship that one variable has on another.

For example, if I'm interested in the difference in if student grades in my course are significantly different based on major. I could compare bio-majors to non-bio majors and let's say I got a significant differences (i.e. my t-test p-value would be \< 0.05). However, what if that average difference was just 1%? That's not a very meaningful difference. Alternatively

The p-value is just a tool from the statistical test to evaluate how much we should believe our data.

# Common Statistical Tests & Analyses

+----------------------------------------+-----------------------------------+----------------------------------------------------------------------------------------------------------------------+
| **Predictive Variable (Independent)**  | **Response Variable (Dependent)** | **Appropriate Data Analysis / Statistical Test**                                                                     |
+========================================+===================================+======================================================================================================================+
| **Categorical (2 levels)**             | **Numerical**                     | t-test if normally distributed, Mann-Whitney U if not                                                                |
+----------------------------------------+-----------------------------------+----------------------------------------------------------------------------------------------------------------------+
| **Categorical (\>2 levels)**           | **Numerical**                     | ANOVA (linear-model extention) if assumptions met,                                                                   |
|                                        |                                   |                                                                                                                      |
|                                        |                                   | Kruskal-Wallis test if not                                                                                           |
|                                        |                                   |                                                                                                                      |
|                                        |                                   | Various post-hoc tests                                                                                               |
+----------------------------------------+-----------------------------------+----------------------------------------------------------------------------------------------------------------------+
|                                        | **Categorical**                   | Chi-Square Test                                                                                                      |
+----------------------------------------+-----------------------------------+----------------------------------------------------------------------------------------------------------------------+
| **Numerical**                          | **Categorical**                   | Logistic Regression (for binary response) or Multinomial Logistic Regression (for multiple categories)               |
+----------------------------------------+-----------------------------------+----------------------------------------------------------------------------------------------------------------------+
|                                        | **Numerical**                     | Linear Regression (for linear relationship) or Nonlinear Regression (for nonlinear relationship)                     |
+----------------------------------------+-----------------------------------+----------------------------------------------------------------------------------------------------------------------+
| **Time Series**                        | **Numerical**                     | Time Series Analysis (e.g., ARIMA, Exponential Smoothing)                                                            |
+----------------------------------------+-----------------------------------+----------------------------------------------------------------------------------------------------------------------+
| **Categorical**                        | **Time to Event**                 | Survival Analysis (e.g., Kaplan-Meier, Cox Proportional-Hazards Regression)                                          |
+----------------------------------------+-----------------------------------+----------------------------------------------------------------------------------------------------------------------+
| **Continuous**                         | **Continuous**                    | Correlation (pearson's for linear, Spearmans for non-linear)                                                         |
+----------------------------------------+-----------------------------------+----------------------------------------------------------------------------------------------------------------------+
| **Multivariate (Multiple Predictors)** | **Numerical**                     | Multiple Linear Regression (for linear relationships) or Multiple Nonlinear Regression (for nonlinear relationships) |
+----------------------------------------+-----------------------------------+----------------------------------------------------------------------------------------------------------------------+
|                                        | **Categorical**                   | Multinomial Logistic Regression                                                                                      |
+----------------------------------------+-----------------------------------+----------------------------------------------------------------------------------------------------------------------+
| **Ordinal**                            | **Ordinal**                       | Ordinal Regression (e.g., Proportional Odds Model)                                                                   |
+----------------------------------------+-----------------------------------+----------------------------------------------------------------------------------------------------------------------+
| **Count**                              | **Count**                         | Poisson Regression (for count data) or Negative Binomial Regression (for overdispersed count data)                   |
+----------------------------------------+-----------------------------------+----------------------------------------------------------------------------------------------------------------------+

## Caveats with common approaches:

Many statistical tests have a set of assumptions. You should evaluate those assumptions prior to applying a test. Plenty of papers get published without evaluation of the assumptions yet it is best to know thel limitations of different data analysis methods.

# FAQ:

There are several things which may get confusing when reading about ecological data analysis.

### Wait, is it significant?

When writing scientifically and discussing data, you need to be very careful about using the term "significant". Typically this is reserved for things which are statistically significant (i.e. there was a sufficiently low p-value from some test). When stating something is statistically significant in your results you should include the p-value and test in parentheses. However, to determine if something is meaningful, you need to consider the effect size as well.

There's a growing movement to reject the p-value dogma in data analysis. I think it is fine as is, yet it can be misleading for uninformed readers who put too much weight in p-values. Personally, in my work I'm trying to increase the use of confidence intervals (rather than p-values) and using terminology like "a clear difference" rather than a "significant difference".

More reading for those interested:

-   [link 1](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3444174/)

-   [link 2](https://www.vox.com/2016/3/15/11225162/p-value-simple-definition-hacking)

-   [link 3](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13159)

### What is a model, what is a test?

Often times, these terms are used loosely and interchangeably. However there is a subtle difference. Statistical tests are some analysis which evaluates how well a sample statistic matches a supposed population parameter. A model is a broad term of a representation of something else. A statistical model is a description of real world phenomena using some algorithm to optimize the fit of the model to observed data. For example, a simple linear regression is a model (usually fit with the ordinary least squares method). Models often have an associated statistical test which allows us to evaluate if the model statistics are good parameter estimators.

### All these hypotheses are confusing me. What is what?

When learning statistics, you are likely introduced to the term "null" and "alternative" hypotheses. These are artifacts of the most common school of statistical thought, Frequentist Statistics, or Null Hypothesis Significance Testing (NHST). At it's most basic, NHST is comparing whether or not the data we observed is significantly different than 0 or no-effect.

Frequentist statistical tests have a null and alternative hypothesis. These are just predictions where the null is generally no-effect and the alternative is there is some effect.

However in biology, we have hypotheses as a much more detailed statement. The biological hypothesis is a proposed mechanism with associated predictions. The biology-based hypothesis can be evaluated based on decisions made with the statistical test. However, when writing your lab reports, l encourage you use hypothesis to refer to the biology-hypothesis not the statistical ones.

# Emerging approaches

Multivariate models are increasingly common in ecology. Often times these are used for preliminary data analysis but there is a wealth of useful tools which are worth investigating.

Machine learning methods are growing in popularity as well. The exact term "machine learning" can be broadly interpretted depending on how flashy authors might want their titles to be. However, at a basic level, OLS regression is technically a machine learning approach. However, there are some increasingly popular tools, namely regression trees and various classifiers. The fundamental idea behind ML approaches is that they are more prediction oriented rather than traditional statistics approaches which are inference oriented. The applicability of each method depends on the context.

Bayesian statistics are an alternative school of thought to frequentist statistics. Rather than rigid testing of fixed hypotheses, Bayesian statistics takes a more probabilistic approach in which a researcher can evaluate the probability of their hypothesis being true given the data they've observed and any prior knowledge. This is really a more intuitive way to think about data analysis yet it is a little more mathematically complicated. However, it is growing in popularity.
