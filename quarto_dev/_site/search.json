[
  {
    "objectID": "addtl_resources.html",
    "href": "addtl_resources.html",
    "title": "Additional Resources",
    "section": "",
    "text": "Learning R:\n\nHadley Wickham is the developer behind Posit, Tidyverse, ggplot, and many of the modern R applications. He has several books which cover R. While there are several books which teach R, his books are by far the most comprehensive. Furthermore, these resources are the forefront of good practice and standards.\n\nr4ds\nAdvanced R\n\nI came across this book Google image searching for the basic RStudio interface to see what it looks like by default. It seems to be good for setting up:\n\nintro2r\n\nA great resource for making graphs is the R graph gallery:\n\nR Graph Gallery\n\n\n\n\nAdvanced Concepts\n\nGit and version control are great ways to organize computational projects. This is increasingly a useful skill to understand:\n\nHappy Git with R\n\nIf you want to learn more about organizing files in a meaningful way, I found this workshop to be extremely helpful:\n\nR for Reproducible Research\n\nThis website was made with quarto, a new markdown format and interactive programming environment. Quarto offers a great way to make pretty documents and presentations with incorporate code:\n\nQuarto"
  },
  {
    "objectID": "code_lab-00.html",
    "href": "code_lab-00.html",
    "title": "Introductory Lab Guide",
    "section": "",
    "text": "When starting a new analysis in R, it is best to create a new R script. You can save this script in your course folder. At the start of each script, it is good form to load any needed packages. In this exercise we’ll need ggplot. If you haven’t already installed ggplot2, enter into the console install.packages(\"ggplot2\").\nAt the top of your script, load the package:\n\nlibrary(ggplot2)"
  },
  {
    "objectID": "code_lab-00.html#reading-the-data-into-r.",
    "href": "code_lab-00.html#reading-the-data-into-r.",
    "title": "Introductory Lab Guide",
    "section": "Reading the data into R.",
    "text": "Reading the data into R.\nIf you downloaded the data to your folder, you can read it in with read.csv(). First make sure your working directory is focused on your file.\n\nsetwd('~/BIOL570L_Docs') # set this to your path in your computer's folder\ncrab_width <- read.csv('LTER_CrabCarapaces.csv')"
  },
  {
    "objectID": "code_lab-00.html#base-r-option",
    "href": "code_lab-00.html#base-r-option",
    "title": "Introductory Lab Guide",
    "section": "Base R Option:",
    "text": "Base R Option:\nFirst, I’ll create individual variables for each mean value and sd value by site:\n\nmean_BC <- mean(crab_width$carapace_width[which(crab_width$Site == 'BC')]) # take mean of BC\nsd_BC <- sd(crab_width$carapace_width[which(crab_width$Site == 'BC')]) # take sd of BC\n\n# same for GTM\nmean_GTM <- mean(crab_width$carapace_width[which(crab_width$Site == 'GTM')])\nsd_GTM <- sd(crab_width$carapace_width[which(crab_width$Site == 'GTM')])\n\nWe can look at those values individually:\n\nprint(mean_GTM)\n\n[1] 12.40321\n\n\nThen we’ll need to pull all those values into a data frame\n\ncrab_means <- data.frame(Site = c('BC','GTM'),\n                        mean = c(mean_BC, mean_GTM),\n                        sd = c(sd_BC, sd_GTM))\n\nWe now can take a look at those values\n\ncrab_means\n\n  Site     mean       sd\n1   BC 16.19730 4.814464\n2  GTM 12.40321 1.804449"
  },
  {
    "objectID": "code_lab-00.html#dplyr-option",
    "href": "code_lab-00.html#dplyr-option",
    "title": "Introductory Lab Guide",
    "section": "dplyr option:",
    "text": "dplyr option:\nThis is the cleaner approach but it does abstract a few steps away. First you want to make sure you have installed dplyr. Also you should load the library (do this at the top of the script).\n\nlibrary(dplyr)\n\nThe tidyverse syntax is really big on piping code together. So we’ll use a number of functions here and chain them all together. Piping takes the value on the left of the pipe operator (|>) and pushes it to the first argument of the function on the right. In tidyverse/dplyr functions, the first argument is often a data.frame. This makes it easy to chain together these functions. In this code, I will chain it all together, but if you want to learn more, you can run one pipe at a time and see what happens in each step.\n\ncrab_means <- crab_width |> \n  filter(Site %in% c('BC', 'GTM')) |> #filter only to these sites\n  group_by(Site) |> #group by site\n  summarize(mean = mean(carapace_width),\n            sd = sd(carapace_width)) # apply mean and sd functions\n\nNow we can look at the data. Note that tidyverse functions will create tibbles rather than data.frames. For most purposes this is a very minor detail that will not matter until you are working on high-level problems or developing software in R.\n\ncrab_means\n\n# A tibble: 2 × 3\n  Site   mean    sd\n  <chr> <dbl> <dbl>\n1 BC     16.2  4.81\n2 GTM    12.4  1.80"
  },
  {
    "objectID": "code_lab-01.html",
    "href": "code_lab-01.html",
    "title": "Lab 1 Code",
    "section": "",
    "text": "The first major challenge (and likely only one) will be organizing your data so that it works well with the provided code. Take the data and format it in an excel sheet. For this analysis, you should total the time spent on each action and put that as rows. You should also count the number of times each action occurred.\nHere’s how my data look. Note that I made this prior to lab so I am using imaginary data:\n\nIf you want to use the code provided, you must exactly match the layout that I used here. While your data might be different R is very picky about a few things. Here’s some potential issues students might run into:\n\nMake sure column names match exactly. R is case sensitive\nGenerally spaces are challenges in character vectors so use a _ instead.\nWhile our data collection sheet on paper had merged cells for behavior, in R, they must be individually represented in each row. So make sure you don’t format the excel sheet in a fancy way\nMake sure to save the files as a .csv file. This is not the excel default. You must select ‘save as’.\nSave the file as a name you can find and remember!"
  },
  {
    "objectID": "code_lab-01.html#check-out-the-data",
    "href": "code_lab-01.html#check-out-the-data",
    "title": "Lab 1 Code",
    "section": "Check out the data",
    "text": "Check out the data\nIf you succesfully loaded the data you should take a look at it to make sure the layout is how you want.\n\nhead(sqdf)\n\n   category           action num_events total_mins\n1  conflict          chasing         19         37\n2  conflict running_squirrel         24         35\n3  conflict         taunting         10          3\n4 avoidance    running_other          4          2\n5 avoidance running_predator          1          1\n6    forage        searching         13         45\n\n\nNote that here I used head(). But in reality, I typically use the View() function to take a peek at data.\nSome possible issues at this step would be that your column headers are wrong or your num_events column or total_mins column are characters (they should be ‘int’). If this is the case, there is something wrong with how you formatted your excel sheet."
  },
  {
    "objectID": "code_lab-01.html#figures",
    "href": "code_lab-01.html#figures",
    "title": "Lab 1 Code",
    "section": "Figures",
    "text": "Figures\nThe original approach can be improved on in a few ways. First, let’s think about that figure. While it was easy to compile the data by behavior category, we smoothed over the details we recorded for individual actions. Below is code to make a similar figure but with a little more detail.\nThis one is a bit trickier since I have to specify the colors I want to use. I’m still making bars by behavior category but now I’m filling the colors with stacked actions. Here, I specify some colors for each action:\n\ncolors = c(\n  `chasing` = '#D81B60',\n  `running_squirrel` = '#C75780',\n  `taunting` = \"#BF7993\",\n  `running_other` = \"#1E88E5\",\n  `running_predator` = \"#66A8E2\",\n  `searching` = \"#FFC107\",\n  `collecting_food` = \"#FDD458\",\n  `storing_food` = \"#FFE597\",\n  `jumping` = \"#FFEFBF\",\n  `eating` = '#004D40',\n  `resting` = '#4D7F77'\n)\n\nNow I can make the same figure but with this enhanced detail. Note that for making your figures, your actions and behavior categories will be different than this example dataset. You’ll need to modify the above color scale for different categories. In R, you can make a different color either with names ('black'). Or you could use HEX codes like I did. Just google any color pallete mixer. As a note, it’s often beneficial to consider colorblind friendly palettes to make your figures accessible to everyone!\n\nggplot(sqdf) + \n  geom_bar(aes(x = category, y = total_mins,\n               fill = action),\n           stat = 'identity', position = 'stack') +\n  scale_fill_manual(values = colors) +\n  labs(x = 'Behavior Category', y = 'Total Minutes', fill = 'Action')+\n  theme_classic()\n\n\n\n\n\n\nTime spent during different behavior categories of Eastern Grey Squirrels observed on USC’s Horshoe. Actions corresponding to each behavior category are shown.\n\n\n\n\nWhile this figure isn’t perfect, I’d probably want to change the colors a bit more. It provides a good idea of how to improve this figure. Another idea, is that while time spent on a category is a great metric, we might also be interested in the number of events that each item happened.\n\nggplot(sqdf) + \n  geom_bar(aes(x = category, y = num_events,\n               fill = action),\n           stat = 'identity', position = 'stack') +\n  scale_fill_manual(values = colors)+\n  labs(x = 'Behavior Category', y = 'Number of Events', fill = 'Action')+\n  theme_classic()\n\n\n\n\n\n\nNumber of events for different actions of squirrels observed on USC’s Horseshoe. Actions are shown grouped by major behavior category.\n\n\n\n\nThis figure provides a different perspective than the first one. We can see that avoidance events occur more often than previously suggested while rest events are less common.\nWe could also look at the average duration of each event:\n\nggplot(sqdf) +\n  geom_bar(aes(x = total_mins/num_events, y = action, fill = category),\n           stat = 'identity')+\n  labs(x = \"Average Duration of Action [mins]\", y = \"Action\", fill = \"Behavior Category\")+\n  theme_classic()\n\n\n\n\n\n\nAverage action direction of Eastern Grey squirrels observed on USC’s Horseshoe campus.\n\n\n\n\nAs you can see, even with a fairly rudimentary dataset, we can display the information in a number of ways. Making a variety of plots can be extremely useful. First, as a researcher this is a critical step of exploratory data analysis (EDA). EDA allows us to notice major trends in our data, sometimes surprising ourselves. Additionally, we can try to think about what is the best way to communicate our findings. We want our figures to be clear to a reader who has no familiarity with our research project. The captions should be brief yet informative. When making your figures, ask your self: “What is the main message I want a reader to take away from this figure?” and then you can think about how effectively you communicate that message through the figure."
  },
  {
    "objectID": "code_lab-01.html#other-data-analyses",
    "href": "code_lab-01.html#other-data-analyses",
    "title": "Lab 1 Code",
    "section": "Other data analyses",
    "text": "Other data analyses\nIn our initial analysis, we simply tested if the distribution of time allocated on one behavior category was statistically significantly different from a uniform time allocation. However, that isn’t the best test of our original hypothesis. The project plan hypothesis suggested that conflict would receive more time allocation than other categories. So let’s run the analysis but with a different “expected” distribution for our squirrel’s time allotment. In this case I’ll define a vector of an expected_model which has the proportions of time spent in my different behavior categories.\nHere, I’m going to make an expected model to match my hypothsis, where a squirrel spends 5% of its time in avoidance, 40% in confilct, 30% foraging, and 25% resting.\nTo make these proportions, I’m matching the order to the order of my observed categories:\n\nsq_summary$category\n\n[1] \"avoidance\" \"conflict\"  \"forage\"    \"rest\"     \n\n\nYou’ll have to create your own expected model to match your unique categories!\n\n# let's say you are interested in comparing \n# for unique categories\n\n# total_observation_time <- sum(sqdf$total_mins) #total up all time observed\nexpected_proportions <- c(0.05,0.4,0.3,0.25)\n\nchisq.test(x = sq_summary$total_time, p = expected_proportions)\n\n\n    Chi-squared test for given probabilities\n\ndata:  sq_summary$total_time\nX-squared = 4.2917, df = 3, p-value = 0.2316\n\n\nFrom this result, we have a p-value > 0.05. This would lead me to write a statement similar to this as a result:\n“There was no significant difference between the time allocation of our observed squirrels and the expected allocations under a conflict-heavy time allocation (chi-squared test, p-value = 0.23).”\nClearly this is fabricated data but it gives a great idea of how we could extend this analysis."
  },
  {
    "objectID": "code_lab-02.html",
    "href": "code_lab-02.html",
    "title": "Lab 1 Code",
    "section": "",
    "text": "Setting up R\nThis week we’ll use three packages: ggplot2, dplyr, and dunn.test. You might not yet have dunn.test installed yet. Remember, if you need to install a package, you only need to do it once. So in your console (the main bottom tab of RStudio) run:\n\ninstall.packages('dunn.test')\n\nHowever, each time we run R, we’ll need to load all our packages\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(dunn.test)\n\n\n\nAccessing the data\nYou can download the data from the course github org: link\nDownload and save the data in your files/folder\nThen load it into R. I’m going to name it raw_data\n\nsetwd('C:/Users/abart/OneDrive/Documents/UofSC/Classes/BIOL570L')\nraw_data <- read.csv('2023_WS2_Data-Share.csv') # whatever you named i\n\n\n\n\nNow let’s take a look and make sure the structure is as we expect:\n\nstr(raw_data)\n\n'data.frame':   146 obs. of  4 variables:\n $ Region : chr  \"High-Disturbance\" \"High-Disturbance\" \"High-Disturbance\" \"High-Disturbance\" ...\n $ Quadrat: int  1 1 1 1 2 2 2 3 3 3 ...\n $ Taxa   : chr  \"A\" \"B\" \"C\" \"D\" ...\n $ Count  : int  1 2 7 2 2 9 4 2 3 7 ...\n\n\nLook’s good to me!\n\n\nAnalyzing the data\nThis week the question is somewhat straightforward but the code will get quite complex. This guide walks through some of the process of building out the code. If you just want to get to plots and core statistical analyses, you don’t need all this code. I will clearly distinguish between the required and the “bonus” portions. However, I encourage you to skim through all sections as it will inform the process.\nThe main object is to test the intermediate disturbance hypothesis. For this, we have three forest regions: High-Disturbance, Mid-Disturbance, Low-Disturbance. Thus, our predictive variable will be “Region” (which is categorical with three levels). Our response variable should test if the three regions are significantly different from one another in some metric of biodiversity. What is expected for this week’s worksheet, is to select ONE metric to test the hypothesis. Our three possible response variables are (1) Species richness (integer/count data), (2) Shannon’s H (continuous), and (3) Pileou’s D (continuous).\nSo in all cases, we’ll have a categorical predictor variable with a continuous response. Because our categorical variable has three levels, we’ll need to use an ANOVA test, or its non-parametric equivalent. In all cases, we will be comparing are the regions on average significantly different from one another. So we’ll need to calculate the metric for each quadrat, then average by region. Thankfully, much of the dplyr syntax (code) we’ve seen is very useful for these operations.\nRegardless of metric, we’ll make a simple barplot and run a statistical analysis to assess if the observed differences in the barplot are significantly different.\n\n\nA. Species Richness\nFirst, let’s run the analysis for species richness. This is the most straightforward from a coding perspective. We just need to count the number of unique species in each quadrat.\nThis could feasibly be done by hand but it is easier in R. Also, it would not be feasible if we had more regions and we’re less likely to make a mistake!\nThe first step will be to make a data.frame with just counts the number of unique species in each quadrat. We can do that with dplyr I’ll name it richness_df.\nInside this complex chunk of code the key operation comes in the summarize function, where I create the new column richness. Note that here, I’m saying “take the raw data, group it by region and quadrat, then for each group, calculate richness in a new column”. We can calculate richness by finding the unique taxa and then counting them (length).\n\nrichness_df <- raw_data |> \n  group_by(Region, Quadrat) |> #group these\n  summarise(richness = length(unique(Taxa))) # make a column for the # of unique taxa\n\nYou can compare the new richness data.frame to your raw data. Take a look with these functions:\n\nView(richness_df)\nView(raw_data)\n\nThis is a good opportunity to spot-check some of the math. Does the high-density, quadrat 1 have the correct number?\n\nA.1 Plotting Species Richness\nTo make our plot, we’ll need to make a summary data frame which has the mean richness per region with standard deviation. I’ll call it richness_plot_df\nWe can use dplyr:\n\nrichness_plot_df <- richness_df |> \n  group_by(Region) |> \n  summarize(mean_richness = mean(richness),\n            sd_richness = sd(richness))\n\nNow we can make our plot:\n\nggplot(richness_plot_df) +\n  geom_bar(aes(x = Region, y = mean_richness),\n           stat = 'identity', fill = 'black')+\n  geom_errorbar(aes(x = Region, ymin = mean_richness,\n                    ymax = mean_richness + sd_richness),\n                width = 0.5, color = 'black')+\n  labs(x = \"Region\",y = \"Mean Richness\")+\n  theme_bw()\n\n\n\n\n\n\nGreat! It looks pretty good!\n\n\nA.1b Plotting Bonus\nOne issue with the plot above is I don’t like the order of the categories. When we look back to the lab reading, Connell’s figure has the categories as a gradient from high-to-mid-to-low.\nSo let’s reorganize the plot to match. Here we need to reassign factor levels in the data frame then it will work in ggplot:\n\n# What if I want my figures in a different order?\nrichness_plot_df$Region <- factor(richness_plot_df$Region, levels = c(\"High-Disturbance\",\n                                                                      \"Mid-Disturbance\",\n                                                                      \"Low-Disturbance\"))\n\nNow we actually can just run the same plot code:\n\nggplot(richness_plot_df) +\n  geom_bar(aes(x = Region, y = mean_richness),\n           stat = 'identity', fill = 'black')+\n  geom_errorbar(aes(x = Region, ymin = mean_richness,\n                    ymax = mean_richness + sd_richness),\n                width = 0.5, color = 'black')+\n  labs(x = \"Region\",y = \"Mean Richness\")+\n  theme_bw()\n\n\np1 = ggplot(richness_plot_df) +\n  geom_bar(aes(x = Region, y = mean_richness),\n           stat = 'identity', fill = 'black')+\n  geom_errorbar(aes(x = Region, ymin = mean_richness,\n                    ymax = mean_richness + sd_richness),\n                width = 0.5, color = 'black')+\n  labs(x = \"Region\",y = \"Mean Richness\")+\n  theme_bw()\nwatermark_plot(p1)\n\n\n\n\n\n\nA.2 Richness Analysis\nIn the project plan we suggested running an ANOVA to compare between the categories. However, ANOVAs assume that our (response) data are normally distributed.\nTechnical note: ANOVAs are just extensions of linear models. There are several assumptions but normality of residuals is the main one. This is not normality of the data, but are the data within a group normally distributed around the mean. However, typically if the data are non-normal then the residuals will be non-normal at small sample sizes. At larger sample sizes this is less of a concern due to the Central Limit Theorem.\nSo first we need to see if the species richness are normally distributed within each group. We can do this a number of ways but the easiest is to see if the density distribution looks normal. Note these are not plots we’d share with anyone, or put in your worksheet but it is a quick way for us to see:\nHere, I’m using base R graphics and some advanced approaches just to keep it short. Don’t worry about all these details. Here, we’re primary concerned about the question of normality:\n\npar(mfrow = c(2,2)) # set up plotting window in 2x2 grid\nfor(group in unique(richness_df$Region)) {\n  density(richness_df$richness[richness_df$Region == group]) |> \n  plot(main = group)\n}\n\n\n\n\nLooking at the figures, it seems that non-normality might be a concern. So we’ll need to use a non-parametric test. The alternative to an ANOVA in this case is a Kruskal-Wallace test.\nThe kruskal-wallace test is similar to our chi-squared test in that it asks: “Are there any differences between the distributions of the groups”.\nLet’s run this test in R:\n\nkruskal.test(richness_df$richness ~ richness_df$Region) # Tell's us it is significantly different\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  richness_df$richness by richness_df$Region\nKruskal-Wallis chi-squared = 3.8114, df = 2, p-value = 0.1487\n\n\nAs of thursday’s data addition, we do not have a significant difference in species richness between groups (p = 0.149, kruskall-wallis test).\n\n\n\nB. Shannon’s H Diversity Metric\nCalculating the Shannon-Wiener index is a little more complicated from an R perspective. Look at the formula:\n\\[\nH = -\\sum_{i = 1}^{R}{\\frac{n_i}{N}ln\\frac{n_i}{N}}\n\\]\nWe need to caculate for each quadrat, the total number of species (\\(N\\)) and the proportion of each \\(i^{th}\\) species in that quadrat, the natural log of that proportion and then sum it up for all species in that quadrat!\nAgain you could brute-force this calculation or do it in Excel. For this lab it might be feasible, but what if you had years of data! This is where R is useful.\n\nB.0 Bonus\nLet’s first calculate H for just one region, one quadrat by “hand”. I’ll create a data frame of just one quadrat\n\n# just for one case:\nquadrat_1 <-  raw_data |> \n  filter(Region == \"Low-Disturbance\", Quadrat == '1')\n\nquadrat_1\n\n           Region Quadrat Taxa Count\n1 Low-Disturbance       1    A     1\n2 Low-Disturbance       1    B     1\n3 Low-Disturbance       1    C     3\n\n\nNow we can calculate all those values. Note here, I’m using p as the term for \\(\\frac{n_i}{N_i}\\)\n\np <- quadrat_1$Count / sum(quadrat_1$Count) # create counts\nlnp <- log(p)\n-sum(p * lnp)\n\n[1] 0.9502705\n\n\n\n\nB.0 Calculating H for all the data\nHere, we will create our own function for calculating H. Honestly, this is more advanced than your typical intro-to-R but it makes the analysis easier on the whole. So let’s roll with it.\n\n# We can write a function to do this repeatedly!\ndiversity_calculator <- function(count) {\n  p <- count / sum(count) # create counts\n  lnp <- log(p)\n  H <-  -sum(p * lnp)\n  return(H)\n}\n\nIf you did the bonus above, you can check the function works but just running on the quadrat 1 data!\nNow we can use same code were familiar with to calculate H, just with our own function inside summarize. I’ll create a new data.frame called diversity_df\n\ndiversity_df <- raw_data |> \n  group_by(Region, Quadrat) |> \n  summarise(H = diversity_calculator(Count))\n\n\n\nB.1 Plotting Diversity\nLet’s make our summary data frame to plot with. I’ll call it div_plot\n\ndiv_plot <- diversity_df |> \n  group_by(Region) |> \n  summarize(mean_H = mean(H),\n            sd_H = sd(H))\n\nNow let’s plot it. Note that I’m included the code in here to reorganize the order of the categories from the A.1b section:\n\n# What if I want my figures in a different order?\ndiv_plot$Region <- factor(div_plot$Region, levels = c(\"High-Disturbance\",\n                                                      \"Mid-Disturbance\",\n                                                      \"Low-Disturbance\"))\n\nggplot(div_plot) +\n  geom_bar(aes(x = Region, y = mean_H),\n           stat = 'identity', fill = 'black')+\n  geom_errorbar(aes(x = Region, ymin = mean_H,\n                    ymax = mean_H + sd_H),\n                width = 0.5, color = 'black')+\n  labs(x = \"Region\",y = \"Shannon's H\")+\n  theme_bw()\n\n\n\n\n\n\n\n\nB.2 Analyzing H\nLet’s take a look at if normality might be a concern for this dataset:\n\npar(mfrow = c(2,2)) # set up plotting window in 2x2 grid\nfor(group in unique(diversity_df$Region)) {\n  density(diversity_df$H[richness_df$Region == group]) |> \n  plot(main = group)\n}\n\n\n\n\nAgain, non-normality might be a considerable concern. So let’s use the kruskal-wallace test.\n\nkruskal.test(diversity_df$H~diversity_df$Region)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  diversity_df$H by diversity_df$Region\nKruskal-Wallis chi-squared = 1.9782, df = 2, p-value = 0.3719\n\n\nHere, this test suggests that there is no significant difference between the regions. So we can write that as our result. (This might change with added data).\n\n\n\nC. Evenness\nFinally let’s calculate pileou’s evenness. Again, this might create some new code challenges but it will make it easier in the long-run.\nRemember the formula:\n\\[ D = 1 - \\sum_{i=1}^{R}{\\frac{n_i(n_i-1)}{N(N - 1)}} \\]\n\nC.0 Bonus\nHere, we can calculate the value for just one quadrat. Here, I’m using the quadrat_1 data frame I created in B.0 bonus. If you skipped that go back and make sure you have quadrat_1 in your environment.\n\nn = quadrat_1$Count\nN = sum(quadrat_1$Count)\n# Solve for D\n1 - sum((n*(n-1))/(N*(N-1)))\n\n[1] 0.7\n\n\n\n\nC.0 Calculating Pileou’s evenness\nHere’s the self-defined function which will calculate pileou’s evenness:\n\npileou_calculator <- function(count) {\n  n = count\n  N = sum(count)\n  D = 1 - sum((n*(n-1))/(N*(N-1)))\n  return(D)\n}\n\nNow we can calculate it for all quadrats. I’ll call it evenness_df\n\n# calculate for all!\nevenness_df <- raw_data |> \n  group_by(Region, Quadrat) |> \n  summarize(D = pileou_calculator(Count))\n\n\n\nC.1 Plotting Evenness\nWe’ll need to make a summary dataframe first. I’ll call it evenness_plot\n\nevenness_plot <- evenness_df |> \n  group_by(Region) |> \n  summarize(mean_D = mean(D),\n            sd_D = sd(D))\n\nNow we can plot that dataframe.\n\n# What if I want my figures in a different order?\nevenness_plot$Region <- factor(evenness_plot$Region, levels = c(\"High-Disturbance\",\n                                                      \"Mid-Disturbance\",\n                                                      \"Low-Disturbance\"))\n\nggplot(evenness_plot) +\n  geom_bar(aes(x = Region, y = mean_D),\n           stat = 'identity', fill = 'black')+\n  geom_errorbar(aes(x = Region, ymin = mean_D,\n                    ymax = mean_D + sd_D),\n                width = 0.5, color = 'black')+\n  labs(x = \"Region\",y = \"Pileou's D\")+\n  theme_bw()\n\n\n\n\n\n\n\n\nC.2 Analyzing D\nFirst, let’s check the normality of our groups:\n\npar(mfrow = c(2,2)) # set up plotting window in 2x2 grid\nfor(group in unique(evenness_df$Region)) {\n  density(evenness_df$D[evenness_df$Region == group]) |> \n  plot(main = group)\n}\n\n\n\n\nAgain, non-normality might be a concern so let’s use the kruskall.wallace test\n\nkruskal.test(evenness_df$D~evenness_df$Region)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  evenness_df$D by evenness_df$Region\nKruskal-Wallis chi-squared = 4.1779, df = 2, p-value = 0.1238\n\n\nAgain there is no difference in evenness between the regions."
  },
  {
    "objectID": "code_lab-03-solutions.html",
    "href": "code_lab-03-solutions.html",
    "title": "Some approaches analyze lab 3",
    "section": "",
    "text": "As we covered in our in-class lab analysis, there is no one correct way to analyze a complex dataset like the one that we have in our lab 3. However, I’ll share my approaches here.\n\nSome Context\nLet’s return to the lab’s hypotheses:\n(H1): Along a strong, but short, moisture gradient the dominant tree species will vary in distinct regions. (H2.A): Trees alter the soil composition at a hyper-local scale, so that their leaf-litter supports distinct communities of invertebrates. (H2.B): Alternative to H2.A, it could simply be that the abiotic factors are the primary factor determining soil community structure, and the community will vary based on the soil characteristics rather than treespecies.\nSo for the first hypothesis, we want to evaluate if the dominance of different trees varies along the hillside. To assess “dominance” of trees, we can use the relative importance index and compare how it changes for different trees along the slope.\nThe second hypothesis is a little more complicated. We are simply interested if there is some difference in community composition between (A) particular trees vs open areas vs others or (B) between soil characteristics (which we assume are associated with the region). Thus, our first hypothesis is someone dependent on the results of the first hypothesis. If the tree community composition varies along the hill side then we’ll only be able to compare . However, if tree dominance doesn’t vary with the slope, we can assess soil community based on the tree-base and the soil community separately.\nOne general concern I had for this data analysis was that the wasn’t really a true “open” area from which we could collect. The tree canopy covers the entire slopeside so we can’t just compare tree vs. non-tree. However, if trees are different based on regions of the slope, we can still assess if the community composition is different. Another challenge of this lab is we have relatively small samples sizes for the transects. However we do have good data for the inverts.\nMy approach to analyze the data is very exploratory, I try a bunch of possible approaches to see if we can tease out a signal. However, as you’ll see, analyses are just a tool - while some may be more useful in a given case, generally if there truly isn’t a pattern - your data will show it!\nFor all this, I’m am using a few more advanced approaches to make it quicker. I’ll show clearly how to run a simple version, but there’s also the advanced options for those of you interested in some of the more powerful applications of R.\n\n\n\n\nSimpleAdv.\n\n\nIn a simple approach, we read in our data by downloading from the github repo’s data folder, put it in our folder of choice, then read it in locally (this is what I’ve taught so far):\n\ninverts <- read.csv('inverts.csv')\ntree_raw <- read.csv('tree_raw.csv')\n\n\n\nWhat I actually do when writing the tutorials is I will read in the files directly from the internet. You can give read.csv an html address with a csv file. These can be found by clicking “raw” on the github page when looking at the csv of the data. That normally would look like this:\n\ninverts <- read.csv(\"https://raw.githubusercontent.com/USC-Ecology-Lab/Lab_3_Community-Assemblages/main/data/inverts.csv\")\n\nHowever, because we have multiple files, I could click through and manually copy all links, but that’s less fun. So I decided to try and figure out how to get all links automatically using githubs API. Now, this took longer to learn how to do than it would’ve to just copy-and-paste. But I had fun learning this for the past 10mins. Admittedly this is way more than just the ‘advanced version’, but you clicked on this tab so ha.\nThanks chatGPT for the framework!\n\n# This is the code that actually got ran\n\n# I need R's api and json libraries\nlibrary(httr)\nlibrary(jsonlite)\n\nrepo <- 'usc-ecology-lab/Lab_3_Community-Assemblages'\nfolder_path <- './data'\n\napi_url <- paste0(\"https://api.github.com/repos/\", repo, \"/contents/\", folder_path)\n\n# read it all in\nresponse <- GET(api_url) |> \n  content('text') |> \n  fromJSON()\n\nfile_names <- response$name |> \n  gsub(pattern = '.csv',\n       replacement = \"\",\n       )\n\n# now I can create a list of dataframe\ndf_list <- list()\n# loop through and download\nfor(i in 1:nrow(response)) {\n  df_list[[i]] <- read.csv(response$download_url[i])\n}\nnames(df_list) <- file_names #name the list\nlist2env(df_list, envir = .GlobalEnv) #load the list to the environment\n\n<environment: R_GlobalEnv>\n\nrm(list = c(\"df_list\", \"repo\", \"api_url\",\n            'file_names','folder_path','i'))\n\n\n\n\n\n\nExploratory Data Analysis for Trees\nTo address the first hypothesis, we need to explore how the importance of trees varies with the slope. However, to do this we need to think about the data we have.\nThe tree_raw dataframe does not offer much to work with. Unless we wanted to investigate a question like “does loblolly size vary based on slope?”. Instead we need to rely on the RIV (relative importance value). I already calculated this for you in a few approaches. First, I did it by species and by category (grouping some similar species together). I also calculated RIV in 5m bins and then again in 3-large bins which correspond to the defined sub regions people listed on their transects.\nRIV is calculated by effectively measuring the total area in a set bin which is occupied by that particular species. Such that for each \\(s\\) species, and \\(i\\) tree (belonging to a \\(s\\) species):\n\\[\nRIV_{s} = \\frac{\\sum_{i_s} \\pi \\frac{DBH_{i_s}}{2}^2}{\\sum_{i} \\pi \\frac{DBH_{i}}{2}^2}\n\\]\n\nAn attempt at regression & correlation\nI really wanted to try a correlation/regression analysis in this lab. However, the data don’t really make it all that clean (or appropriate). Nonetheless, I figured if I make 5-m bins we can try to treat them as a continuous variable and run a regression. Regressions, as you can see below don’t apply in this case. However, we can use a non-parametric correlation to say something about our data.\n\nSimpleAdv.Better plots\n\n\nWe can only look at one tree’s RIV change at a time. For the simple example, let’s just do hardwoods because they are the most abundant overall.\n\nlibrary(dplyr)\nhardwood_bins <- tree_cat_5m |> \n  filter(Tree_cat == 'hardwood')\n\nhardwood_lm <- lm(riv ~ bin, data = tree_cat_5m)\nsummary(hardwood_lm)\n\n\nCall:\nlm(formula = riv ~ bin, data = tree_cat_5m)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.25000 -0.18992 -0.13095 -0.08333  0.89286 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.250000   0.065199   3.834 0.000228 ***\nbin         -0.004762   0.003117  -1.528 0.129951    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3499 on 94 degrees of freedom\nMultiple R-squared:  0.02423,   Adjusted R-squared:  0.01385 \nF-statistic: 2.334 on 1 and 94 DF,  p-value: 0.13\n\n\nFrom the above output, we can look at the p-value of “bin” to see if there is a signficant effect of bin (slope location) on riv. It is 0.129951, so we would conclude the slope is not significantly different from 0. Or in simpler words there is no effect.\nTo be quick, I just looked at this data using base R:\n\nplot(riv ~ bin, hardwood_bins)\nabline(hardwood_lm)\n\n\n\n\nThis isn’t a pretty plot I would present, but it is useful for my purposes. We can see that the regression line is largely influenced by the large amount of 0’s at high bins. A linear regression in this case is not really the best method given the fact the LIINE assumptions are not met. However, we can test if there is a significant correlation using a non-parametric correlation test. Correlation tests aren’t as useful as regressions since they don’t give us an effect size like slope - which is a quantitative relationship with predictive power. Instead, correlations just tell us how tightly two variables are related (from 0-1). Correlations work well with monotonic relationships, and there are several methods for using non-normal, non-linear data. Here, we’ll use spearman’s rank correlation\n\ncor.test(x = hardwood_bins$bin, y = hardwood_bins$riv, method = 'spearman')\n\n\n    Spearman's rank correlation rho\n\ndata:  hardwood_bins$bin and hardwood_bins$riv\nS = 3873.7, p-value = 0.0002268\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.6842051 \n\n\nHere, we can conclude that there is a significant, negative correlation between increasing distance up the slope (bin) and the relative importance value of hardwoods (spearman’s \\(\\rho\\) = -0.684, p < 0.001).\nTo do this with other categories, you can just change where we filter for hardwoods and try for other categories!\n\n\nReally, in fully data exploration mode, I want to run the simple analysis for every single species and every single categories. It gets fairly messy to do this “by-hand” and copy-pasting code over and over and changing little details. Instead what I really would do here is loop through all categories and print out the regression output and make quick, ugly plots. If there’s something interesting, I can clean it up later.\nI can now scroll through all this output and read what I want!\n\n# Loop analysis:\ntree_sp <- list()\nfor(tree in unique(tree_sp_riv_5m$tree_id)) {\n  tree_sp[[tree]] <- tree_sp_riv_5m |> \n    filter(tree_id == tree)\n}\n\n# run the regressions here\ntree_5m_reg_mod <- list()\nfor(tree in names(tree_sp)) {\n  tree_5m_reg_mod[[tree]] <- lm(riv ~ bin, data = tree_sp[[tree]])\n  print(tree) # print out name of trees\n  print(cor.test(x = tree_sp[[tree]]$bin, y = tree_sp[[tree]]$riv,\n                 method = 'spearman'))\n}\n\n[1] \"sycamore\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_sp[[tree]]$bin and tree_sp[[tree]]$riv\nS = 3358.3, p-value = 0.02367\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.4601307 \n\n[1] \"dogwood\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_sp[[tree]]$bin and tree_sp[[tree]]$riv\nS = 2857.1, p-value = 0.2541\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.2422138 \n\n[1] \"hickory\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_sp[[tree]]$bin and tree_sp[[tree]]$riv\nS = 2946.6, p-value = 0.1833\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.2811346 \n\n[1] \"loblolly\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_sp[[tree]]$bin and tree_sp[[tree]]$riv\nS = 2110.8, p-value = 0.7024\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n0.08226127 \n\n[1] \"whiteoak\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_sp[[tree]]$bin and tree_sp[[tree]]$riv\nS = 1694.7, p-value = 0.214\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.2631807 \n\n[1] \"poplar\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_sp[[tree]]$bin and tree_sp[[tree]]$riv\nS = 3032.6, p-value = 0.1293\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n-0.318511 \n\n[1] \"bay\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_sp[[tree]]$bin and tree_sp[[tree]]$riv\nS = 3055.9, p-value = 0.1169\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.3286648 \n\n[1] \"wateroak\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_sp[[tree]]$bin and tree_sp[[tree]]$riv\nS = 2419.1, p-value = 0.8101\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n        rho \n-0.05178796 \n\nfor(tree in names(tree_sp)) {\n  try({\n    plot(riv ~ bin, tree_sp[[tree]],\n         main = tree)\n    abline(tree_5m_reg_mod[[tree]])\n  })\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooking at it by species, it is not very clear\nI can also do this by category:\n\n# Loop analysis:\ntree_cat <- list()\nfor(cat in unique(tree_cat_5m$Tree_cat)) {\n  tree_cat[[cat]] <- tree_cat_5m |> \n    filter(Tree_cat == cat)\n}\n\ntree_cat_reg_mod <- list()\nfor(cat in names(tree_cat)) {\n  tree_cat_reg_mod[[cat]] <- lm(riv ~ bin, data = tree_cat[[cat]])\n  print(cat)\n  print(cor.test(x = tree_cat[[cat]]$riv,\n                 y = tree_cat[[cat]]$bin,\n                 method = 'spearman'))\n}\n\n[1] \"hardwood\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_cat[[cat]]$riv and tree_cat[[cat]]$bin\nS = 3873.7, p-value = 0.0002268\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.6842051 \n\n[1] \"pine\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_cat[[cat]]$riv and tree_cat[[cat]]$bin\nS = 2110.8, p-value = 0.7024\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n0.08226127 \n\n[1] \"oak\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_cat[[cat]]$riv and tree_cat[[cat]]$bin\nS = 1953.5, p-value = 0.4823\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.1506457 \n\n[1] \"other\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_cat[[cat]]$riv and tree_cat[[cat]]$bin\nS = 3055.9, p-value = 0.1169\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.3286648 \n\nfor(cat in names(tree_cat)) {\n  try({\n    plot(riv ~ bin, tree_cat[[cat]],\n         main = cat)\n    abline(tree_cat_reg_mod[[cat]])\n  })\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooking at all the categories, our only significant correlation is with the hardwoods. However, it is worth noting that the pines only occur in the mid-slope area. This would make the relationship non-monotonic so we can’t capture that relationship with a simple rank-correlation test.\n\n\nIf I wanted to plot these data to look a little better, I’d use ggplot and loess smoothers:\n\nggplot(hardwood_bins, aes(x = bin, y = riv)) +\n  geom_point() +\n  geom_smooth(color = 'black') +\n  labs(x = 'Distance up slope [m]', y = 'Hardwood RIV') +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nThe Kruskall-Wallace approach\nSince we don’t really have enough data for a regression, the alternative approach is to use an ANOVA with RIV as the response variable and sub region as the predictor. Since we have a RIV value for each tree, we will need to run the ANOVA for each response variable. I’m not even investigating my assumptions for an ANOVA however because I have such small sample size, I’m going to assume that the non-parametric approach is necessary.\nI can make one graph to show all this data, but then run multiple tests to investigate the data.\n\nPlotSimpleAdv.\n\n\nI’ll just do this for groups by category since we already well know that there is not enough data for grouping by species.\n\nlibrary(ggplot2)\n\ntree_cat_summary <- tree_cat_subregion |> \n  group_by(Subregion, Tree_cat) |> \n  summarize(mean_riv = mean(riv),\n            sd_riv = sd(riv))\n\nggplot(tree_cat_summary) +\n  geom_bar(aes(x = Tree_cat, y = mean_riv, fill = Subregion),\n           stat = 'identity', position = 'dodge') +\n  geom_errorbar(aes(x = Tree_cat, ymin = mean_riv,\n                    ymax = mean_riv + sd_riv,\n                    color = Subregion),\n                stat = 'identity', position = 'dodge') +\n  labs(x = 'Tree Category', y = 'RIV', fill = \"Subregion\",\n       color = \"Subregion\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nSimilar to the regression approach, we will need to run a kruskall.wallace test for each individual tree-category. I’m going to use the dunn.test() function from the dunn.test package because it will run both the kruskall wallace test and the post-hoc tests at once. The KW test tells us if there is some difference between the groups then the dunn test tells us pair-wise comparisons.\n\nlibrary(dunn.test)\n\n# we'll use the category & subregion dataframe\n# let's look at pines this time\npine_regions <- tree_cat_subregion |> \n  filter(Tree_cat == 'pine')\n\ndunn.test(g = pine_regions$Subregion, x = pine_regions$riv)\n\n  Kruskal-Wallis rank sum test\n\ndata: x and group\nKruskal-Wallis chi-squared = 2.102, df = 2, p-value = 0.35\n\n                           Comparison of x by group                            \n                                (No adjustment)                                \nCol Mean-|\nRow Mean |        bog    dryhill\n---------+----------------------\n dryhill |  -1.416983\n         |     0.0782\n         |\n   slope |  -0.974176   0.442807\n         |     0.1650     0.3290\n\nalpha = 0.05\nReject Ho if p <= alpha/2\n\n\nThis tells us that the relative importance value of pines is not significantly different across regions (Kruskall wallace test, p = 0.35)\n\n\nAgain, I just want to run all the tests at once. This time, to keep it clean, I’ll just do it for the categories:\n\ntree_cat_reg <- list()\n\nfor(cat in unique(tree_cat_subregion$Tree_cat)) {\n  tree_cat_reg[[cat]] <- tree_cat_subregion |> \n    filter(Tree_cat == cat)\n}\n  \nfor(cat in names(tree_cat_reg)) {\n  print(cat)\n  dunn.test(g= tree_cat_reg[[cat]]$Subregion,\n            x = tree_cat_reg[[cat]]$riv)\n}\n\n[1] \"hardwood\"\n  Kruskal-Wallis rank sum test\n\ndata: x and group\nKruskal-Wallis chi-squared = 4.3556, df = 2, p-value = 0.11\n\n                           Comparison of x by group                            \n                                (No adjustment)                                \nCol Mean-|\nRow Mean |        bog    dryhill\n---------+----------------------\n dryhill |   2.086996\n         |    0.0184*\n         |\n   slope |   1.043498  -1.043498\n         |     0.1484     0.1484\n\nalpha = 0.05\nReject Ho if p <= alpha/2\n[1] \"oak\"\n  Kruskal-Wallis rank sum test\n\ndata: x and group\nKruskal-Wallis chi-squared = 2.7152, df = 2, p-value = 0.26\n\n                           Comparison of x by group                            \n                                (No adjustment)                                \nCol Mean-|\nRow Mean |        bog    dryhill\n---------+----------------------\n dryhill |  -1.556997\n         |     0.0597\n         |\n   slope |  -0.311399   1.245598\n         |     0.3777     0.1065\n\nalpha = 0.05\nReject Ho if p <= alpha/2\n[1] \"pine\"\n  Kruskal-Wallis rank sum test\n\ndata: x and group\nKruskal-Wallis chi-squared = 2.102, df = 2, p-value = 0.35\n\n                           Comparison of x by group                            \n                                (No adjustment)                                \nCol Mean-|\nRow Mean |        bog    dryhill\n---------+----------------------\n dryhill |  -1.416983\n         |     0.0782\n         |\n   slope |  -0.974176   0.442807\n         |     0.1650     0.3290\n\nalpha = 0.05\nReject Ho if p <= alpha/2\n[1] \"other\"\n  Kruskal-Wallis rank sum test\n\ndata: x and group\nKruskal-Wallis chi-squared = 1.1667, df = 2, p-value = 0.56\n\n                           Comparison of x by group                            \n                                (No adjustment)                                \nCol Mean-|\nRow Mean |        bog    dryhill\n---------+----------------------\n dryhill |   1.020620\n         |     0.1537\n         |\n   slope |   0.204124  -0.816496\n         |     0.4191     0.2071\n\nalpha = 0.05\nReject Ho if p <= alpha/2\n\n\n\n\n\nAll in all, the kruskall-wallace tests suggest that there are no significant differences between subregions and RIV for any tree category. However, we did see a significant negative correlation between increasing slope distance and hardwood RIV. If I were discussing these results, I’d probably talk about how there were strong trends for some taxa to be in certain regions but there isn’t a large enough amount of data to distinguish between different regions. I’d also note that the hardwood RIV relationship was influenced by the one large Tulip poplar and the American sycamores which tended to be in the bog, or at the bog edge. There was a consistent amount of oaks, albeit from different species up the hill and pines were notable, but sparse.\nAlso maybe DBH isn’t the best metric because although pines don’t get too thick, they get very tall and produce many leaves which may have a dispropotionate influence on the leaf litter.\n\n\n\nExploratory Data Analysis for inverts\nTo assess if the invertebrate community is significantly different between litter invertebrate community and (A) tree-base or (B) slope-region. These data are not too clear to disentangle because based on our tree analysis, we know the tree community is not that different along the slope. However, there are some subtle differences between tree RIV (hardwoods tend to be lower on the slope). So if we see a significant difference, is it from the slope-soil characteristics or the tree litter composition. Theoretically, our open-sampling locations would help disentangle this, however, we didn’t get a truly open sample. Nonetheless, I’ll treat it as such.\n\nPlotSimpleAdvanced\n\n\nFirst, we need to make a summary dataframe with means and standard deviations.\n\ninvert_sum <- inverts |> \n  group_by(Spp, Sampling_tree, Region) |> \n  summarize(mean_count = mean(Num),\n          sd_count = sd(Num))\n\nNow, I have three categories to plot, Species, Sampling_tree, and Region. And I want to show the mean count for each species. This is too much to fit on one plot so I’ll make a plot for each species then display it. Given the large number of species in a study like this, it would be OK to focus on the most abundant or a few unique ones.\n\n# let's just look at mites\nacari_sum <- invert_sum |> \n  filter(Spp == 'Acari')\n\nggplot(acari_sum) +\n  geom_bar(aes(x = Region, y = mean_count, fill = Sampling_tree),\n           stat = 'identity', position = 'dodge') +\n  geom_errorbar(aes(x = Region, ymin = mean_count,\n                    ymax = mean_count + sd_count, color = Sampling_tree),\n                position = 'dodge') +\n  labs(x = \"Slope Region\", y = \"Mean Acari Abundance\", \n       fill = \"Tree\", color  = \"Tree\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nThis analysis is a little bit tricky. We want to compare how abundance of a single species (we’ll use acari) varies between certain trees and slope region. If we had a balanced dataset, we could use a two-way ANOVA to compare both slope region, tree, and the interaction. However, because some tree categories didn’t occur in some regions we can’t do that. Instead, I’ll use an kruskall-wallace test to compare: within a region, does the insect litter community vary between the tree which were sampled. For the example, I’ll just do upper, but you can run this for every region.\n\nupper_acari <- inverts |> \n  filter(Region == 'Upper', Spp == 'Acari')\n\n\ndunn.test(g = upper_acari$Sampling_tree, x = upper_acari$Num)\n\n  Kruskal-Wallis rank sum test\n\ndata: x and group\nKruskal-Wallis chi-squared = 3.1767, df = 2, p-value = 0.2\n\n                           Comparison of x by group                            \n                                (No adjustment)                                \nCol Mean-|\nRow Mean |   Loblolly        Oak\n---------+----------------------\n     Oak |  -0.674849\n         |     0.2499\n         |\n    Open |   1.173673   1.724615\n         |     0.1203     0.0423\n\nalpha = 0.05\nReject Ho if p <= alpha/2\n\n\nSo based in the results of the kruskall-wallace test there’s no significant difference in acari abundance between the Loblolly site, the open site, and the oak site (KW test p-value = 0.2). Discussing these results, I’d note that the pine litter appeared to dominate the whole system and acari were generally well spread throughout all litter communities. I’d also likely run a few more tests in the different regions and with different taxa to see what patterns arise.\n\n\nA common approach for assessing community composition is a Non-metic Multidimensional Scaling approach. This is an ordination technique usable for count data (with a lot of 0’s). Effectively, what NMDS does is it takes all the many response variables (counts of every taxa) and assess how different each row (site) is based on the counts of the taxa. It does this by ranking them in multidimensional space and measuring the distance. Then, it mushes those distances into a lower dimension where we can look at it. Then we can overlay the site/tree/whatever to visualize if there are any clear patterns that arise. It is important to note that when plotting in NMDS-space the distances between the points do not mean anything (hence non-metric) yet we can make relative assessments on the visual groupings.\nWe’ll need to use the vegan package so install it if you don’t have it. I’ll also use tidyr for the data prep.\nWhen running the nmds, it iterates to find an optimal solution. Yet we specify the number of dimensions to reduce into. Ideally, we want a low stress value (<0.2). So I’ll play around with the number of dimensions to get it lower. I did this a few times and ended up setting k = 3 for three dimensions.\n\nlibrary(vegan)\nlibrary(tidyr)\ninvert_comm <- inverts |> \n  pivot_wider(names_from = Spp,\n              values_from = Num)\nset.seed(1000)\ninvert_nmds=metaMDS(invert_comm[,4:ncol(invert_comm)], # Our community-by-species matrix\n                     k=3) # The number of reduced dimensions\n\nSquare root transformation\nWisconsin double standardization\nRun 0 stress 0.1285056 \nRun 1 stress 0.1340613 \nRun 2 stress 0.1348327 \nRun 3 stress 0.1297093 \nRun 4 stress 0.1297095 \nRun 5 stress 0.1297095 \nRun 6 stress 0.1348331 \nRun 7 stress 0.1285056 \n... New best solution\n... Procrustes: rmse 3.606478e-05  max resid 8.770583e-05 \n... Similar to previous best\nRun 8 stress 0.1480649 \nRun 9 stress 0.1315938 \nRun 10 stress 0.1285056 \n... Procrustes: rmse 0.0003471511  max resid 0.0007761196 \n... Similar to previous best\nRun 11 stress 0.1649935 \nRun 12 stress 0.1305927 \nRun 13 stress 0.1285056 \n... New best solution\n... Procrustes: rmse 2.07782e-05  max resid 4.203106e-05 \n... Similar to previous best\nRun 14 stress 0.1285059 \n... Procrustes: rmse 0.0005192305  max resid 0.001164083 \n... Similar to previous best\nRun 15 stress 0.1285059 \n... Procrustes: rmse 0.0005116436  max resid 0.001147285 \n... Similar to previous best\nRun 16 stress 0.1315938 \nRun 17 stress 0.1480648 \nRun 18 stress 0.1315938 \nRun 19 stress 0.1285056 \n... Procrustes: rmse 0.0002657652  max resid 0.0005953675 \n... Similar to previous best\nRun 20 stress 0.1480647 \n*** Best solution repeated 4 times\n\n\nTo plot my nmds, I’ll need to make three plots because I have three dimensions. I can look at 1v2, 1v3, and 2v3. Effectively, I’m plotting the different faces of the cube that these points exist in.\n\nplot_nmds <- cbind(invert_comm[,1:3], invert_nmds$points)\n\n\np1 = ggplot() +\n  geom_point(data = plot_nmds,\n             aes(x = MDS1, y = MDS2, color = Sampling_tree,\n                 shape = Region),\n             size = 5)+\n  geom_label(data = as.data.frame(invert_nmds$species), \n             aes(x = MDS1, y = MDS2, label = rownames(as.data.frame(invert_nmds$species))))+\n  theme_minimal()\n\np2 = ggplot() +\n  geom_point(data = plot_nmds,\n             aes(x = MDS2, y = MDS3, color = Sampling_tree,\n                 shape = Region),\n             size = 5)+\n  geom_label(data = as.data.frame(invert_nmds$species), \n             aes(x = MDS2, y = MDS3, label = rownames(as.data.frame(invert_nmds$species))))+\n  theme_minimal()\n\np3 = ggplot() +\n  geom_point(data = plot_nmds,\n             aes(x = MDS1, y = MDS3, color = Sampling_tree,\n                 shape = Region),\n             size = 5)+\n  geom_label(data = as.data.frame(invert_nmds$species), \n             aes(x = MDS1, y = MDS3, label = rownames(as.data.frame(invert_nmds$species))))+\n  theme_minimal()\n\n\np1\np2\np3\n\n\n\n\n\n\n\n\n\n\n\n\nLooking at these figures there are not really any clear clusters which pop out. This is another way to see there isn’t a strong difference in community composition across the slope/tree sides"
  },
  {
    "objectID": "code_lab-03.html",
    "href": "code_lab-03.html",
    "title": "Analyzing Community Structure",
    "section": "",
    "text": "Analyzing community structure can be achieved through a wide range of approaches. Our two main questions for this lab are (1) Does the tree community vary up the hillside? and (2) Does litter invertebrate community vary based on tree composition or soil composition?\n\nIn-class analysis\nIn class, form groups. Groups should choose to address either the first or second question using our data.\nYou can access your data here.\nThere’s a lot of data, and I’ve formatted it in different ways. Here is a brief description of each:\n\ninverts.csv: Counts of different invertebrate data, with region, tree, and Tag (bag ID) as columns\nraw_litter-invert-data.csv: Unprocessed data, don’t use\ntree_cat_5m.csv: Trees, group into broad categories, calculated their relative importance value (RIV) in 5m bins along the transect\ntree_cat_subregion.csv: Trees, group into broad categories, calculated their relative importance value (RIV) in the three sub regions of the hillside\ntree_sp_riv 5m.csv: Trees, grouped by species, calculated their relative importance value (RIV) in 5m bins along the transect\ntree_sp_riv_subregion.csv: Trees, grouped by species, calculated their relative importance value (RIV) in the three sub regions of the hillside\ntree_raw.csv: Raw measurements of trees up the hillside with DBH (diameter at breast height), exact transect location, and subregion.\n\nIn your groups, choose which dataset you think will be most useful for your analyses. With your dataset:\n\nIdentify the different types of data\nDecide how to visualize your data\nChoose the appropriate statistical analysis for you data\n\n\n\nLink to my approaches:\nlink"
  },
  {
    "objectID": "code_main.html",
    "href": "code_main.html",
    "title": "Programming in Ecology",
    "section": "",
    "text": "Broadly speaking, ecology is a field focused studying organisms and how they interact with others and their surroundings. This often involves identifying, describing, and hypothesizing about ecological patterns. In the pursuit of studying such patterns, ecologists in the modern world utilize a vast array of tools to collect and analyze data. Ecological data spans a wide range of formats and sampling distributions. As ecologists, we must be able to properly interrogate our data so that we can identify meaningful trends. That is where the application of statistics comes into the picture. Simply collecting data and making general conclusions cannot inform thorough conclusions. Analyzing data through the proper application of statistical tests will help us as scientists.\nWith the advancement of technology and statistical computing, ecological data analysis has progressed beyond the point of simple calculations. The large amounts of data acquired in ecology require us to be proficient programmers and statisticians. Luckily, there are several, free tools which are growing in popularity for people to use, making data analysis extreme accessible. Arguably the most common tool used in ecology, academia, and data science is R. R is an open-source programming environment and language. R was developed primarily by statisticians which makes it extreme versatile. Additionally, because so many ecologists use it, there are many add-ons (called packages) which are taylor-made for ecological applications.\nIn this course, I will provide examples and support for processing lab data in R. The goal is that someone who has never used R will be able to still be successful. This means at times I may over-explain somethings. Alternatively, it is difficult to remember what it was like to first learn something, so if I am not explaining anything well, please come ask me! Finally, when it comes to completing your assignments, I don’t care what software you use to get it done. For example, if you have to make a bar graph, you might know how to complete something quicker in excel than in R. So, go ahead and use what is most comfortable. However, time invested in learning R will only better equip you to expand your skills.\n\n\nGoogle, chatGPT, and Stack Overflow are all extremely useful resources for solving any issues you may encounter while working with R. Often times if you get an error, simply copy and paste it in a search and there’s likely someone else who experienced a similar issue. This is a fundamental piece of the process. Working with R (or any programming language) is a rarely a smooth process. Don’t be dissuaded by needing to search and solve issues. It’s all just part of the game."
  },
  {
    "objectID": "code_main.html#downloading-r-studio-r",
    "href": "code_main.html#downloading-r-studio-r",
    "title": "Programming in Ecology",
    "section": "Downloading R studio & R",
    "text": "Downloading R studio & R\nGo to the posit website at https://posit.co/download/rstudio-desktop/. This website has instruction for downloading both R and RStudio.\nWhen downloading R, select the right one for your computer at the top panel. If on the initial installation it asks you to select a mirror, it doesn’t matter. Just select whatever.\nOnce you have both R and Rstudio downloaded, you can just open Rstudio."
  },
  {
    "objectID": "code_main.html#rstudio-layout",
    "href": "code_main.html#rstudio-layout",
    "title": "Programming in Ecology",
    "section": "RStudio Layout",
    "text": "RStudio Layout\nWhen you first open RStudio, there will be three main tabs, the largest of which is the console. On the right are two windows with multiple tabs. The preselected tab on the bottom is a file browser and the one of top shows what is active in your R environment. At the start this is empty but as you create variables or functions they are visible in this window.\nMy setup is a bit different but you can see a good orientation at this website."
  },
  {
    "objectID": "code_main.html#understanding-r-code",
    "href": "code_main.html#understanding-r-code",
    "title": "Programming in Ecology",
    "section": "Understanding R code",
    "text": "Understanding R code\nThere are several great resources for learning R in detail which I’ll link at the bottom. Very briefly, I want people to understand some very basic items:\nComments are great for understanding code. You should include it for your future self to refer back to. I will include comments in all our class code.\n\n# comments are written on lines starting with hastags. These will appear different.\n# Comments can also be written after code\n2+2 == 4 # running this line will return TRUE\n\nWe can assign values to objects using an “assignment operator”. Traditionally, in R this is an arrow <-. You can also use an equal sign =. I prefer the arrow for a number of reasons but it is also intuitive. You take all the values on the left and put them into the storage variable on the left.\n\nx <- 5 # take 5 and put it into x\ny = 5 # take 5 and put it into y\n\nprint(x == y) # will print TRUE\n\nz <- x * y # take the product of x and y and put it into z\nprint(z) # print z\n\nFinally, functions are operations which can be preformed on values/objects. Functions are executed by feeding arguments into a call. This generally looks like function(arg1, arg2, ...). This is very similar to excel where you would write =function(arg1, arg2) in a cell.\nNote in the examples above, print is a function. You can learn more about any function through the documentation. Simply write into the r console ?function. where the function is listed.\n\n?print\n\nFunctions can also be wrapped around one another and are preformed inside-out. Standard order of operations. For example:\n\n# c() is a function short for concatenate or combine. it chains together values\nc(5,5,3,4)\n\n# the mean function takes the mean of a range of values.\n# so you could do this two ways:\n\n# option 1:\nx <- c(5,5,3,4)\nmean(x)\n\n# option 2:\nmean(c(5,5,3,4))\n\nA unique aspect of R, which has recently been added to the base functionality\nThis is an extremely simplistic overview. I encourage everyone to look at some of the additional resources for help with learning R."
  },
  {
    "objectID": "code_main.html#organizing-your-computer-files-importing-to-r",
    "href": "code_main.html#organizing-your-computer-files-importing-to-r",
    "title": "Programming in Ecology",
    "section": "Organizing your computer files & importing to R",
    "text": "Organizing your computer files & importing to R\nMany people do not maintain a clear organization of files in their computer’s storage. This problem is compounded by .\nA common problem that many students using the university’s office licence run into is that their word/excel documents are saved into their University cloud storage (onedrive). Then it can be tricky to find those files through a programming approach. If you have a good file organization system, please keep to it and you should be fine. However, if you haven’t put much thought into organizing your files, I encourage you to create a new folder for this course. You can store all your files in that folder. Then when trying to work with R, you can load your files from that path.\nSo let’s talk about file paths. In windows, you can use the file explorer to look up files. Then you can access the path by clicking on the top bar. Typically, it will look something like this: ‘C:\\Users\\yourname\\Documents\\EcologyLab’. In Mac, it will look a little different. Notably, the slashes will be front-slash instead of back-slash. If you try to read a file from R, mac users can just copy-paste their file paths. However, windows users need to write it out with a front-slash. Alternatively, you can use double-backslashes but this is a bad habit to develop.\nIf you want to be an advanced R user, you should learn about organizing your files in a package format and taking advantage of RStudio’s project feature. You should also use relative file paths and set-up projects using git. However, this is beyond the scope of the course and likely most applications for ecology projects.\nRead more about file organization in the additional resources page.\n\nReading in files\nThe most common US-based data format is a comma separated value file (*.csv). However, most people are more familiar with working in excel which has its own file format (.xlsx or .xls). There are several R packages to help you read in excel files. But for the sake of keeping things simple this course, we will stick to .csv files.\nYou can save any excel file as a .csv. Just make sure you keep your headers simple.\nTo read in a file, you can call read.csv(). This is where file paths become important. In R, you have a working directory. This is where R is currently looking for files. You can look at your working directory with the function getwd(). Also RStudio displays the working directory at the top of the console window. Note that the tilde (~) often will refer to your Documents folder. At least for Windows machines.\n\nYou can change your working directory with setwd(). You simply put the file path to the folder where you want to access data from.\nIn short, loading files might look like this:\n\nmy_path <- '~/BIOL570L Docs' #replace this with your path file\nsetwd(my_path) #reassign working directory. This is only necessary if not already there\nmy_data <- read.csv('mydata.csv') #load data\n\n#altarntively\nmy_data <- read.csv('~/BIOL570L Docs/mydata.csv')"
  },
  {
    "objectID": "code_main.html#packages",
    "href": "code_main.html#packages",
    "title": "Programming in Ecology",
    "section": "Packages",
    "text": "Packages\nA great benefit of R is that there are user-made packages which contain functions and data for particular purposes. Most functions I’ve used in this example are ‘base’ R, meaning they exist by default. Other functions can be loaded by packages. To keep R simple, most packages are not loaded by default. To load a package, you must first install it. Luckily, R does a great job maintaining all packages in R, through the Comprehensive R Archive Network (CRAN). So you can download CRAN packages directly though R.\nTo install a package, you can use the install.packages() function.\n\n# we will use ggplot2 in our first assingment so you'll need to download it.\ninstall.packages(\"ggplot2\")\n\nThen each time you need to utilize a package in an R script, you will need to load the package. Typically this is done at the top of a script. You can load a package with the library() function.\n\nlibrary(ggplot2)\n\nNote that when installing we use quotes and when loading we don’t. It’s tricky that way. You should only need to install a package once unless you need to update it. Often times beginners in R get stuck on trying to use functions they haven’t loaded. So make sure you called library() if you get an error about not finding a function."
  },
  {
    "objectID": "gen_expectations.html",
    "href": "gen_expectations.html",
    "title": "General Expectations for Scientific Writing",
    "section": "",
    "text": "Throughout the course of the semester, we have general themes which will consistently arise in how we present our findings. For example, in every worksheet, we have a figure, a results statement, a mini-discussion, hypotheses, etc. All these elements will also arise in our lab reports and to an extent, our presentations.\nAll these elements are generally based on the core concepts of a scientific paper. Broadly speaking, scientific papers follow a funnel structure. The introduction takes big-scale ideas about the field of knowledge, identifies what missing knowledge there is (what are your core questions), then introduces the specific study and hypotheses. The methods and results are narrowly focused on the exact details of the study. Then the discussion takes those results, evaluates them in a narrow sense and then places them in the context of larger ideas / other studies.\n\n\nBackground / Introduction Statements\nThe background of your assignments should focus broadly, starting with general ecological ideas, then focus into the specifics of your investigation. Often times, this means introducing the broad theoretical or hypothetical underpinnings of your topic. Then you introduce the specifics of your study system and how it relates to those broad scale system. Then finally, you introduce the specific aims of the investigation (hypotheses, goals, questions).\nTo accomplish writing these sections, it is highly necessary to utilize primary literature and/or review papers to introduce the background. These should be written in a way that someone with a basic ecological background can learn and understand this study.\n\n\nHypotheses:\nIn a full paper, the main questions, hypotheses and predictions are listed in the Introduction portion (Typically in the last paragraph). For the project plans, worksheets, and grading, I’ve listed them separately. That is because of how critically important a good understanding of a hypothesis is.\nFar too many people do not adequately distinguish between hypotheses, predictions, and theories. While this is in part a societal problem - people often refer to a hypothesis they have as a theory - we will not fall for it in this class. A hypothesis is a statement about how things work. It is a proposed explanation for an observed pattern. However, what makes a good hypothesis is that it is explicit in mentioning a mechanism which can be tests. Hypotheses should have a clear prediction which can be derived from them, that we then seek to test in our observations.\n\n\nFigures:\nAs stated in the syllabus, you are welcome to create figures however you wish. I am teaching the course in R and encouraging you to give it a try. Regardless of how you created your figure, there are some elements which should be universal. I’ll discuss them in detail below. However, one thing that every figure should do is represent a summary of the data in some quick way. Really, the purpose of a figure is to show your results and key messages in an easy-to-digest way. If you were really excited about some scientific finding, you could put all the data into a google-sheet, share the link and send it out to the masses. However, who would actually engage with your data? Probably no-one unless they were extremely interested in whatever sub-discipline you were researching. Figures should be readily accessible ways to share information. People are extremely visual creatures. We are exceptionally good at recognizing patterns… even if they are not there. Think about how often you looked at a cloud and recognized some familiar entity.\nThus, make a good figure follows the classic saying: with great power comes great responsibility. We always want to present our data in a clear figure. But we also want to make sure that figure accurately reflects our data.\nThere are plenty of great resources out there about creative ways to summarize your data. One of my favorites is the r graph gallery.\n\nAxes titles:\nAll axis on a figure should have a title. Only sometimes, does a categorical axis have enough detail in the labels to not warrant an axis title. Units, when needed, should be included as well. If there is a legend, you can include a title for it if necessary but sometimes it should be evident.\n\n\nFigure Color:\nTraditionally, figures simply relied on black, white, and grey due to the cost of printing color articles. Now that there are web-based publications primarily, color is increasingly common. However use color with caution. You don’t want it to look unprofessional. Additionally, keep in mind that many people are color blind, so it is important to make the figure accessible to all types of vision. Check out some great resources on color-blind friendly, palettes here.\n\n\nError bars & sources of variation\nBecause figures often have summary data shown, we want to make sure that our figure has some measure of variation shown. Many times we show the standard error, hence the term error bars. However, I prefer to show the more raw data with standard deviation shown. Either way, make sure when creating a figure, you evaluate what variation needs to be shown in the figure and you communicate what is shown in the figure caption. If this is not possible with the figure you’ve created, make sure you are confident in your figure choice.\n\n\nFigure Captions\nEvery figure you submit should have a figure caption. We are striving to communicate in a way which is consistent with general conventions in science. You’ll notice as you read papers, very, very, very, very, very, very, very rarely would a figure ever have a title at the top. Rather figures have captions which provide a little more detail about the image above.\nThis is not to say that figure titles are completely obsolete. While academic articles often avoid titles, they are fairly common in popular writing. You might notice in a book you are reading or a news article, graphs typically come with a quick title. Like on this page, I used a title because it is a web-article. Note the key difference is that titles are generally going to be short, while figure captions contain a lot of detail.\nA figure caption should follow the general format:\nFigure 1. Statement describing the figure briefly. Important stats for the figure. Another important detail.\nCaptions should be terse and informative. Every figure should be “stand-alone”. This means that you should include enough detail where someone who hasn’t read the paper could glance at the figure and understand what is going on. This doesn’t mean you need to write an entire methods and results, but provide some context. If you are looking for a good example, just go to google scholar and search for some topic you are interested in. Take a look at how they format their figures in published, professional work. Also note figure captions go below the figure.\n\n\nMaking tables:\nYou can also provide a table to summarize data in a quick way. Tables are dangerous as people often want to fill them with content which is unnecessary. That said, a well-made table can really improve a manuscript. When making your tables, a key difference is by convention, table captions go ontop, while figure captions go on the bottom. They are also counted separately, (Figure 1)…(Figure 2)…(Table 1)…(Figure 3), etc.\n\n\nSome Figure Don’ts:\n\nDo not include a title unless appropriate\nDo not start your figure captions by stating “Figure 1 shows….” or “As seen in this figure….”.\n\n\n\n\nResults Statements:\nWhen writing your results, you are describing your data and key findings in words. Note you shouldn’t simply regurgitate all your information in a lengthy list. You want to present the data in a narrative form. The results should include references to your figures and statistics when appropriate. When reporting statistics, make sure to include not just the p-values, but also effect sizes as that is what we are typically interested. I will even accept reporting of confidence intervals rather than p-values.\nHowever, a common pitfall many beginning scientific writers fall into is just stating their stats/ figures.\nFor example:\n\nFigure 1 shows a clear pattern between the observed groups. The first group was larger than the second. A t-test revealed a significant difference (p < 0.001).\n\nThat’s not very easy to read and does not clearly show that you, the writer, understand what you are saying. Think about it as describing what happened (past tense). For example:\n\nThe first group’s average body weight (##kg) was significantly larger than the second group’s average body weight (##kg) (Figure 1, p < 0.001, t-test).\n\nNote in this second case demonstrates a lot more information. We narrate the findings and put supporting details in parentheses. The results should narrate both significant (from a statistical test) results, and interesting trends. For example, if you plotted a regression, with a ton of variation around the linear model, you might say “While there was a significant relationship between x and y (Figure 1, linear model, p-value = 0.002, \\(R^2\\) = 0.24), there was extreme variation in the y variable at high values of x (Figure 1).”\nAnother commonly discussed theme when writing results is to “not interpret the results in the discussion.” This phrase, if you’ve heard it before, is suggesting that you shouldn’t evaluate the hypotheses in the results section. That means, don’t say “Clearly, the hypothesis was not supported” in the results, save that for the discussion.\n\n\nDiscussion Statement:\nThe discussion should evaluate results in context of your original hypotheses and other general ecological ideas. You should suggest alternative hypotheses to explain your data or unexpected trends. Note that this should always be in the context of your results.\nFor example, you should never just state “Human error counting may have skewed results”. Such a statement doesn’t add any information.\nInstead, you might write “While there was a large difference between the two forested regions, there was also huge variation within the sub-plots. Such variation might be natural, however differences between student’s accuracy in counting may have contributed. Yet, Smith et al. (2021) observed similar levels of variation between closely-located soil sites. Thus, it is likely the variation is a real signal.” That text is all made-up but should convey a general idea as to how these things should be written.\nIn a full paper, the discussion should utilize prior research as well, citing relevant sources. For our worksheets, I am mainly expecting a discussion of the current research. Thus, citations are not necessary (for worksheets), just a thoughtful consideration of the sampling design, data, and alternative explanations.\n\n\nGeneral Writing Style:\nWhen writing scientifically, we typically take a slightly different tack than when writing for other purposes. Scientific writing often utilizes the passive voice. This is where the direct object takes the focus of the sentence and the subject (the one doing the action) is less emphasized. For example:\n“The samples were collected in a randomized grid format”\nRather than”\n“We counted using a randomized grid format”\nThat said, many traditionalists will argue that you should never use the first person when writing scientifically. I completely disagree. There are appropriate times to use difference tenses, voices, and persons when writing scientifically. Generally, while methods are written passively, an active voice is useful when describing a subjective decision made by the researchers. For example:\n“While samples were collected along quadrats placed systematically in intervals along a transect, we excluded plots which had clearly been altered by evil fire-monsters as they were outside the scope of this investigation. In such a case, the plot was omitted from the dataset.”\nFinally, there are levels to writing well. And largely, it comes with practice and revision. I typically write things very poorly on my first pass, then have to self-revise for clarity. When you are writing however, I encourage you to think about how to write briefly, yet communicate all needed information. Also, while there are certain expectations in the rubric, you should not need to write them explicitly (all the time). For example:\n“An alternative hypothesis would be that squirrels just a lazy losers.”\nThat type of sentence does not actually enhance the writing, it just shows you attempted to meet the requirement. Instead:\n“While the squirrels observed in this study were largely resting, it may have been that this is a diel pattern. Notably, it was extremely hot outside. Possibly, squirrel activity varies throughout the day and this variation was not captured in our sampling design. To fully investigate the hypothesis, squirrels ought to be observed at multiple points throughout the day.”\n\n\nGeneralized Rubric for Common Elements:\n\n\n\n\n\n\n\n\n\n\n\nCategory\nInsufficient\n(<60)\nMajor Revisions\n(60-75)\nMinor Revisions\n(75-85)\nMeets Expectations\n(85-90)\nExemplary\n(90+)\n\n\n\n\nIntroduction/\nBackground\nBackground is missing literature or not appropriately cited. Content is confusing and does not follow a logical progression. Writing displays a lack of direction or understanding.\nBackground is present but cited studies are not relevant. Some information may be slightly inaccurate or not well described. Detail is lacking overall.\nThe introduction is lacking in some small aspect. Literature may be present yet not well placed or in context. Funnel structure is wobbly. Hypotheses do not clearly come from background.\nThe introduction utilizes 3+ published studies to have background. Major ecological concepts are briefly discussed then transitioned to study specifics. Some inconsistencies may be present.\nThe introduction incorporates a comprehensive review of major hypotheses utilizing literature to establish state of knowledge on topic. Introduces study system and narrowly focuses into hypotheses\n\n\nHypotheses\nSeveral potential issues:\nOnly predictions are listed,\nhypotheses are incoherent,\nWriting is unclear to the point of complete confusion\nHypotheses may be present but predictions are lacking or unclear. Hypotheses are clearly untestable. other major issues with study ideas.\nHypotheses and predictions are listed, although they may be lacking a small component. There may be some mismatch between hypotheses & predictions\nHypotheses are clearly stated with predictions. Predictions are testable and match study design\nHypotheses are clearly stated and logical extensions from background. Hypotheses have direct predictions which can be logically derived from them into a testable study.\n\n\nMethods\nMethods are not well thought-out and clearly do not relate to hypotheses or context of study.\nMethods describe collection and analyses but have a fundamental flaw which compromises the study design. Or massively unclear how data may be connected to the study.\nMethods have data collection and analysis described. There may be some unclear sections. Study design may have some potential flaws with analyses.\nMethods are described with data collection and analysis well described. Some detail may be missing about exactly why/how a procedure was/will be done.\nThe methods are well thought-out. Described data collection and analyses methods are consistent with a goal of directly testing the hypotheses. A clear, comprehensive understanding of data is displayed\n\n\nFigures\nFigures do not accurately show the data. There is a clear issue with how information is presented.\nFigures are missing major elements. Or figure does not have an appropriate use of showing data. Confusing or unrelated to the project\nData are well summarized however some minor element may be missing. Figures are still coherent\nData are well summarized, all captions, axes titles, etc are present. Figures are able to be interpreted without context\nFigures are clear, creative, and aesthetically thoughtful. Data are well summarized, all captions, axes titles, etc are present.\n\n\nResults Statements\nResults statement is incorrect in its description of data\nSeveral potential issues may have occurred:\nResults do not utilize correct analyses\nResults do not correctly reference figures\nResults are not detailed enough in the description of data\nResults have too much or irrelevant detail which distracts from an overall message\nResults list key findings and statistics however, may be lacking in a complete description or missing minor elements.\nResults correctly utilize statistics and figures. Statistics are reported using appropriate metrics and effect sizes. Statements are clear and correct.\nResults are brief, yet informative. Statistics are correctly utilized and described well in statements. Results is an easy to follow narrative format and highlights key findings accurately.\n\n\nDiscussion Statements\nresults are incorrectly interpretted. Outside studies are not utilized or utilized incorrectly. Overall discussion is severely limited.\nHypotheses are evaluated, yet there is a major disconnect between results and discussion or lacking in key areas.\nDiscussion meets minimum requirements of hypothesis evaluation and connection to other studies. Yet is limited in the overall discussion of ideas.\nDiscussion evaluates results & original hypotheses. Makes some connections to other studies and other potential ideas.\nThe discussion is a comprehensive evaluation of the results from this study. Makes clear connections to other studies both the place results in context but also evaluate alternative trends/ideas.\n\n\nGeneral Quality\nDemonstrates a lack of effort, or confusing at multiple points to the stage of difficult to understand.\nConfusing in some sections. Writing is choppy or does not follow scientific standards\nWriting is clear generally but in some sections lacking or inconsistent.\nWriting is clear, may be constrained at points but consistently accurate throughout.\nWriting is quality and scientific. It is natural to read, clear, and demonstrates a thorough understanding of ideas.\n\n\n\n\n\nSubmitting Assignments for Revision:\nNow that this is clearly available, I am going to increase the harshness with which I grade. However, I don’t want people getting caught up on assignments as we progress through the semester. Ultimately the goal of the worksheets and the project proposal is for you to learn how to go about thinking, working, and writing like a professional ecologist. Over the next several worksheets, I will grade with detailed comments and feedback, according to the general rubric above. However, I really encourage you to learn from these assignments, not just get penalized for not having done it.\nSo for the worksheets, I am going to allow you to submit revisions.\nThe project plan will also be eligible for revisions.\nRevised submissions will be eligible to receive 66.66% of the lost points on regrading. For example, if you received a 38/50, you lost 12 points. If you fully addressed all concerns in your revision, you can get 8 points back. I settled on this exact percentage as it allows most people to improve your grades\nA few things to note:\n\nAssignment 1 was not graded to the new standards described here. Mainly the figure captions were absent and I did not grade for those. However, revised submissions will be expected to be completed according to details described above. Yet, I will allow assignment 1 to have 100% revision credit because it will require a higher-bar than the initial expectations.\nAssignment 2 while technically not due until after this page was written, it was assigned before. So while I will grade it following these standards, assignment 2 is eligible for 100% revision credit as well.\nAll worksheet revisions are due by the end of fall break (10/19). Please send me an email if you submitted a revision you would like to have graded.\nRevisions are only eligible for a singular submission. No multiple re-submits.\nProject plan revisions will be more flexible depending on individual situations"
  },
  {
    "objectID": "gen_R-guide-plotting.html",
    "href": "gen_R-guide-plotting.html",
    "title": "A guide to making basic plots in R",
    "section": "",
    "text": "Making good plots requires researchers to be well-informed about ways to best communicate their data. Below, I’ll provide a quick guide for making plots for different types of data. Like most of this course, this information is far from exhaustive. Yet, it might provide a good launching pad to explore ideas. Other good resources for making nice plots are the R graph gallery, and asking ChatGPT.\nIn this example, I’ll use ggplot2, so if you are interested in recreating these figures in your own machine, make sure to load the package. However, I’ll also display how the plot() function in R allows for versitile and quick plotting. This can be used to get a quick idea about how to best plot your data. We’ll also use dplyr"
  },
  {
    "objectID": "gen_R-guide-plotting.html#the-simplest-form",
    "href": "gen_R-guide-plotting.html#the-simplest-form",
    "title": "A guide to making basic plots in R",
    "section": "The simplest form:",
    "text": "The simplest form:\n\nBase R plotting\nPlotting in base R allows for a similar format to many of the functions you’ll see in the stats package where you can use what I call “formula” structure. That is, rather than specifying the x and y axis, you can specify a relationship with ~.\n\nplot(Petal.Width ~ Petal.Length, data = iris,\n     xlab = 'Petal Length [mm]',\n     ylab = 'Petal Width [mm]')\n\n\n\n# The same plot could be achieved with\n# plot(iris$Petal.Width ~ iris$Petal.Length)\n# plot(y = iris$Petal.Width, x = iris$Petal.Length)\n\n\n\nggplot approach\nplotting in ggplot may seem a little more complicated. However, in the long-run it facilitates better looking graphics with easier-to-read code. The fundamental idea behind ggplot is using geoms, which are plotting objects (called by functions) to make a particular type of plot. In a geom function, there is a similar layout where plotting objects can be called using aes() where users specify x and y values. Similar to base R plotting, the data argument can be utilized to avoid specifying each value with $. Learning ggplot can be particularly tricky because of the range of ways to format the code and produce the same plot. I have very particular reasons for my preferred method but I won’t divulge them all here.\nAnother great feature of ggplot is the built in theme functions. These can allow you to quickly clean up the plot an make the features all look the same! In this guide, I’ll rely on theme_bw(). However, in most my work, I use theme_pubr() from the ggpubr package, with customization in theme(). Explore around with the themes for your own purposes!\n\nggplot(data = iris)+\n  geom_point(aes(x = Petal.Length, y=Petal.Width))+\n  labs(x = 'Petal Length [mm]', y = 'Petal Width [mm]') +\n  theme_bw()\n\n\n\n# same as:\n# ggplot(data = iris, aes(x = Petal.Length, y = Petal.Width))"
  },
  {
    "objectID": "gen_R-guide-plotting.html#adding-to-the-scatterplot",
    "href": "gen_R-guide-plotting.html#adding-to-the-scatterplot",
    "title": "A guide to making basic plots in R",
    "section": "Adding to the scatterplot",
    "text": "Adding to the scatterplot\n\nTrendlines:\nScatterplots can be greatly improved by adding features. For example, we might want to add a trendline to the figure. These can be useful for demonstrating a linear replationship between the variables.\nIn baseR, we need to use the abline() function. This function requires you to specify the linear relationship. Fortunately, we can just feed it a model object. Here, I use lm() inside the abline function. However, if you defined your linear model elsewhere, you can put that object in place.\n\nplot(Petal.Width ~ Petal.Length, data = iris,\n     xlab = 'Petal Length [mm]',\n     ylab = 'Petal Width [mm]')\nabline(lm(Petal.Width ~ Petal.Length, data = iris))\n\n\n\n\nPersonally I find ggplot a bit more flexible for making trend lines. We can use geom_smooth() or stat_smooth(). By default, this function will create a smoothed trend line (the exact smoothing default varies based on the data):\n\nggplot(data = iris)+\n  geom_point(aes(x = Petal.Length, y=Petal.Width))+\n  geom_smooth(aes(x = Petal.Length, y=Petal.Width))+\n  labs(x = 'Petal Length [mm]', y = 'Petal Width [mm]') +\n  theme_bw()\n\n\n\n\nHowever, we can change the smoothing method to fit whatever trendline we want. Here, we can use ‘lm’. I also usually like to get rid of the error line with se = F:\n\nggplot(data = iris)+\n  geom_point(aes(x = Petal.Length, y=Petal.Width))+\n  geom_smooth(aes(x = Petal.Length, y=Petal.Width),\n              method = 'lm', se = F)+\n  labs(x = 'Petal Length [mm]', y = 'Petal Width [mm]') +\n  theme_bw()\n\n\n\n\n\n\nColoring by group:\nColor can be a great tool to add to scatterplots to give context to the data or display a third dimension.\nWe can first look at grouping by species:\nIn base r we can use the col argument. I’m also going to add a legend but it is a bit trick to do this in base R:\n\nplot(Petal.Width ~ Petal.Length, \n     col = Species,\n     data = iris,\n     xlab = 'Petal Length [mm]',\n     ylab = 'Petal Width [mm]')\nlegend('bottomright', legend = unique(iris$Species),\n       col = c('black','red', 'green'), pch = c(20))\nabline(lm(Petal.Width ~ Petal.Length, data = iris))\n\n\n\n\nThe above plot is OK, but we’re starting to hit the wall of base R graphics. In ggplot, we can make the same figure but we can add some better features. For example, group-specific trend lines!\n\nggplot(data = iris,\n       aes(x = Petal.Length, y=Petal.Width,\n                 color = Species))+\n  geom_point()+\n  geom_smooth(method = 'lm', se = F)+\n  labs(x = 'Petal Length [mm]', y = 'Petal Width [mm]') +\n  theme_bw()\n\n\n\n\n\n\nColor by a continuous factor:\nWe could also color by a continuous variable. In the iris dataset, this is not the most useful approach. However, I’ll demonstrate here with coloring by Sepal.Width. I’ll also change the color scale to make it more visible using the scale_color_diverge() function.\n\nggplot(data = iris,\n       aes(x = Petal.Length, y=Petal.Width))+\n  geom_point(aes(color = Sepal.Width))+\n  geom_smooth(method = 'lm', se = F)+\n  scale_color_gradient(low = 'grey', high = 'black')+\n  labs(x = 'Petal Length [mm]', y = 'Petal Width [mm]') +\n  theme_bw()\n\n\n\n\n\n\nMore factors!\nWe could also use size to communicate a fourth dimension. Again, the iris dataset may not really require this feature. However, I’ll use the Sepal.Length as a bubbling element for demonstration purposes. If you are paying attention, you’ll notice I’m moving around where I assign the aesthetic mappings (aes()). I also added a transparency value to the points (alpha).\n\nggplot(data = iris,\n       aes(x = Petal.Length, y=Petal.Width))+\n  geom_point(alpha = 0.7, aes(size = Sepal.Length,\n                              color = Sepal.Width))+\n  geom_smooth(method = 'lm', se = F)+\n  scale_color_gradient(low = 'grey', high = 'black')+\n  labs(x = 'Petal Length [mm]', y = 'Petal Width [mm]') +\n  theme_bw()\n\n\n\n\n\n\nCorrelogram\nCorrelograms are great ways to assess multiple relationships at once. There’s some great packages to make nice figures for theses. However, base R offers a quick way to assess multiple relationships at once. Here we can use the first four columns of the iris dataset to see how each individual variable is related to one another.\n\nplot(iris[,1:4])\n\n\n\n\nWe can calculate the correlation matrix for each of those values using cor()\n\ncor(iris[,1:4])\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\nSepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\nPetal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\nPetal.Width     0.8179411  -0.3661259    0.9628654   1.0000000"
  },
  {
    "objectID": "gen_R-guide-plotting.html#summarizing-the-data",
    "href": "gen_R-guide-plotting.html#summarizing-the-data",
    "title": "A guide to making basic plots in R",
    "section": "Summarizing the data",
    "text": "Summarizing the data\nAs discussed in the general expectations, a good plot should offer a summary of the data. So here, we can average the chick weights to show a better summary of the data.\n\nSummary by all chicks\n\nall_chicks <- ChickWeight |>\n  group_by(Time = Time) |> \n  summarize(mean_weight = mean(weight),\n            se_weight = sd(weight)/sqrt(nrow(ChickWeight)))\n\nNow we can use that data to make a plot of the average chick growth across time, regardless of diet:\nIn Base R:\nfor the base r case, I’m going to use both points with lines that way we can show the standard error\n\nplot(mean_weight ~ Time, all_chicks,\n     type = 'b',\n     xlab = 'Days Since Hatching',\n     ylab = 'Mean Weight [g]')\narrows(x0 = all_chicks$Time, \n       x = all_chicks$Time, \n       y0 = all_chicks$mean_weight - all_chicks$se_weight,\n       y = all_chicks$mean_weight + all_chicks$se_weight,\n       angle = 90, length = 0.075, code = 3)\n\n\n\n\nIn ggplot:\nThere’s two nice ways to make this figure in ggplot. We can use the point-and-line method as shown above or we can use ribbons for standard error.\n\nggplot(all_chicks,\n       aes(x = Time, y = mean_weight))+\n  geom_point()+\n  geom_errorbar(aes(ymin = mean_weight - se_weight,\n                    ymax = mean_weight + se_weight))+\n  geom_line()+\n  labs(x = \"Days Since Hatching\",\n       y = 'Mean Weight [g]')+\n  theme_bw()\n\n\n\n\n\nggplot(all_chicks,\n       aes(x = Time, y = mean_weight))+\n  geom_line(size = 1)+\n  geom_ribbon(aes(ymin = mean_weight - se_weight,\n                  ymax = mean_weight + se_weight),\n              size = 1,\n              fill = 'grey', alpha = 0.7)+\n  labs(x = \"Days Since Hatching\",\n       y = 'Mean Weight [g]')+\n  theme_bw()\n\n\n\n\n\n\nSummary by diet\nThe nature of this dataset really implies we should be investigating how growth varied based on diet. So let’s look at the data grouped by diet. I’m only going to do this in ggplot at this point and jump to the final figure, but we build on the figures above!\n\nchicks_by_diet <- ChickWeight |>\n  group_by(Time = Time, diet = Diet) |> \n  summarize(mean_weight = mean(weight),\n            se_weight = sd(weight)/sqrt(nrow(ChickWeight)))\n\nFor this ggplot, I’ll just use the ribbon approach. Pay attention to the details here. Note that lines require color arguments while ribbons require fill.\n\nggplot(chicks_by_diet,\n       aes(x = Time, y = mean_weight))+\n  geom_line(size = 1, aes(color = diet))+\n  geom_ribbon(aes(ymin = mean_weight - se_weight,\n                  ymax = mean_weight + se_weight,\n                  fill = diet),\n              size = 1, alpha = 0.25)+\n  labs(x = \"Days Since Hatching\",\n       y = 'Mean Weight [g]',\n       fill = 'Diet', color = \"Diet\")+\n  theme_bw()"
  },
  {
    "objectID": "gen_R-guide-plotting.html#a-complex-example",
    "href": "gen_R-guide-plotting.html#a-complex-example",
    "title": "A guide to making basic plots in R",
    "section": "A complex example:",
    "text": "A complex example:\nLet’s take this all one step further. While it may be nice to show summary data, it can be fun to have each individual line shown as well. This is effectively the “raw” approach but we can clean it up a bit by playing around with transparency values:\n\nggplot()+\n  geom_line(data = chicks_by_diet,\n            aes(x = Time, y = mean_weight,\n                color = diet),\n            size = 1) +\n  geom_line(data = ChickWeight,\n            aes(x = Time, y = weight,\n                color = Diet, group = Chick),\n            alpha = 0.15)+\n  labs(x = \"Days Since Hatching\",\n       y = 'Mean Weight [g]',\n       fill = 'Diet', color = \"Diet\")+\n  theme_bw()\n\n\n\n\nI don’t think this plot is as effective as the summary ones, however, it is fun to make. This type of figure might be useful to display simulation results."
  },
  {
    "objectID": "gen_R-guide-stats.html",
    "href": "gen_R-guide-stats.html",
    "title": "A starter guide to analyzing data in R",
    "section": "",
    "text": "There is a wide, wide, wide, range of ways to analyze data. Here, I hope to provide a starting tool kit for you to explore your data with. However, keep in mind you should always take extended precaution when using an analysis to ensure that you are checking your assumptiosn"
  },
  {
    "objectID": "gen_R-guide-stats.html#anova",
    "href": "gen_R-guide-stats.html#anova",
    "title": "A starter guide to analyzing data in R",
    "section": "ANOVA",
    "text": "ANOVA"
  },
  {
    "objectID": "gen_R-guide-stats.html#simple-linear-regression",
    "href": "gen_R-guide-stats.html#simple-linear-regression",
    "title": "A starter guide to analyzing data in R",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression"
  },
  {
    "objectID": "gen_R-guide-stats.html#multiple-linear-regression",
    "href": "gen_R-guide-stats.html#multiple-linear-regression",
    "title": "A starter guide to analyzing data in R",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression"
  },
  {
    "objectID": "gen_R-guide-stats.html#logistic-regression",
    "href": "gen_R-guide-stats.html#logistic-regression",
    "title": "A starter guide to analyzing data in R",
    "section": "Logistic Regression",
    "text": "Logistic Regression"
  },
  {
    "objectID": "gen_R-guide-stats.html#nmds",
    "href": "gen_R-guide-stats.html#nmds",
    "title": "A starter guide to analyzing data in R",
    "section": "NMDS",
    "text": "NMDS"
  },
  {
    "objectID": "gen_R-guide-stats.html#clustering",
    "href": "gen_R-guide-stats.html#clustering",
    "title": "A starter guide to analyzing data in R",
    "section": "Clustering",
    "text": "Clustering"
  },
  {
    "objectID": "gen_R-guide-stats.html#pca",
    "href": "gen_R-guide-stats.html#pca",
    "title": "A starter guide to analyzing data in R",
    "section": "PCA",
    "text": "PCA"
  },
  {
    "objectID": "gen_stats-guide.html",
    "href": "gen_stats-guide.html",
    "title": "570L Fall 2023",
    "section": "",
    "text": "As ecologists, we often are interested in testing hypotheses about some ecological relationship, prediction, or theory. Generally, our hypotheses may relate to some wild population, large ecosystem, or a broad-scale relationship (e.g. a predicted relationship from a theory; like allometric scaling or survival under different conditions, etc). However, it is typically impossible to survey the entire group of interest. In statistics, we define the entire group of interest as our “population” or the “population-level”. Note that this may be slightly different than the ecological definition of population as a group of same-species organisms in an ecosystem. For the purposes of this article, I’ll use “population” in the statistical sense.\nLet’s say we were interested in whether or not campus squirrels were fatter than squirrels from Harbison forest. To test this prediction, we’d need to collect some campus squirrels and some forest squirrels, get a body-fat composition analysis and compare them. Well, in this example, our “population-level” interest is all the squirrels in the forest and all the squirrels on campus. What specifically we are interested in is the difference between mean campus body fat and mean forest body fat. In statistical notation, this would be written as: \\(\\mu_{c} - \\mu_{f}\\). Where c and f denote campus and forest. The term \\(\\mu\\) is common for population-level mean. Any metric of interest at the population-level, we can call a parameter. However, it would be impossible to capture every single squirrel. So we can’t actually measure our parameter of interest. Instead we have to take a sample from the population. Let’s say we collect 5 squirrels each from the forest and campus. Well now we could calculate the mean of our sample. The metric calculated at the sample-level, is referred to as a statistic. In our case that would be the sample mean of campus and forest squirrels (in statistics notation this is either \\(\\bar{x}_c - \\bar{x}_f\\) or \\(\\hat{\\mu}_c - \\hat{\\mu}_f\\)). Well, how can we know if our statistics are actually representative of the true population parameter of interest? That is where inferential statistical tests enter the picture. At the most broad level, statistical analyses give us the ability to discuss how confident we are that our samples are representative of reality (the population-level parameters).\n\nIn this document, I’ll briefly summarize some basics of data types, how to plot them, and a basic tool kit for statistically testing your data. This is by no means an exhaustive list, but hopefully a good starting point. Additionally, in this article I’ll try to provide a little bit more detail for those who are interested. Don’t feel the need to deeply understand everything. However, please read through the whole article. A common issue in applying statistics to ecological data, is that ecologists fall into traps of how to correctly understand and interpret the results of a test. To avoid these traps, we need to understand the basics of common approaches, then why they might be misleading when interpreting them.\nLike most things, learning how and when to apply a given test, comes with experience. So I hope you are able to refer back while working on independent projects.\nLet’s recap some basic terms:\nPopulation - in a statistical sense, this is the entire group of interest (could be multiple, or less, biological populations)\nParameter - a metric which describes a feature at the population level\nSample - a sub-group of the population\nStatistic - a metric describing the sample-level, which can be used to make inferences about the population."
  },
  {
    "objectID": "gen_stats-guide.html#caveats-with-common-approaches",
    "href": "gen_stats-guide.html#caveats-with-common-approaches",
    "title": "570L Fall 2023",
    "section": "Caveats with common approaches:",
    "text": "Caveats with common approaches:\nMany statistical tests have a set of assumptions. You should evaluate those assumptions prior to applying a test. Plenty of papers get published without evaluation of the assumptions yet it is best to know thel limitations of different data analysis methods."
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "Fall 2023 570L - Ecology Lab",
    "section": "About this course:",
    "text": "About this course:\nThis is an independent, one-credit lab course for upper division students interested in ecology. The primary goal of this course is for students to develop independent ecological investigations which they will complete over the course of the semester. However, there will also be several assigned labs at the beginning of the course to introduce students to basic ecology field techniques."
  },
  {
    "objectID": "lab_0_intro_assignment.html",
    "href": "lab_0_intro_assignment.html",
    "title": "Introductory Assignment",
    "section": "",
    "text": "Submit all answers as a pdf on blackboard’s submission portal.\n\nPart 1: Background about you (5 points):\n\n1.a What made you interested in this course?\n\n\n1.b What skills do you hope to gain through this lab?\n\n\n1.c What are your career goals?\n\n\n\nPart 2: Intro to R (5 points):\nTo get some basic familiarity with using R, I want you to make a graph.\nGraph fiddler crab carapace width by two sites. The graph should be appropriate for the type of data and display the mean crab carapace width by site. Standard error bars should be available as well.\nYou can download fiddler crab data which I formatted here. The data originally comes from the LTER’s educational resources page. Just click the link, right-click (or command/control S) and save the data as a .csv file. Note you should be able to open the .csv file using excel.\n\nBONUS (5 points):\nRemake the plot but this time include all sites, not just the two. hint: this can be achieved by removing one line of dplyr code."
  },
  {
    "objectID": "lab_1_behavioral-ecology.html",
    "href": "lab_1_behavioral-ecology.html",
    "title": "Behavioral & Urban Ecology",
    "section": "",
    "text": "When thinking of studying ecology, may people think we need travel to national parks, undisturbed areas, or wild places. However, we are surrounded by ecology. All interactions which govern our lives and the environments in which we live can be studied with an ecological lens. Generally, urbanization has negatively impacted biodiversity and native ecosystems. However, within these areas, there are several organisms which, while traditionally evolved to live in other areas, have developed strategies to survive and thrive. An organism around USC’s campus which seemingly thrives is the squirrels (most likely eastern grey squirrels, Sciurus carolinensis).\nFun Squirrel Information:\n\nhttps://sc.edu/uofsc/posts/2021/10/remembering_the_days_natural_history_stroll_ep_35.php\nhttps://www.instagram.com/usc.squirrels/?hl=en\nhttps://www.facebook.com/watch/?v=499139211340770\nhttps://www.tiktok.com/@uofsc/video/7191269652230720814 (not available on campus wifi due to tiktok restrictions despite this is university’s account)\n\nIn this lab, we will study movement and behavior of squirrels on the USC campus. The primary objective will be to introduce students to conducting basic ecological fieldwork in a convenient location. However, don’t take this lab for granted! There are plenty of opportunities to develop an independent project utilizing nearby spaces on campus. Try to be curious of your surroundings!\nStudying organism behavior may seem fairly straightforward. However, it can be extremely challenging to do so with robust scientific methods. A basic tool in behavioral ecology research is to use an ethogram. This is a table which clearly defines possible actions to observe in an individual’s behavior.\nIn this lab we will construct ethograms to test a hypothesis about squirrel activity.\nDownload the full project plan here\nDownload the worksheet here"
  },
  {
    "objectID": "lab_2_forest-biodiversity.html",
    "href": "lab_2_forest-biodiversity.html",
    "title": "Forest Biodiversity",
    "section": "",
    "text": "Download the project plan here\nDownload the worksheet here\n\nLab Context\nFor this lab, we will be evaluating how biodiversity varies based on different disturbance regimes. A classic concept in ecology is the intermediate disturbance hypothesis. This concept was formulated by Connell in 1978. The hypothesis is an extension of the concept of ecological succession. Succession theory suggests that communities, in a new environment, will develop following a predictable pattern in which the first organisms are colonizing ones (fast-life histories, r-selected individuals). These taxa are quick to reproduce and establish a population. Then, as resources develop in the environment, longer living yet better competing individuals will move into the area. Thus the community will slowly become dominated by organisms which have slow-life history strategies (K-selected). This concept is a persistent idea in ecology and was developed in coastal dunes [1], freshwater streams [2], and carrion studies [3]. Yet is has been extended into many other areas of ecology, including plankton (see Margalef’s Mandala).\nSo, how does this lead to the intermediate disturbance hypothesis? Well, what “resets” an environment is disturbances (wildfires, storms, waves, etc.). So the intermediate disturbance hypothesis posits that if disturbances are too frequent, then only early successional species will be present. However, if undisturbed, the community will move towards a late-stage successional community equilibrium. If there are “intermediate” levels of disturbance. Then we might expect higher levels of biodiversity as it sustains a mixture of community types.\n\nIn this lab we will test the intermediate disturbance hypothesis by sampling thre regions in Harbison forest which correspond to different levels of disturbance. We’ll have a low-disturbance area which has been essentially undisturbed. Then we’ll have a high disturbance region which is consistently mowed. Finally, we’ll have a mid-disturbance area which was subjected to a burn 1 year ago.\n\n\nMeasuring biodiversity\nThere are several metrics to evaluate biodiversity. In this lab we will calculate a few of the most common ones. For your worksheets you will only be required to select one of them.\nA straightforward metric is Species Richness (\\(R\\)). This is the number of unique species in a region. \\[\nR = \\# of\\_unique\\_taxa\n\\] However, this is not the most informative metric as it does not account for the number of taxa there are of each species. For example, if there are 100 different species, yet 99% of them were one type, the area wouldn’t be very biodiverse. Species richness alone is not very informative in such cases.\nAlternatively species diversity can be measured through a variety of indices. Arguable the most common is the Shannon-Wiener Index (\\(H\\)). This ranges from 0 (a low diversity) to larger numbers. The larger the \\(H\\), the more diverse the community is.\n\\[\nH = -\\sum_{i = 1}^{R}{p_i lnp_i}\n\\] Here, for each unique taxa, we add up the proportion (\\(p\\)) of each (\\(i^{th}\\)) taxa, multiplied by the natural log of that taxa. We do that for all unique taxa (\\(R\\)) For count data, we can extend the formula:\n\\[\nH = -\\sum_{i = 1}^{R}{\\frac{n_i}{N}ln\\frac{n_i}{N}}\n\\]\nHere, \\(n_i\\) is the count of the specific taxa while \\(N\\) is the total count of taxa.\nWhile \\(H\\) provides a nice measure of biodiversiy, it is still related to the overall richness. An alternative metric is to measure evenness. This will report the how spread across the different taxa the relative abundace is. For example, if we had 5 individuals each of two taxa, it would be more “even” than 1 of taxa A and 9 of taxa B. This metric is independent of the overall richness and give a perspective to the relative success of the taxa which are present. The more common metric for evenness is Pileou’s (\\(D\\)):\n\\[\nD = 1 - \\sum_{i=1}^{R}{\\frac{n_i(n_i-1)}{N(N - 1)}}\n\\]\n\n\nQuantitative Sampling\nA large challenge for ecologists who conduct field studies is how to collect quantitative, unbiased estimates of population abundance, community structure, or other ecologically relevant metrics. Here we want to collect biodiversity metrics in four forest regions.\nThere are several common field sampling techniques:\nSome basic tools:\n\nQuadrat (basically a square, typically of PVC)\n\nThese are useful for defining a set sampling area. They give us the ability to quantify density (count per area) which then standardizes our metrics across regions\n\nTransect\n\nA fixed line which we go and sample along. Sampling along a transect can be done with point counts (whatever is touching the transect), line-quadrat methods (placing quadrats systematically or randomly along the transect), or band-transect methods (counting everything within a fixed width of the transect).\n\n\nWhen deploying these tools, there are several ways we can try to reduce bias. We want to have a fixed way to sample. Some common methods are:\n\nHaphazard sampling\n\nThis is just sampling when we encounter our study target. This is inherently biased although it is sometimes necessary when we are interested in highly mobile or elusive organisms. It also could be that we are interested in particular taxa so we just observe those. This is what we did for our Behavioral Ecology Lab last week\n\nSystematic Sampling\n\nHere, samples are collected at a fixed interval (say every 2m along a transect or in set regions of a grid). This allows us to sub-sample a region. This can be particularly useful for measuring change along a gradient.\n\nRandom Sampling\n\nHere, random coordinates are selected in a study area (grid or transect). This ensures a non-bias sample of our study region. However, we should be cautious if there is confounding variables in the area. This is what we will do this week for the intermediate disturbance hypothesis lab.\n\nStratified-Random Sampling\n\nIf we are sampling an area with known variation (differences in moisture, elevation, etc), we can create strata. These are set sub-regions to sample. Here we can then generate random coordinates within each strata for a stratified, but still random approach.\n\n\nWhen we construct our sampling design some key considerations are: How can we be unbias our samples? What are we measuring? If it is just the overall area, random sampling may be sufficient. However if we believe there is variation within our study area we might want to use a transect with systematic sampling, or a stratified random design. These are all things you’ll want to consider when designing your sampling schemes.\n\n\nHarbison Forest Directions\nGo to the main entrance to Harbison Forest off broad river road. Note that some people turn early and go to the South Carolina Forestry Commission. Drive forward (past the first parking lot on the left) and keep left towards the gate (marked in blue). Drive straight down the gravel road. Shortly after the intersection for the Eagle Trail lot (you’ll see signs about a gazeebo, just go straight), you’ll see a parking lot on the left (marked by yellow X).\n\nIf you have problems getting there, you can call my cell 440-668-8376.\nDress for being outside! It will be warm but there are a lot of bugs and sharp plants. I always opt for covering up rather than being cool but it’s a personal trade-off. Also wear appropriate shoes!"
  },
  {
    "objectID": "lab_3_community-assemblages.html",
    "href": "lab_3_community-assemblages.html",
    "title": "Community Assemblages",
    "section": "",
    "text": "Download project plan here\nDownload worksheet here\n\nLab Context\nLast week we got the opportunity to look at forest communities in different stages of successional communities. However, we just looked at the different plants present in the area. Yet, the community structure of one portion of the environment can influence the entire ecosystem. A fairly long standing concept in ecology is the idea of foundational species. These are organisms whose presence defines the overall structure of an ecosystem. In forests, trees often can play a foundational role in an ecosystem. When thinking of trees, many people probably think of the animals that are associated with them. Birds, squirrels, insects all live in the upper portion of trees. However, trees also can influence the ecosystem by altering soil properties. Trees have root networks which support sub-surface communities of fungal networks, invertebrates, and bacteria. Additionally, their roots can create physical structure to the soil, altering the hydrological characteristics. Trees also can alter the soil chemistry through their leaves. As their leaves fall, they can change soil chemistry as they decompose.\nWe refer to these fallen leaves as the “leaf litter”. In the litter, there can be a diverse community of arthropods which thrive in the leaf litter and upper layer of the top soil. This week, we’ll be investigating which factor is the major factor in determining the soil invertebrate community: trees or abiotic factors.\nWe’ll be sampling up a hill, with a strong moisture gradient. We’ll start in a wetland and move to the top of a dry hill. Along this gradient, we’ll expect to see a difference difference in the soil characteristics as well as the primary trees which occupy this area.\n\n\nSampling Plan\nIn last weeks lab background I provided a brief overview different field sampling techniques. Last week we did a random-grid approach. That was the appropriate approach because we were interested in the average biodiversity in a given plot of land. However, this week, we’re interested in how things change along a gradient.\nSo we’ll want to use a transect-based approach. Because this week we’re looking at trees, which are relatively sparse, we’ll use a band transect approach. This allows us to characterize every tree and it’s exact position along the hill. We’ll identify and measure each within 1-m of either side of the transect (2-m bands). The starting position of the transects will be randomly assigned. These data will allow us to characterize changes in abundance of major trees, as well as the relative importance of each tree in regions of the hill.\nThen, to characterize the soil invertebrate community, we’ll want to collect samples along the hill. However, this is a little more tricky. Notably, we are interested in testing the hypothesis that trees alter the invertebrate community, more so than abiotic factors. However, the dominant tree species will likely change as we go up the hill. So we need account for the fact that species are confounded with the abiotic environment. To do this, we’ll take multiple invertebrate samples in distinct regions along the transect. The exact position of the samples will be collected haphazardly within distinct 10-m sections of the transect. One sample will be collected next to representative species of trees in that region of the transect. The sample will be collected right at the base. Then to account for soil-moisture variation, we’ll collect a sample which is located far-away from any tree. Thus, within each region we can compare litter community and account for variation due to the soil invertebrate community.\n\n\nMeeting location:\nWe’ll meet at the Gordon Belser Arboretum: see details here.\nThe address for the gate is 4080 Bloomwood Road. We it takes about 15 minutes to get there from campus. So let’s plan on getting there and starting lab at 3:15pm."
  },
  {
    "objectID": "proj_main.html",
    "href": "proj_main.html",
    "title": "Independent Projects",
    "section": "",
    "text": "The primary objective of this lab course will be for students to pursue a self-led investigation of an ecological hypothesis. Students must propose their own projects which they will pursue over the course of the semester.\n\nGeneral Guidelines:\n\nStudents may work in groups up to 3 members or individually\nProjects should be feasible, clearly organized, and interesting. A common pitfall is to create a project that is too complicated and unable to complete in the limited time of lab. A good experiment should be elegant!\nProjects will take the bulk of the semester, however there will be several milestones along the way to ensure progress.\nThe projects will incorporate ecological concepts learned throughout the semester.\n\n\n\nPhase 1: Idea generation\nThe first few weeks of lab will involve individual one-week labs. These will introduce students to field work and working in nearby ecosystems. During these labs students should take some time to think about the lab topic and their surroundings. Ask questions about the ecosystem and think of possible projects on these activities.\nAccess the project plan guide here\nAccess the project plan rubric here"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Instructor & Office Information\nInstructor: Alex Barth, AB93@email.sc.edu\nOffice: EWS 706\nOffice Hours: MW: 1pm - 2:30pm; F: 11am - 12pm; other times by appointment.\nI am generally in my office, if not there I will be in Josh Stone’s Lab in EWS 615. Please email me to ensure I’ll be present if you urgently need to see me!\n\n\nCourse Website:\nAll course documents will be shared via the course website: https://USC-Ecology-Lab.github.io\n\n\nCourse Description:\nThis is a 1 credit lab based course for ecological investigation. Students will be trained in basic field techniques and analyses common in ecology. A core portion of this course will be an independent investigation on an ecological topic selected by students.\n\n\nLearning Outcomes:\n\nUnderstand fundamental ecology sampling schemes\nUtilize appropriate statistical tools for analyzing ecological data\nAbility to design quality investigations to test hypotheses\nGain experience with communicating scientific results to peers\n\n\n\nCourse Technology:\nLab activities will included data analysis in the R programming language. While this course will rely heavily on R, prior knowledge is not expected. Students should have access to both R and Rstudio. See here for download information. Data, documents, and code for this course will all be hosted on github. Git and GitHub will not be taught yet it is a great tool for any scientist to be familiar with. Interested people should look more here for a friendly introduction.\nFor independent projects, students are welcome to use whichever software they are most productive in. However R is encouraged.\n\n\nOutdoor activity & safety policy:\nEcology research often requires outdoor data collections in variable conditions and environments. This course is no exception. There will be multiple field trips to nearby areas. Students are expected to consult the pre-lab content and arrive prepared for weather and lab conditions. It is expected that students, to the best of their ability, will engage in data collection during labs. However, please contact be if you feel uncomfortable or unsafe with any lab activity.\n\n\nTransportation:\nFor labs which are not initially meeting at the lab space (Week 2,3,4). You can either meet at the lab space, or at the specified location. Meeting time will vary based on location. Transportation will be available to any students who don’t have access. Carpooling is encouraged.\nAll field trips will end with enough time for students to return to campus by the specified end time of lab.\n\n\nGrades:\nThis is a one credit lab based course. The majority of student grades will be centered on their independent project and lab-based worksheets. There is a total of 1000 points available. Point breakdowns are as follows:\n\nIntroduction Worksheet (10 points) : Prior to the start of lab, students must complete the introduction worksheet.\nLab-based worksheets (50 points each, 200 total): In the first weeks of lab students will complete field labs to test assigned hypotheses. These assignments will be worth 50 points each. Note that there is a tentative 5th worksheet. If we are able to complete the 5th worksheet, then the lowest of the 5 worksheet grades will be dropped. If a lab is cancelled due to weather or other unforeseen circumstance, we will adjust the value of all worksheets to scale to a total of 200 points.\nProject Plan (100 points): Groups/Individual students will propose their independent project following the assigned project plan worksheet. This assignment may be subject to a round of revisions. Early submissions are encouraged.\nData Collection Progress Report (15 points each, 45 total): During the data collection period, students must attend the beginning of lab regardless of their planned collection activities. Students will share with the class their project status and challenges. Active discussion between groups is expected.\nData Analysis Progress Report (15 points each, 45 total): During the data analysis period, students must attend the beginning of lab regardless of their planned activities. Students will share with the class their project status and challenges. Active discussion between groups is expected.\nFinal Presentation (250 Points): Final independent projects will be shared in a standard 15 minute research presentation format. Group members may allocate presentation responsibilities independently, however all group members are expected to present an equal portion. See rubric for more details.\nFinal Paper (300 Points): Final independent projects will be written in a full paper format. See rubric for grading details.\nAuthor contribution statement (50 Points): Evaluation of group members contributions to the final project. Group members will independently report the activity of all group members and assign points. Severe discrepancies will may warrant point deductions on individual project grades.\n\n\n\nLate work & make-up policy:\nAny assignments turned in late will be subject to a 10% grade penalty starting immediately following the due date. For each additional 24-hour period late, a 10% penalty will be added. Some assignments are ineligible for late submission, including the progress reports, final presentation, and author contribution statement.\n\n\nAttendance Policy:\nAttendance is expected for all lab sections. Any expected lab absences should be discussed prior to the start of that lab period.\n\n\nAcademic Integrity:\nAny cases of plagiarism will result in a minimum failure of assignment and may result in further penalties, including automatic failure of the course.\n\n\nGenerative AI & ChatGPT:\nGenerative AI is an extremely powerful tool for anyone who wishes to be a productive scientist. It can be particularly helpful for organizing and writing code and providing the blueprint for text. However, in its current state, ChatGPT and other generative AI applications do not produce work at the level expected of an upper-division undergraduate. If you want to receive a good grade in the course you should not rely exclusively on ChatGPT. If you are not outworking the machine, you are falling behind. Be skeptical of its responses as it cannot be trusted.\n\n\nCo-enrollment with 570 Lecture:\nThis lab course is a fully independent credit. Co-enrollment in the lecture section of 570 is not required nor expected. While content will overlap and it will be beneficial to take both courses, the content will not be synchronous. Note that the major assignment in the lecture section is the Research Proposal. You are welcome to use your independent project as the basis for that assignment. However note that these are separate assignments with different grading structure.\n\n\nClass Schedule:\n\n\n\n\n\n\n\n\n\n\nWeek (Dates)\nLocation\nLab\nAssignments Given\nAssignments Due\n\n\n\n\n0 (8/29 & 8/31)\nVirtual\nIntroduction & R set-up\nIntro Worksheet\n\n\n\n1 (9/5 & 9/7)\nLab room and near-by campus areas\nIntroduction & Mammal Behavior\nWS1\nProject Plan\nIntro Worksheet\n\n\n2 (9/12 & 9/14)\nHarbison State Forest\nIntermediate Disturbance & Succession\nWS2\nWS1\n\n\n3 (9/19 & 9/21)\nUSC Arboretum\nCommunity Assemblages\nWS3\nWS2\n\n\n4 (9/26 & 9/28)\n\n\n\n\n\n\n5 (10/3 & 10/5)\nLab Room\nAnalyzing Community Data & Review of Projects\n\n\n\n\n6 (10/10 & 10/12)\nVarious Stream Locations\nUrban Aquatic Ecology\nPhenology Lab (Virtually available)\nWS4\nWS5*\nWS3\nPlan\n\n\n7 (10/17 & 10/19)\nFall Break Modified Class!\nLab Room (TUESDAY ONLY)\nTuesday (Phenology lab)\nProject Planning\n\nWS4\nProject Plan\n\n\n8 (10/24 & 10/26)\nLab Room & TBD\nProject Planning & In-Class sharing\n\nWS5*\nData Collection Progress Report\n\n\n9 (10/31 & 11/2)\nLab Room & TBD\nData Collection\n\nData Collection Progress Report\n\n\n10 (11/7 & 11/9)\nLab Room & TBD\nData Collection\n\nData Collection Progress Report\n\n\n11 (11/14 & 11/16)\nLab Room\nData Analysis\n\nData Analysis Progress Report\n\n\n12 (11/21 & 11/23)\nThanksgiving Break\nIndividual Meetings\nData Analysis\n\nData Analysis Progress Report\n\n\n13 (11/28 & 11/31)\nLab Room\nData Analysis\n\nData Analysis Progress Report\n\n\n14 (12/5 & 12/7)\nTBD\nFinal Presentations\n\nFinal Presentation\n\n\n15 (12/12)\n\nFinal Paper Submission\nNote I encourage you to submit before, this is just latest possible since grades are due 12/15\n\nFinal Project Paper\nAuthor Contribution Statement"
  }
]