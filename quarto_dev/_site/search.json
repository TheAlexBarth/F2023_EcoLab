[
  {
    "objectID": "addtl_resources.html",
    "href": "addtl_resources.html",
    "title": "Additional Resources",
    "section": "",
    "text": "Advanced Concepts\n\nGit and version control are great ways to organize computational projects. This is increasingly a useful skill to understand:\n\nHappy Git with R\n\nIf you want to learn more about organizing files in a meaningful way, I found this workshop to be extremely helpful:\n\nR for Reproducible Research\n\nThis website was made with quarto, a new markdown format and interactive programming environment. Quarto offers a great way to make pretty documents and presentations with incorporate code:\n\nQuarto"
  },
  {
    "objectID": "code_lab-00.html",
    "href": "code_lab-00.html",
    "title": "Introductory Lab Guide",
    "section": "",
    "text": "When starting a new analysis in R, it is best to create a new R script. You can save this script in your course folder. At the start of each script, it is good form to load any needed packages. In this exercise we’ll need ggplot. If you haven’t already installed ggplot2, enter into the console install.packages(\"ggplot2\").\nAt the top of your script, load the package:\n\nlibrary(ggplot2)"
  },
  {
    "objectID": "code_lab-00.html#reading-the-data-into-r.",
    "href": "code_lab-00.html#reading-the-data-into-r.",
    "title": "Introductory Lab Guide",
    "section": "Reading the data into R.",
    "text": "Reading the data into R.\nIf you downloaded the data to your folder, you can read it in with read.csv(). First make sure your working directory is focused on your file.\n\nsetwd('~/BIOL570L_Docs') # set this to your path in your computer's folder\ncrab_width <- read.csv('LTER_CrabCarapaces.csv')"
  },
  {
    "objectID": "code_lab-00.html#base-r-option",
    "href": "code_lab-00.html#base-r-option",
    "title": "Introductory Lab Guide",
    "section": "Base R Option:",
    "text": "Base R Option:\nFirst, I’ll create individual variables for each mean value and sd value by site:\n\nmean_BC <- mean(crab_width$carapace_width[which(crab_width$Site == 'BC')]) # take mean of BC\nsd_BC <- sd(crab_width$carapace_width[which(crab_width$Site == 'BC')]) # take sd of BC\n\n# same for GTM\nmean_GTM <- mean(crab_width$carapace_width[which(crab_width$Site == 'GTM')])\nsd_GTM <- sd(crab_width$carapace_width[which(crab_width$Site == 'GTM')])\n\nWe can look at those values individually:\n\nprint(mean_GTM)\n\n[1] 12.40321\n\n\nThen we’ll need to pull all those values into a data frame\n\ncrab_means <- data.frame(Site = c('BC','GTM'),\n                        mean = c(mean_BC, mean_GTM),\n                        sd = c(sd_BC, sd_GTM))\n\nWe now can take a look at those values\n\ncrab_means\n\n  Site     mean       sd\n1   BC 16.19730 4.814464\n2  GTM 12.40321 1.804449"
  },
  {
    "objectID": "code_lab-00.html#dplyr-option",
    "href": "code_lab-00.html#dplyr-option",
    "title": "Introductory Lab Guide",
    "section": "dplyr option:",
    "text": "dplyr option:\nThis is the cleaner approach but it does abstract a few steps away. First you want to make sure you have installed dplyr. Also you should load the library (do this at the top of the script).\n\nlibrary(dplyr)\n\nThe tidyverse syntax is really big on piping code together. So we’ll use a number of functions here and chain them all together. Piping takes the value on the left of the pipe operator (|>) and pushes it to the first argument of the function on the right. In tidyverse/dplyr functions, the first argument is often a data.frame. This makes it easy to chain together these functions. In this code, I will chain it all together, but if you want to learn more, you can run one pipe at a time and see what happens in each step.\n\ncrab_means <- crab_width |> \n  filter(Site %in% c('BC', 'GTM')) |> #filter only to these sites\n  group_by(Site) |> #group by site\n  summarize(mean = mean(carapace_width),\n            sd = sd(carapace_width)) # apply mean and sd functions\n\nNow we can look at the data. Note that tidyverse functions will create tibbles rather than data.frames. For most purposes this is a very minor detail that will not matter until you are working on high-level problems or developing software in R.\n\ncrab_means\n\n# A tibble: 2 × 3\n  Site   mean    sd\n  <chr> <dbl> <dbl>\n1 BC     16.2  4.81\n2 GTM    12.4  1.80"
  },
  {
    "objectID": "code_lab-01.html",
    "href": "code_lab-01.html",
    "title": "Lab 1 Code",
    "section": "",
    "text": "The first major challenge (and likely only one) will be organizing your data so that it works well with the provided code. Take the data and format it in an excel sheet. For this analysis, you should total the time spent on each action and put that as rows. You should also count the number of times each action occurred.\nHere’s how my data look. Note that I made this prior to lab so I am using imaginary data:\n\nIf you want to use the code provided, you must exactly match the layout that I used here. While your data might be different R is very picky about a few things. Here’s some potential issues students might run into:\n\nMake sure column names match exactly. R is case sensitive\nGenerally spaces are challenges in character vectors so use a _ instead.\nWhile our data collection sheet on paper had merged cells for behavior, in R, they must be individually represented in each row. So make sure you don’t format the excel sheet in a fancy way\nMake sure to save the files as a .csv file. This is not the excel default. You must select ‘save as’.\nSave the file as a name you can find and remember!"
  },
  {
    "objectID": "code_lab-01.html#check-out-the-data",
    "href": "code_lab-01.html#check-out-the-data",
    "title": "Lab 1 Code",
    "section": "Check out the data",
    "text": "Check out the data\nIf you succesfully loaded the data you should take a look at it to make sure the layout is how you want.\n\nhead(sqdf)\n\n   category           action num_events total_mins\n1  conflict          chasing         19         37\n2  conflict running_squirrel         24         35\n3  conflict         taunting         10          3\n4 avoidance    running_other          4          2\n5 avoidance running_predator          1          1\n6    forage        searching         13         45\n\n\nNote that here I used head(). But in reality, I typically use the View() function to take a peek at data.\nSome possible issues at this step would be that your column headers are wrong or your num_events column or total_mins column are characters (they should be ‘int’). If this is the case, there is something wrong with how you formatted your excel sheet."
  },
  {
    "objectID": "code_lab-01.html#figures",
    "href": "code_lab-01.html#figures",
    "title": "Lab 1 Code",
    "section": "Figures",
    "text": "Figures\nThe original approach can be improved on in a few ways. First, let’s think about that figure. While it was easy to compile the data by behavior category, we smoothed over the details we recorded for individual actions. Below is code to make a similar figure but with a little more detail.\nThis one is a bit trickier since I have to specify the colors I want to use. I’m still making bars by behavior category but now I’m filling the colors with stacked actions. Here, I specify some colors for each action:\n\ncolors = c(\n  `chasing` = '#D81B60',\n  `running_squirrel` = '#C75780',\n  `taunting` = \"#BF7993\",\n  `running_other` = \"#1E88E5\",\n  `running_predator` = \"#66A8E2\",\n  `searching` = \"#FFC107\",\n  `collecting_food` = \"#FDD458\",\n  `storing_food` = \"#FFE597\",\n  `jumping` = \"#FFEFBF\",\n  `eating` = '#004D40',\n  `resting` = '#4D7F77'\n)\n\nNow I can make the same figure but with this enhanced detail. Note that for making your figures, your actions and behavior categories will be different than this example dataset. You’ll need to modify the above color scale for different categories. In R, you can make a different color either with names ('black'). Or you could use HEX codes like I did. Just google any color pallete mixer. As a note, it’s often beneficial to consider colorblind friendly palettes to make your figures accessible to everyone!\n\nggplot(sqdf) + \n  geom_bar(aes(x = category, y = total_mins,\n               fill = action),\n           stat = 'identity', position = 'stack') +\n  scale_fill_manual(values = colors) +\n  labs(x = 'Behavior Category', y = 'Total Minutes', fill = 'Action')+\n  theme_classic()\n\n\n\n\n\n\nTime spent during different behavior categories of Eastern Grey Squirrels observed on USC’s Horshoe. Actions corresponding to each behavior category are shown.\n\n\n\n\nWhile this figure isn’t perfect, I’d probably want to change the colors a bit more. It provides a good idea of how to improve this figure. Another idea, is that while time spent on a category is a great metric, we might also be interested in the number of events that each item happened.\n\nggplot(sqdf) + \n  geom_bar(aes(x = category, y = num_events,\n               fill = action),\n           stat = 'identity', position = 'stack') +\n  scale_fill_manual(values = colors)+\n  labs(x = 'Behavior Category', y = 'Number of Events', fill = 'Action')+\n  theme_classic()\n\n\n\n\n\n\nNumber of events for different actions of squirrels observed on USC’s Horseshoe. Actions are shown grouped by major behavior category.\n\n\n\n\nThis figure provides a different perspective than the first one. We can see that avoidance events occur more often than previously suggested while rest events are less common.\nWe could also look at the average duration of each event:\n\nggplot(sqdf) +\n  geom_bar(aes(x = total_mins/num_events, y = action, fill = category),\n           stat = 'identity')+\n  labs(x = \"Average Duration of Action [mins]\", y = \"Action\", fill = \"Behavior Category\")+\n  theme_classic()\n\n\n\n\n\n\nAverage action direction of Eastern Grey squirrels observed on USC’s Horseshoe campus.\n\n\n\n\nAs you can see, even with a fairly rudimentary dataset, we can display the information in a number of ways. Making a variety of plots can be extremely useful. First, as a researcher this is a critical step of exploratory data analysis (EDA). EDA allows us to notice major trends in our data, sometimes surprising ourselves. Additionally, we can try to think about what is the best way to communicate our findings. We want our figures to be clear to a reader who has no familiarity with our research project. The captions should be brief yet informative. When making your figures, ask your self: “What is the main message I want a reader to take away from this figure?” and then you can think about how effectively you communicate that message through the figure."
  },
  {
    "objectID": "code_lab-01.html#other-data-analyses",
    "href": "code_lab-01.html#other-data-analyses",
    "title": "Lab 1 Code",
    "section": "Other data analyses",
    "text": "Other data analyses\nIn our initial analysis, we simply tested if the distribution of time allocated on one behavior category was statistically significantly different from a uniform time allocation. However, that isn’t the best test of our original hypothesis. The project plan hypothesis suggested that conflict would receive more time allocation than other categories. So let’s run the analysis but with a different “expected” distribution for our squirrel’s time allotment. In this case I’ll define a vector of an expected_model which has the proportions of time spent in my different behavior categories.\nHere, I’m going to make an expected model to match my hypothsis, where a squirrel spends 5% of its time in avoidance, 40% in confilct, 30% foraging, and 25% resting.\nTo make these proportions, I’m matching the order to the order of my observed categories:\n\nsq_summary$category\n\n[1] \"avoidance\" \"conflict\"  \"forage\"    \"rest\"     \n\n\nYou’ll have to create your own expected model to match your unique categories!\n\n# let's say you are interested in comparing \n# for unique categories\n\n# total_observation_time <- sum(sqdf$total_mins) #total up all time observed\nexpected_proportions <- c(0.05,0.4,0.3,0.25)\n\nchisq.test(x = sq_summary$total_time, p = expected_proportions)\n\n\n    Chi-squared test for given probabilities\n\ndata:  sq_summary$total_time\nX-squared = 4.2917, df = 3, p-value = 0.2316\n\n\nFrom this result, we have a p-value > 0.05. This would lead me to write a statement similar to this as a result:\n“There was no significant difference between the time allocation of our observed squirrels and the expected allocations under a conflict-heavy time allocation (chi-squared test, p-value = 0.23).”\nClearly this is fabricated data but it gives a great idea of how we could extend this analysis."
  },
  {
    "objectID": "code_lab-02.html",
    "href": "code_lab-02.html",
    "title": "Lab 1 Code",
    "section": "",
    "text": "Accessing the data\nYou can download the data from the course github org: link\nDownload and save the data in your files/folder\nThen load it into R. I’m going to name it raw_data\n\nsetwd('C:/Users/abart/OneDrive/Documents/UofSC/Classes/BIOL570L')\nraw_data <- read.csv('2023_WS2_Data-Share.csv') # whatever you named i\n\n\n\n\nNow let’s take a look and make sure the structure is as we expect:\n\nstr(raw_data)\n\n'data.frame':   146 obs. of  4 variables:\n $ Region : chr  \"High-Disturbance\" \"High-Disturbance\" \"High-Disturbance\" \"High-Disturbance\" ...\n $ Quadrat: int  1 1 1 1 2 2 2 3 3 3 ...\n $ Taxa   : chr  \"A\" \"B\" \"C\" \"D\" ...\n $ Count  : int  1 2 7 2 2 9 4 2 3 7 ...\n\n\nLook’s good to me!\n\n\nAnalyzing the data\nThis week the question is somewhat straightforward but the code will get quite complex. This guide walks through some of the process of building out the code. If you just want to get to plots and core statistical analyses, you don’t need all this code. I will clearly distinguish between the required and the “bonus” portions. However, I encourage you to skim through all sections as it will inform the process.\nThe main object is to test the intermediate disturbance hypothesis. For this, we have three forest regions: High-Disturbance, Mid-Disturbance, Low-Disturbance. Thus, our predictive variable will be “Region” (which is categorical with three levels). Our response variable should test if the three regions are significantly different from one another in some metric of biodiversity. What is expected for this week’s worksheet, is to select ONE metric to test the hypothesis. Our three possible response variables are (1) Species richness (integer/count data), (2) Shannon’s H (continuous), and (3) Pileou’s D (continuous).\nSo in all cases, we’ll have a categorical predictor variable with a continuous response. Because our categorical variable has three levels, we’ll need to use an ANOVA test, or its non-parametric equivalent. In all cases, we will be comparing are the regions on average significantly different from one another. So we’ll need to calculate the metric for each quadrat, then average by region. Thankfully, much of the dplyr syntax (code) we’ve seen is very useful for these operations.\nRegardless of metric, we’ll make a simple barplot and run a statistical analysis to assess if the observed differences in the barplot are significantly different.\n\n\nA. Species Richness\nFirst, let’s run the analysis for species richness. This is the most straightforward from a coding perspective. We just need to count the number of unique species in each quadrat.\nThis could feasibly be done by hand but it is easier in R. Also, it would not be feasible if we had more regions and we’re less likely to make a mistake!\nThe first step will be to make a data.frame with just counts the number of unique species in each quadrat. We can do that with dplyr I’ll name it richness_df.\nInside this complex chunk of code the key operation comes in the summarize function, where I create the new column richness. Note that here, I’m saying “take the raw data, group it by region and quadrat, then for each group, calculate richness in a new column”. We can calculate richness by finding the unique taxa and then counting them (length).\n\nrichness_df <- raw_data |> \n  group_by(Region, Quadrat) |> #group these\n  summarise(richness = length(unique(Taxa))) # make a column for the # of unique taxa\n\nYou can compare the new richness data.frame to your raw data. Take a look with these functions:\n\nView(richness_df)\nView(raw_data)\n\nThis is a good opportunity to spot-check some of the math. Does the high-density, quadrat 1 have the correct number?\n\nA.1 Plotting Species Richness\nTo make our plot, we’ll need to make a summary data frame which has the mean richness per region with standard deviation. I’ll call it richness_plot_df\nWe can use dplyr:\n\nrichness_plot_df <- richness_df |> \n  group_by(Region) |> \n  summarize(mean_richness = mean(richness),\n            sd_richness = sd(richness))\n\nNow we can make our plot:\n\nggplot(richness_plot_df) +\n  geom_bar(aes(x = Region, y = mean_richness),\n           stat = 'identity', fill = 'black')+\n  geom_errorbar(aes(x = Region, ymin = mean_richness,\n                    ymax = mean_richness + sd_richness),\n                width = 0.5, color = 'black')+\n  labs(x = \"Region\",y = \"Mean Richness\")+\n  theme_bw()\n\n\n\n\n\n\nGreat! It looks pretty good!\n\n\nA.1b Plotting Bonus\nOne issue with the plot above is I don’t like the order of the categories. When we look back to the lab reading, Connell’s figure has the categories as a gradient from high-to-mid-to-low.\nSo let’s reorganize the plot to match. Here we need to reassign factor levels in the data frame then it will work in ggplot:\n\n# What if I want my figures in a different order?\nrichness_plot_df$Region <- factor(richness_plot_df$Region, levels = c(\"High-Disturbance\",\n                                                                      \"Mid-Disturbance\",\n                                                                      \"Low-Disturbance\"))\n\nNow we actually can just run the same plot code:\n\nggplot(richness_plot_df) +\n  geom_bar(aes(x = Region, y = mean_richness),\n           stat = 'identity', fill = 'black')+\n  geom_errorbar(aes(x = Region, ymin = mean_richness,\n                    ymax = mean_richness + sd_richness),\n                width = 0.5, color = 'black')+\n  labs(x = \"Region\",y = \"Mean Richness\")+\n  theme_bw()\n\n\np1 = ggplot(richness_plot_df) +\n  geom_bar(aes(x = Region, y = mean_richness),\n           stat = 'identity', fill = 'black')+\n  geom_errorbar(aes(x = Region, ymin = mean_richness,\n                    ymax = mean_richness + sd_richness),\n                width = 0.5, color = 'black')+\n  labs(x = \"Region\",y = \"Mean Richness\")+\n  theme_bw()\nwatermark_plot(p1)\n\n\n\n\n\n\nA.2 Richness Analysis\nIn the project plan we suggested running an ANOVA to compare between the categories. However, ANOVAs assume that our (response) data are normally distributed.\nTechnical note: ANOVAs are just extensions of linear models. There are several assumptions but normality of residuals is the main one. This is not normality of the data, but are the data within a group normally distributed around the mean. However, typically if the data are non-normal then the residuals will be non-normal at small sample sizes. At larger sample sizes this is less of a concern due to the Central Limit Theorem.\nSo first we need to see if the species richness are normally distributed within each group. We can do this a number of ways but the easiest is to see if the density distribution looks normal. Note these are not plots we’d share with anyone, or put in your worksheet but it is a quick way for us to see:\nHere, I’m using base R graphics and some advanced approaches just to keep it short. Don’t worry about all these details. Here, we’re primary concerned about the question of normality:\n\npar(mfrow = c(2,2)) # set up plotting window in 2x2 grid\nfor(group in unique(richness_df$Region)) {\n  density(richness_df$richness[richness_df$Region == group]) |> \n  plot(main = group)\n}\n\n\n\n\nLooking at the figures, it seems that non-normality might be a concern. So we’ll need to use a non-parametric test. The alternative to an ANOVA in this case is a Kruskal-Wallace test.\nThe kruskal-wallace test is similar to our chi-squared test in that it asks: “Are there any differences between the distributions of the groups”.\nLet’s run this test in R:\n\nkruskal.test(richness_df$richness ~ richness_df$Region) # Tell's us it is significantly different\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  richness_df$richness by richness_df$Region\nKruskal-Wallis chi-squared = 3.8114, df = 2, p-value = 0.1487\n\n\nAs of thursday’s data addition, we do not have a significant difference in species richness between groups (p = 0.149, kruskall-wallis test).\n\n\n\nB. Shannon’s H Diversity Metric\nCalculating the Shannon-Wiener index is a little more complicated from an R perspective. Look at the formula:\n\\[\nH = -\\sum_{i = 1}^{R}{\\frac{n_i}{N}ln\\frac{n_i}{N}}\n\\]\nWe need to caculate for each quadrat, the total number of species (\\(N\\)) and the proportion of each \\(i^{th}\\) species in that quadrat, the natural log of that proportion and then sum it up for all species in that quadrat!\nAgain you could brute-force this calculation or do it in Excel. For this lab it might be feasible, but what if you had years of data! This is where R is useful.\n\nB.0 Bonus\nLet’s first calculate H for just one region, one quadrat by “hand”. I’ll create a data frame of just one quadrat\n\n# just for one case:\nquadrat_1 <-  raw_data |> \n  filter(Region == \"Low-Disturbance\", Quadrat == '1')\n\nquadrat_1\n\n           Region Quadrat Taxa Count\n1 Low-Disturbance       1    A     1\n2 Low-Disturbance       1    B     1\n3 Low-Disturbance       1    C     3\n\n\nNow we can calculate all those values. Note here, I’m using p as the term for \\(\\frac{n_i}{N_i}\\)\n\np <- quadrat_1$Count / sum(quadrat_1$Count) # create counts\nlnp <- log(p)\n-sum(p * lnp)\n\n[1] 0.9502705\n\n\n\n\nB.0 Calculating H for all the data\nHere, we will create our own function for calculating H. Honestly, this is more advanced than your typical intro-to-R but it makes the analysis easier on the whole. So let’s roll with it.\n\n# We can write a function to do this repeatedly!\ndiversity_calculator <- function(count) {\n  p <- count / sum(count) # create counts\n  lnp <- log(p)\n  H <-  -sum(p * lnp)\n  return(H)\n}\n\nIf you did the bonus above, you can check the function works but just running on the quadrat 1 data!\nNow we can use same code were familiar with to calculate H, just with our own function inside summarize. I’ll create a new data.frame called diversity_df\n\ndiversity_df <- raw_data |> \n  group_by(Region, Quadrat) |> \n  summarise(H = diversity_calculator(Count))\n\n\n\nB.1 Plotting Diversity\nLet’s make our summary data frame to plot with. I’ll call it div_plot\n\ndiv_plot <- diversity_df |> \n  group_by(Region) |> \n  summarize(mean_H = mean(H),\n            sd_H = sd(H))\n\nNow let’s plot it. Note that I’m included the code in here to reorganize the order of the categories from the A.1b section:\n\n# What if I want my figures in a different order?\ndiv_plot$Region <- factor(div_plot$Region, levels = c(\"High-Disturbance\",\n                                                      \"Mid-Disturbance\",\n                                                      \"Low-Disturbance\"))\n\nggplot(div_plot) +\n  geom_bar(aes(x = Region, y = mean_H),\n           stat = 'identity', fill = 'black')+\n  geom_errorbar(aes(x = Region, ymin = mean_H,\n                    ymax = mean_H + sd_H),\n                width = 0.5, color = 'black')+\n  labs(x = \"Region\",y = \"Shannon's H\")+\n  theme_bw()\n\n\n\n\n\n\n\n\nB.2 Analyzing H\nLet’s take a look at if normality might be a concern for this dataset:\n\npar(mfrow = c(2,2)) # set up plotting window in 2x2 grid\nfor(group in unique(diversity_df$Region)) {\n  density(diversity_df$H[richness_df$Region == group]) |> \n  plot(main = group)\n}\n\n\n\n\nAgain, non-normality might be a considerable concern. So let’s use the kruskal-wallace test.\n\nkruskal.test(diversity_df$H~diversity_df$Region)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  diversity_df$H by diversity_df$Region\nKruskal-Wallis chi-squared = 1.9782, df = 2, p-value = 0.3719\n\n\nHere, this test suggests that there is no significant difference between the regions. So we can write that as our result. (This might change with added data).\n\n\n\nC. Evenness\nFinally let’s calculate pileou’s evenness. Again, this might create some new code challenges but it will make it easier in the long-run.\nRemember the formula:\n\\[ D = 1 - \\sum_{i=1}^{R}{\\frac{n_i(n_i-1)}{N(N - 1)}} \\]\n\nC.0 Bonus\nHere, we can calculate the value for just one quadrat. Here, I’m using the quadrat_1 data frame I created in B.0 bonus. If you skipped that go back and make sure you have quadrat_1 in your environment.\n\nn = quadrat_1$Count\nN = sum(quadrat_1$Count)\n# Solve for D\n1 - sum((n*(n-1))/(N*(N-1)))\n\n[1] 0.7\n\n\n\n\nC.0 Calculating Pileou’s evenness\nHere’s the self-defined function which will calculate pileou’s evenness:\n\npileou_calculator <- function(count) {\n  n = count\n  N = sum(count)\n  D = 1 - sum((n*(n-1))/(N*(N-1)))\n  return(D)\n}\n\nNow we can calculate it for all quadrats. I’ll call it evenness_df\n\n# calculate for all!\nevenness_df <- raw_data |> \n  group_by(Region, Quadrat) |> \n  summarize(D = pileou_calculator(Count))\n\n\n\nC.1 Plotting Evenness\nWe’ll need to make a summary dataframe first. I’ll call it evenness_plot\n\nevenness_plot <- evenness_df |> \n  group_by(Region) |> \n  summarize(mean_D = mean(D),\n            sd_D = sd(D))\n\nNow we can plot that dataframe.\n\n# What if I want my figures in a different order?\nevenness_plot$Region <- factor(evenness_plot$Region, levels = c(\"High-Disturbance\",\n                                                      \"Mid-Disturbance\",\n                                                      \"Low-Disturbance\"))\n\nggplot(evenness_plot) +\n  geom_bar(aes(x = Region, y = mean_D),\n           stat = 'identity', fill = 'black')+\n  geom_errorbar(aes(x = Region, ymin = mean_D,\n                    ymax = mean_D + sd_D),\n                width = 0.5, color = 'black')+\n  labs(x = \"Region\",y = \"Pileou's D\")+\n  theme_bw()\n\n\n\n\n\n\n\n\nC.2 Analyzing D\nFirst, let’s check the normality of our groups:\n\npar(mfrow = c(2,2)) # set up plotting window in 2x2 grid\nfor(group in unique(evenness_df$Region)) {\n  density(evenness_df$D[evenness_df$Region == group]) |> \n  plot(main = group)\n}\n\n\n\n\nAgain, non-normality might be a concern so let’s use the kruskall.wallace test\n\nkruskal.test(evenness_df$D~evenness_df$Region)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  evenness_df$D by evenness_df$Region\nKruskal-Wallis chi-squared = 4.1779, df = 2, p-value = 0.1238\n\n\nAgain there is no difference in evenness between the regions."
  },
  {
    "objectID": "code_lab-03-solutions.html",
    "href": "code_lab-03-solutions.html",
    "title": "Some approaches analyze lab 3",
    "section": "",
    "text": "Some Context\nLet’s return to the lab’s hypotheses:\n(H1): Along a strong, but short, moisture gradient the dominant tree species will vary in distinct regions. (H2.A): Trees alter the soil composition at a hyper-local scale, so that their leaf-litter supports distinct communities of invertebrates. (H2.B): Alternative to H2.A, it could simply be that the abiotic factors are the primary factor determining soil community structure, and the community will vary based on the soil characteristics rather than treespecies.\nSo for the first hypothesis, we want to evaluate if the dominance of different trees varies along the hillside. To assess “dominance” of trees, we can use the relative importance index and compare how it changes for different trees along the slope.\nThe second hypothesis is a little more complicated. We are simply interested if there is some difference in community composition between (A) particular trees vs open areas vs others or (B) between soil characteristics (which we assume are associated with the region). Thus, our first hypothesis is someone dependent on the results of the first hypothesis. If the tree community composition varies along the hill side then we’ll only be able to compare . However, if tree dominance doesn’t vary with the slope, we can assess soil community based on the tree-base and the soil community separately.\nOne general concern I had for this data analysis was that the wasn’t really a true “open” area from which we could collect. The tree canopy covers the entire slopeside so we can’t just compare tree vs. non-tree. However, if trees are different based on regions of the slope, we can still assess if the community composition is different. Another challenge of this lab is we have relatively small samples sizes for the transects. However we do have good data for the inverts.\nMy approach to analyze the data is very exploratory, I try a bunch of possible approaches to see if we can tease out a signal. However, as you’ll see, analyses are just a tool - while some may be more useful in a given case, generally if there truly isn’t a pattern - your data will show it!\nFor all this, I’m am using a few more advanced approaches to make it quicker. I’ll show clearly how to run a simple version, but there’s also the advanced options for those of you interested in some of the more powerful applications of R.\n\n\n\n\nSimpleAdv.\n\n\nIn a simple approach, we read in our data by downloading from the github repo’s data folder, put it in our folder of choice, then read it in locally (this is what I’ve taught so far):\n\ninverts <- read.csv('inverts.csv')\ntree_raw <- read.csv('tree_raw.csv')\n\n\n\nWhat I actually do when writing the tutorials is I will read in the files directly from the internet. You can give read.csv an html address with a csv file. These can be found by clicking “raw” on the github page when looking at the csv of the data. That normally would look like this:\n\ninverts <- read.csv(\"https://raw.githubusercontent.com/USC-Ecology-Lab/Lab_3_Community-Assemblages/main/data/inverts.csv\")\n\nHowever, because we have multiple files, I could click through and manually copy all links, but that’s less fun. So I decided to try and figure out how to get all links automatically using githubs API. Now, this took longer to learn how to do than it would’ve to just copy-and-paste. But I had fun learning this for the past 10mins. Admittedly this is way more than just the ‘advanced version’, but you clicked on this tab so ha.\nThanks chatGPT for the framework!\n\n# This is the code that actually got ran\n\n# I need R's api and json libraries\nlibrary(httr)\nlibrary(jsonlite)\n\nrepo <- 'usc-ecology-lab/Lab_3_Community-Assemblages'\nfolder_path <- './data'\n\napi_url <- paste0(\"https://api.github.com/repos/\", repo, \"/contents/\", folder_path)\n\n# read it all in\nresponse <- GET(api_url) |> \n  content('text') |> \n  fromJSON()\n\nfile_names <- response$name |> \n  gsub(pattern = '.csv',\n       replacement = \"\",\n       )\n\n# now I can create a list of dataframe\ndf_list <- list()\n# loop through and download\nfor(i in 1:nrow(response)) {\n  df_list[[i]] <- read.csv(response$download_url[i])\n}\nnames(df_list) <- file_names #name the list\nlist2env(df_list, envir = .GlobalEnv) #load the list to the environment\n\n<environment: R_GlobalEnv>\n\nrm(list = c(\"df_list\", \"repo\", \"api_url\",\n            'file_names','folder_path','i'))\n\n\n\n\n\n\nExploratory Data Analysis for Trees\nTo address the first hypothesis, we need to explore how the importance of trees varies with the slope. However, to do this we need to think about the data we have.\nThe tree_raw dataframe does not offer much to work with. Unless we wanted to investigate a question like “does loblolly size vary based on slope?”. Instead we need to rely on the RIV (relative importance value). I already calculated this for you in a few approaches. First, I did it by species and by category (grouping some similar species together). I also calculated RIV in 5m bins and then again in 3-large bins which correspond to the defined sub regions people listed on their transects.\nRIV is calculated by effectively measuring the total area in a set bin which is occupied by that particular species. Such that for each \\(s\\) species, and \\(i\\) tree (belonging to a \\(s\\) species):\n\\[\nRIV_{s} = \\frac{\\sum_{i_s} \\pi \\frac{DBH_{i_s}}{2}^2}{\\sum_{i} \\pi \\frac{DBH_{i}}{2}^2}\n\\]\n\nAn attempt at regression & correlation\nI really wanted to try a correlation/regression analysis in this lab. However, the data don’t really make it all that clean (or appropriate). Nonetheless, I figured if I make 5-m bins we can try to treat them as a continuous variable and run a regression. Regressions, as you can see below don’t apply in this case. However, we can use a non-parametric correlation to say something about our data.\n\nSimpleAdv.Better plots\n\n\nWe can only look at one tree’s RIV change at a time. For the simple example, let’s just do hardwoods because they are the most abundant overall.\n\nlibrary(dplyr)\nhardwood_bins <- tree_cat_5m |> \n  filter(Tree_cat == 'hardwood')\n\nhardwood_lm <- lm(riv ~ bin, data = tree_cat_5m)\nsummary(hardwood_lm)\n\n\nCall:\nlm(formula = riv ~ bin, data = tree_cat_5m)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.25000 -0.18992 -0.13095 -0.08333  0.89286 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.250000   0.065199   3.834 0.000228 ***\nbin         -0.004762   0.003117  -1.528 0.129951    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3499 on 94 degrees of freedom\nMultiple R-squared:  0.02423,   Adjusted R-squared:  0.01385 \nF-statistic: 2.334 on 1 and 94 DF,  p-value: 0.13\n\n\nFrom the above output, we can look at the p-value of “bin” to see if there is a signficant effect of bin (slope location) on riv. It is 0.129951, so we would conclude the slope is not significantly different from 0. Or in simpler words there is no effect.\nTo be quick, I just looked at this data using base R:\n\nplot(riv ~ bin, hardwood_bins)\nabline(hardwood_lm)\n\n\n\n\nThis isn’t a pretty plot I would present, but it is useful for my purposes. We can see that the regression line is largely influenced by the large amount of 0’s at high bins. A linear regression in this case is not really the best method given the fact the LIINE assumptions are not met. However, we can test if there is a significant correlation using a non-parametric correlation test. Correlation tests aren’t as useful as regressions since they don’t give us an effect size like slope - which is a quantitative relationship with predictive power. Instead, correlations just tell us how tightly two variables are related (from 0-1). Correlations work well with monotonic relationships, and there are several methods for using non-normal, non-linear data. Here, we’ll use spearman’s rank correlation\n\ncor.test(x = hardwood_bins$bin, y = hardwood_bins$riv, method = 'spearman')\n\n\n    Spearman's rank correlation rho\n\ndata:  hardwood_bins$bin and hardwood_bins$riv\nS = 3873.7, p-value = 0.0002268\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.6842051 \n\n\nHere, we can conclude that there is a significant, negative correlation between increasing distance up the slope (bin) and the relative importance value of hardwoods (spearman’s \\(\\rho\\) = -0.684, p < 0.001).\nTo do this with other categories, you can just change where we filter for hardwoods and try for other categories!\n\n\nReally, in fully data exploration mode, I want to run the simple analysis for every single species and every single categories. It gets fairly messy to do this “by-hand” and copy-pasting code over and over and changing little details. Instead what I really would do here is loop through all categories and print out the regression output and make quick, ugly plots. If there’s something interesting, I can clean it up later.\nI can now scroll through all this output and read what I want!\n\n# Loop analysis:\ntree_sp <- list()\nfor(tree in unique(tree_sp_riv_5m$tree_id)) {\n  tree_sp[[tree]] <- tree_sp_riv_5m |> \n    filter(tree_id == tree)\n}\n\n# run the regressions here\ntree_5m_reg_mod <- list()\nfor(tree in names(tree_sp)) {\n  tree_5m_reg_mod[[tree]] <- lm(riv ~ bin, data = tree_sp[[tree]])\n  print(tree) # print out name of trees\n  print(cor.test(x = tree_sp[[tree]]$bin, y = tree_sp[[tree]]$riv,\n                 method = 'spearman'))\n}\n\n[1] \"sycamore\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_sp[[tree]]$bin and tree_sp[[tree]]$riv\nS = 3358.3, p-value = 0.02367\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.4601307 \n\n[1] \"dogwood\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_sp[[tree]]$bin and tree_sp[[tree]]$riv\nS = 2857.1, p-value = 0.2541\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.2422138 \n\n[1] \"hickory\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_sp[[tree]]$bin and tree_sp[[tree]]$riv\nS = 2946.6, p-value = 0.1833\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.2811346 \n\n[1] \"loblolly\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_sp[[tree]]$bin and tree_sp[[tree]]$riv\nS = 2110.8, p-value = 0.7024\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n0.08226127 \n\n[1] \"whiteoak\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_sp[[tree]]$bin and tree_sp[[tree]]$riv\nS = 1694.7, p-value = 0.214\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.2631807 \n\n[1] \"poplar\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_sp[[tree]]$bin and tree_sp[[tree]]$riv\nS = 3032.6, p-value = 0.1293\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n-0.318511 \n\n[1] \"bay\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_sp[[tree]]$bin and tree_sp[[tree]]$riv\nS = 3055.9, p-value = 0.1169\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.3286648 \n\n[1] \"wateroak\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_sp[[tree]]$bin and tree_sp[[tree]]$riv\nS = 2419.1, p-value = 0.8101\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n        rho \n-0.05178796 \n\nfor(tree in names(tree_sp)) {\n  try({\n    plot(riv ~ bin, tree_sp[[tree]],\n         main = tree)\n    abline(tree_5m_reg_mod[[tree]])\n  })\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooking at it by species, it is not very clear\nI can also do this by category:\n\n# Loop analysis:\ntree_cat <- list()\nfor(cat in unique(tree_cat_5m$Tree_cat)) {\n  tree_cat[[cat]] <- tree_cat_5m |> \n    filter(Tree_cat == cat)\n}\n\ntree_cat_reg_mod <- list()\nfor(cat in names(tree_cat)) {\n  tree_cat_reg_mod[[cat]] <- lm(riv ~ bin, data = tree_cat[[cat]])\n  print(cat)\n  print(cor.test(x = tree_cat[[cat]]$riv,\n                 y = tree_cat[[cat]]$bin,\n                 method = 'spearman'))\n}\n\n[1] \"hardwood\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_cat[[cat]]$riv and tree_cat[[cat]]$bin\nS = 3873.7, p-value = 0.0002268\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.6842051 \n\n[1] \"pine\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_cat[[cat]]$riv and tree_cat[[cat]]$bin\nS = 2110.8, p-value = 0.7024\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n0.08226127 \n\n[1] \"oak\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_cat[[cat]]$riv and tree_cat[[cat]]$bin\nS = 1953.5, p-value = 0.4823\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.1506457 \n\n[1] \"other\"\n\n    Spearman's rank correlation rho\n\ndata:  tree_cat[[cat]]$riv and tree_cat[[cat]]$bin\nS = 3055.9, p-value = 0.1169\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.3286648 \n\nfor(cat in names(tree_cat)) {\n  try({\n    plot(riv ~ bin, tree_cat[[cat]],\n         main = cat)\n    abline(tree_cat_reg_mod[[cat]])\n  })\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooking at all the categories, our only significant correlation is with the hardwoods. However, it is worth noting that the pines only occur in the mid-slope area. This would make the relationship non-monotonic so we can’t capture that relationship with a simple rank-correlation test.\n\n\nIf I wanted to plot these data to look a little better, I’d use ggplot and loess smoothers:\n\nggplot(hardwood_bins, aes(x = bin, y = riv)) +\n  geom_point() +\n  geom_smooth(color = 'black') +\n  labs(x = 'Distance up slope [m]', y = 'Hardwood RIV') +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nThe Kruskall-Wallace approach\nSince we don’t really have enough data for a regression, the alternative approach is to use an ANOVA with RIV as the response variable and sub region as the predictor. Since we have a RIV value for each tree, we will need to run the ANOVA for each response variable. I’m not even investigating my assumptions for an ANOVA however because I have such small sample size, I’m going to assume that the non-parametric approach is necessary.\nI can make one graph to show all this data, but then run multiple tests to investigate the data.\n\nPlotSimpleAdv.\n\n\nI’ll just do this for groups by category since we already well know that there is not enough data for grouping by species.\n\nlibrary(ggplot2)\n\ntree_cat_summary <- tree_cat_subregion |> \n  group_by(Subregion, Tree_cat) |> \n  summarize(mean_riv = mean(riv),\n            sd_riv = sd(riv))\n\nggplot(tree_cat_summary) +\n  geom_bar(aes(x = Tree_cat, y = mean_riv, fill = Subregion),\n           stat = 'identity', position = 'dodge') +\n  geom_errorbar(aes(x = Tree_cat, ymin = mean_riv,\n                    ymax = mean_riv + sd_riv,\n                    color = Subregion),\n                stat = 'identity', position = 'dodge') +\n  labs(x = 'Tree Category', y = 'RIV', fill = \"Subregion\",\n       color = \"Subregion\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nSimilar to the regression approach, we will need to run a kruskall.wallace test for each individual tree-category. I’m going to use the dunn.test() function from the dunn.test package because it will run both the kruskall wallace test and the post-hoc tests at once. The KW test tells us if there is some difference between the groups then the dunn test tells us pair-wise comparisons.\n\nlibrary(dunn.test)\n\n# we'll use the category & subregion dataframe\n# let's look at pines this time\npine_regions <- tree_cat_subregion |> \n  filter(Tree_cat == 'pine')\n\ndunn.test(g = pine_regions$Subregion, x = pine_regions$riv)\n\n  Kruskal-Wallis rank sum test\n\ndata: x and group\nKruskal-Wallis chi-squared = 2.102, df = 2, p-value = 0.35\n\n                           Comparison of x by group                            \n                                (No adjustment)                                \nCol Mean-|\nRow Mean |        bog    dryhill\n---------+----------------------\n dryhill |  -1.416983\n         |     0.0782\n         |\n   slope |  -0.974176   0.442807\n         |     0.1650     0.3290\n\nalpha = 0.05\nReject Ho if p <= alpha/2\n\n\nThis tells us that the relative importance value of pines is not significantly different across regions (Kruskall wallace test, p = 0.35)\n\n\nAgain, I just want to run all the tests at once. This time, to keep it clean, I’ll just do it for the categories:\n\ntree_cat_reg <- list()\n\nfor(cat in unique(tree_cat_subregion$Tree_cat)) {\n  tree_cat_reg[[cat]] <- tree_cat_subregion |> \n    filter(Tree_cat == cat)\n}\n  \nfor(cat in names(tree_cat_reg)) {\n  print(cat)\n  dunn.test(g= tree_cat_reg[[cat]]$Subregion,\n            x = tree_cat_reg[[cat]]$riv)\n}\n\n[1] \"hardwood\"\n  Kruskal-Wallis rank sum test\n\ndata: x and group\nKruskal-Wallis chi-squared = 4.3556, df = 2, p-value = 0.11\n\n                           Comparison of x by group                            \n                                (No adjustment)                                \nCol Mean-|\nRow Mean |        bog    dryhill\n---------+----------------------\n dryhill |   2.086996\n         |    0.0184*\n         |\n   slope |   1.043498  -1.043498\n         |     0.1484     0.1484\n\nalpha = 0.05\nReject Ho if p <= alpha/2\n[1] \"oak\"\n  Kruskal-Wallis rank sum test\n\ndata: x and group\nKruskal-Wallis chi-squared = 2.7152, df = 2, p-value = 0.26\n\n                           Comparison of x by group                            \n                                (No adjustment)                                \nCol Mean-|\nRow Mean |        bog    dryhill\n---------+----------------------\n dryhill |  -1.556997\n         |     0.0597\n         |\n   slope |  -0.311399   1.245598\n         |     0.3777     0.1065\n\nalpha = 0.05\nReject Ho if p <= alpha/2\n[1] \"pine\"\n  Kruskal-Wallis rank sum test\n\ndata: x and group\nKruskal-Wallis chi-squared = 2.102, df = 2, p-value = 0.35\n\n                           Comparison of x by group                            \n                                (No adjustment)                                \nCol Mean-|\nRow Mean |        bog    dryhill\n---------+----------------------\n dryhill |  -1.416983\n         |     0.0782\n         |\n   slope |  -0.974176   0.442807\n         |     0.1650     0.3290\n\nalpha = 0.05\nReject Ho if p <= alpha/2\n[1] \"other\"\n  Kruskal-Wallis rank sum test\n\ndata: x and group\nKruskal-Wallis chi-squared = 1.1667, df = 2, p-value = 0.56\n\n                           Comparison of x by group                            \n                                (No adjustment)                                \nCol Mean-|\nRow Mean |        bog    dryhill\n---------+----------------------\n dryhill |   1.020620\n         |     0.1537\n         |\n   slope |   0.204124  -0.816496\n         |     0.4191     0.2071\n\nalpha = 0.05\nReject Ho if p <= alpha/2\n\n\n\n\n\nAll in all, the kruskall-wallace tests suggest that there are no significant differences between subregions and RIV for any tree category. However, we did see a significant negative correlation between increasing slope distance and hardwood RIV. If I were discussing these results, I’d probably talk about how there were strong trends for some taxa to be in certain regions but there isn’t a large enough amount of data to distinguish between different regions. I’d also note that the hardwood RIV relationship was influenced by the one large Tulip poplar and the American sycamores which tended to be in the bog, or at the bog edge. There was a consistent amount of oaks, albeit from different species up the hill and pines were notable, but sparse.\nAlso maybe DBH isn’t the best metric because although pines don’t get too thick, they get very tall and produce many leaves which may have a dispropotionate influence on the leaf litter.\n\n\n\nExploratory Data Analysis for inverts\nTo assess if the invertebrate community is significantly different between litter invertebrate community and (A) tree-base or (B) slope-region. These data are not too clear to disentangle because based on our tree analysis, we know the tree community is not that different along the slope. However, there are some subtle differences between tree RIV (hardwoods tend to be lower on the slope). So if we see a significant difference, is it from the slope-soil characteristics or the tree litter composition. Theoretically, our open-sampling locations would help disentangle this, however, we didn’t get a truly open sample. Nonetheless, I’ll treat it as such.\n\nPlotSimpleAdvanced\n\n\nFirst, we need to make a summary dataframe with means and standard deviations.\n\ninvert_sum <- inverts |> \n  group_by(Spp, Sampling_tree, Region) |> \n  summarize(mean_count = mean(Num),\n          sd_count = sd(Num))\n\nNow, I have three categories to plot, Species, Sampling_tree, and Region. And I want to show the mean count for each species. This is too much to fit on one plot so I’ll make a plot for each species then display it. Given the large number of species in a study like this, it would be OK to focus on the most abundant or a few unique ones.\n\n# let's just look at mites\nacari_sum <- invert_sum |> \n  filter(Spp == 'Acari')\n\nggplot(acari_sum) +\n  geom_bar(aes(x = Region, y = mean_count, fill = Sampling_tree),\n           stat = 'identity', position = 'dodge') +\n  geom_errorbar(aes(x = Region, ymin = mean_count,\n                    ymax = mean_count + sd_count, color = Sampling_tree),\n                position = 'dodge') +\n  labs(x = \"Slope Region\", y = \"Mean Acari Abundance\", \n       fill = \"Tree\", color  = \"Tree\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nThis analysis is a little bit tricky. We want to compare how abundance of a single species (we’ll use acari) varies between certain trees and slope region. If we had a balanced dataset, we could use a two-way ANOVA to compare both slope region, tree, and the interaction. However, because some tree categories didn’t occur in some regions we can’t do that. Instead, I’ll use an kruskall-wallace test to compare: within a region, does the insect litter community vary between the tree which were sampled. For the example, I’ll just do upper, but you can run this for every region.\n\nupper_acari <- inverts |> \n  filter(Region == 'Upper', Spp == 'Acari')\n\n\ndunn.test(g = upper_acari$Sampling_tree, x = upper_acari$Num)\n\n  Kruskal-Wallis rank sum test\n\ndata: x and group\nKruskal-Wallis chi-squared = 3.1767, df = 2, p-value = 0.2\n\n                           Comparison of x by group                            \n                                (No adjustment)                                \nCol Mean-|\nRow Mean |   Loblolly        Oak\n---------+----------------------\n     Oak |  -0.674849\n         |     0.2499\n         |\n    Open |   1.173673   1.724615\n         |     0.1203     0.0423\n\nalpha = 0.05\nReject Ho if p <= alpha/2\n\n\nSo based in the results of the kruskall-wallace test there’s no significant difference in acari abundance between the Loblolly site, the open site, and the oak site (KW test p-value = 0.2). Discussing these results, I’d note that the pine litter appeared to dominate the whole system and acari were generally well spread throughout all litter communities. I’d also likely run a few more tests in the different regions and with different taxa to see what patterns arise.\n\n\nA common approach for assessing community composition is a Non-metic Multidimensional Scaling approach. This is an ordination technique usable for count data (with a lot of 0’s). Effectively, what NMDS does is it takes all the many response variables (counts of every taxa) and assess how different each row (site) is based on the counts of the taxa. It does this by ranking them in multidimensional space and measuring the distance. Then, it mushes those distances into a lower dimension where we can look at it. Then we can overlay the site/tree/whatever to visualize if there are any clear patterns that arise. It is important to note that when plotting in NMDS-space the distances between the points do not mean anything (hence non-metric) yet we can make relative assessments on the visual groupings.\nWe’ll need to use the vegan package so install it if you don’t have it. I’ll also use tidyr for the data prep.\nWhen running the nmds, it iterates to find an optimal solution. Yet we specify the number of dimensions to reduce into. Ideally, we want a low stress value (<0.2). So I’ll play around with the number of dimensions to get it lower. I did this a few times and ended up setting k = 3 for three dimensions.\n\nlibrary(vegan)\nlibrary(tidyr)\ninvert_comm <- inverts |> \n  pivot_wider(names_from = Spp,\n              values_from = Num)\nset.seed(1000)\ninvert_nmds=metaMDS(invert_comm[,4:ncol(invert_comm)], # Our community-by-species matrix\n                     k=3) # The number of reduced dimensions\n\nSquare root transformation\nWisconsin double standardization\nRun 0 stress 0.1285056 \nRun 1 stress 0.1340613 \nRun 2 stress 0.1348327 \nRun 3 stress 0.1297093 \nRun 4 stress 0.1297095 \nRun 5 stress 0.1297095 \nRun 6 stress 0.1348331 \nRun 7 stress 0.1285056 \n... New best solution\n... Procrustes: rmse 3.606478e-05  max resid 8.770583e-05 \n... Similar to previous best\nRun 8 stress 0.1480649 \nRun 9 stress 0.1315938 \nRun 10 stress 0.1285056 \n... Procrustes: rmse 0.0003471511  max resid 0.0007761196 \n... Similar to previous best\nRun 11 stress 0.1649935 \nRun 12 stress 0.1305927 \nRun 13 stress 0.1285056 \n... New best solution\n... Procrustes: rmse 2.07782e-05  max resid 4.203106e-05 \n... Similar to previous best\nRun 14 stress 0.1285059 \n... Procrustes: rmse 0.0005192305  max resid 0.001164083 \n... Similar to previous best\nRun 15 stress 0.1285059 \n... Procrustes: rmse 0.0005116436  max resid 0.001147285 \n... Similar to previous best\nRun 16 stress 0.1315938 \nRun 17 stress 0.1480648 \nRun 18 stress 0.1315938 \nRun 19 stress 0.1285056 \n... Procrustes: rmse 0.0002657652  max resid 0.0005953675 \n... Similar to previous best\nRun 20 stress 0.1480647 \n*** Best solution repeated 4 times\n\n\nTo plot my nmds, I’ll need to make three plots because I have three dimensions. I can look at 1v2, 1v3, and 2v3. Effectively, I’m plotting the different faces of the cube that these points exist in.\n\nplot_nmds <- cbind(invert_comm[,1:3], invert_nmds$points)\n\n\np1 = ggplot() +\n  geom_point(data = plot_nmds,\n             aes(x = MDS1, y = MDS2, color = Sampling_tree,\n                 shape = Region),\n             size = 5)+\n  geom_label(data = as.data.frame(invert_nmds$species), \n             aes(x = MDS1, y = MDS2, label = rownames(as.data.frame(invert_nmds$species))))+\n  theme_minimal()\n\np2 = ggplot() +\n  geom_point(data = plot_nmds,\n             aes(x = MDS2, y = MDS3, color = Sampling_tree,\n                 shape = Region),\n             size = 5)+\n  geom_label(data = as.data.frame(invert_nmds$species), \n             aes(x = MDS2, y = MDS3, label = rownames(as.data.frame(invert_nmds$species))))+\n  theme_minimal()\n\np3 = ggplot() +\n  geom_point(data = plot_nmds,\n             aes(x = MDS1, y = MDS3, color = Sampling_tree,\n                 shape = Region),\n             size = 5)+\n  geom_label(data = as.data.frame(invert_nmds$species), \n             aes(x = MDS1, y = MDS3, label = rownames(as.data.frame(invert_nmds$species))))+\n  theme_minimal()\n\n\np1\np2\np3\n\n\n\n\n\n\n\n\n\n\n\n\nLooking at these figures there are not really any clear clusters which pop out. This is another way to see there isn’t a strong difference in community composition across the slope/tree sides"
  },
  {
    "objectID": "code_lab-03.html",
    "href": "code_lab-03.html",
    "title": "Analyzing Community Structure",
    "section": "",
    "text": "In-class analysis\nIn class, form groups. Groups should choose to address either the first or second question using our data.\nYou can access your data here.\nThere’s a lot of data, and I’ve formatted it in different ways. Here is a brief description of each:\n\ninverts.csv: Counts of different invertebrate data, with region, tree, and Tag (bag ID) as columns\nraw_litter-invert-data.csv: Unprocessed data, don’t use\ntree_cat_5m.csv: Trees, group into broad categories, calculated their relative importance value (RIV) in 5m bins along the transect\ntree_cat_subregion.csv: Trees, group into broad categories, calculated their relative importance value (RIV) in the three sub regions of the hillside\ntree_sp_riv 5m.csv: Trees, grouped by species, calculated their relative importance value (RIV) in 5m bins along the transect\ntree_sp_riv_subregion.csv: Trees, grouped by species, calculated their relative importance value (RIV) in the three sub regions of the hillside\ntree_raw.csv: Raw measurements of trees up the hillside with DBH (diameter at breast height), exact transect location, and subregion.\n\nIn your groups, choose which dataset you think will be most useful for your analyses. With your dataset:\n\nIdentify the different types of data\nDecide how to visualize your data\nChoose the appropriate statistical analysis for you data\n\n\n\nLink to my approaches:\nlink"
  },
  {
    "objectID": "code_lab-04.html",
    "href": "code_lab-04.html",
    "title": "Lab 4 Analysis",
    "section": "",
    "text": "Our hypothesis states that less disturbed streams will have better habitat quality, thus supporting a higher abundance of sensitive and moderate taxa while the poor-quality stream will have a higher abundance of tolerant taxa. To do this we have a response variable of taxa abundance (count/continuous data) and a predictor of stream site (Categorical with two levels: “ShoppingPlaza” and “Upstream”). However, we also have taxa group with three levels: Tolerant, Moderate, and Sensitive. Because taxa count is our response variable, will will need three analyses, one for each level of taxa type. Thus we will only compare “Is there a significant difference between the plaza and the upstream site in terms of tolerant taxa abundance”, then ask that same comparison for moderate, and sensitive taxa (it’s like having three different response variables). We will use a t-test, or more likely a mann-whitney U-test to compare the two levels of site for each response variable.\nBefore you get started, make sure to load all the packages you might want:\n\nlibrary(ggplot2) \nlibrary(dplyr)\n\n\n\n\nLook below for where to get which datasets:\n\nStream Quality\nWhile this is not necessary for just comparing two site, we did the stream health assessment form to score the stream on a standarized unit. Reporting these data in a table will look nice and add value to the worksheet. This tells a reader how different these two streams qualitatively are.\nYou can access our stream health assessment data here.\n\nLocal ImportAdvanced Web-Import\n\n\nIf you download the data you can read it in the same way we always have\n\nsetwd('path/to/your/wd')\nstream_quality <- read.csv('stream_quality.csv')\n\n\n\n\nstream_quality <- read.csv('https://raw.githubusercontent.com/USC-Ecology-Lab/Lab_4_Urban-Stream-Ecology/main/data/stream_quality.csv')\n\n\n\n\nYou can check out the data before you get started:\n\nView(stream_quality)\n\n\n\n\n\n\n\n\nFor each site, we have 10 evaluation parameters, on a score of one-to-ten. To summarize this data, we just add up all ten parameters for each site. We can do that using dplyr language:\n\nstream_quality |> \n  group_by(Site) |> \n  summarize(total_score = sum(Score))\n\n# A tibble: 2 × 2\n  Site          total_score\n  <chr>               <dbl>\n1 ShoppingPlaza        53.5\n2 Upstream             71.5\n\n\nSo our qualitative assessment suggests that the ShoppingPlaza is considerably poorer quality than the Upstream habitat. The score is out of 100, so the ShoppingPlaza gets an F while the Upstream gets a C-! This would be a nice table to include in your figures section of the worksheet. Make sure to review how to format table captions (vs figure captions).\n\n\nMacroinvertebrate assessment\nNow we can test the hypothesis. As described above we’ll have three response variables, abundance of tolerant, moderate, and sensitive taxa. We’ll be comparing abundance at two levels (ShoppingPlaza and Upstream). So we need to run three t-tests/Mann Whitney U-tests.\nFirst, you can get your data here.\nYou can load it into r the same way we always have - with read.csv. I’m going to assign the file to a variable named macro_survey\n\nLocal ImportAdvanced Web-Import\n\n\nIf you download the data you can read it in the same way we always have\n\nmacro_survey <- read.csv('macro_survey.csv')\n\n\n\nYou can read it straight from the internet:\n\nmacro_survey <- read.csv('https://raw.githubusercontent.com/USC-Ecology-Lab/Lab_4_Urban-Stream-Ecology/main/data/macro_survey.csv')\n\n\n\n\nNow before we do anything, take a look at the data. You’ll notice that there are records for each survey (kick seine haul) with a row for each possible taxa we observed. Now we don’t really care at that level of specificity.\n\nView(macro_survey)\n\n\n\n\n\n\n\n\nWe want to group things by Tolerant, Moderate, or Sensitive. So let’s create a new dataframe which adds taxa counts into those groups for each survey, we can use dplyr language here and group by tolerance levels, site, and survey. We can assign this new dataframe to a variable called macro_by_group. We will mostly work with this from here on out:\n\nmacro_by_group <- macro_survey |> \n  group_by(Tolerance, Site, Survey) |> \n  summarize(count = sum(Count))\n\nNow let’s get into the meat of this worksheet:\n\nPlotting:Assumption Checking:Analysis:Advanced Code Analysis:\n\n\nWhile we do have three separate analyses to run, we can still fit all the data on the same plot. Again this figure will be similar to some work we’ve done before making a grouped barplot with standard deviation shown.\nFirst, we’ll need to make our summary dataframe which we can plot from:\n\n# create sumamry df\nmacro_plot_df <- macro_by_group |> \n  group_by(Tolerance, Site) |> \n  summarize(mean_count = mean(count),\n            sd_count = sd(count))\n\nNow lets make our plot using ggplot:\n\nggplot(macro_plot_df) +\n  geom_bar(aes(x = Tolerance, y = mean_count,\n               fill = Site),\n           stat = 'identity', position = 'dodge') +\n  geom_errorbar(aes(x = Tolerance, ymin = mean_count,\n                    ymax = mean_count + sd_count,\n                    color = Site),\n                position = 'dodge') +\n  scale_color_manual(values = c('lightblue', 'darkblue')) +\n  scale_fill_manual(values = c('lightblue', 'darkblue')) +\n  labs(x = 'Invertebrate Tolerance Level',\n       y = 'Mean Abundance',\n       fill = \"\", color = \"\") +\n  theme_classic()\n\n\n\n\n\n\nThis figure looks nice! Just by glancing at it, we can see that the tolerant taxa were most abundant on average at both sites. However, more so at the Shopping Plaza. We can can see there is pretty high variation within sites, particularly at the shopping plaza. That could be due to clustering of clams in the sediment or differences in kicking efficiency of samplers.\n\n\nBefore we embark on the statstical analysis, we need to decide whether or not to do a t-test or a mann whitney U-test. The core difference is that the t-test assumes that the data are normally distributed (or at least we have a fairly large sample size).\nTo check for normality we can make some figures. Note these are not figures we need to share (don’t put it in your worksheet). But we want to see this for ourselves.\nFor the simple analyses, I’m just going to work with the tolerant taxa. However, you will want to repeat this analysis for moderate and sensitive taxa. To do this, we can make a new dataframe which filters macro_by_group to just have what we are focusing on. Let’s call it Tolerant_group\n\nTolerant_group <- macro_by_group |> \n  filter(Tolerance == 'Tolerant')\n\nNow we can visualize the distribution of data for each site. Note, I don’t really care if you understand this code (it isn’t ggplot/dplyr so it is a little more complex and uses base R logic). However, the main thing to take away is evaluating if the data are normal:\n\n# check assumption of normality\nTolerant_group$count[Tolerant_group$Site == 'ShoppingPlaza'] |> \n  density() |> \n  plot(main = 'ShoppingPlaza')\n\n\n\nTolerant_group$count[Tolerant_group$Site == 'Upstream'] |> \n  density() |> \n  plot(main = 'Upstream')\n\n\n\n\nWe can see the data are fairly right-skewed and zero-inflated. We also have a fairly small sample size (<25) so let’s use the non-parametric test. This is a Mann Whitney U test.\n\n\nIf you jumped to this tab without doing the assumption checking you’ll have some issues. Mainly because we’ve already made our filtered dataframe. Make sure you have created a dataframe, filtered by tolerance group!\nAgain here I’m just going to do the analysis for Tolerant taxa, but you will want to repeat the same analysis for moderate and sensitive taxa. You can do this by changing filter(Tolerance == 'Tolerant') to filter(Tolerance == 'Sensitive') (or moderate). Make sure that you consider renaming the dataframe from Tolerant_group to sensitive or whatever. I’ll also save you the trouble of plotting and tell you the data are all non-normal and we should use the Mann Whitney test like below for all tolerance levels.\nTo run the mann-whitney U test in R we use the wilcox.test function and the formula layout. There’s some additional arguments as well.\n\nwilcox.test(count ~ Site, data = Tolerant_group,\n            exact = FALSE, conf.int = TRUE)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  count by Site\nW = 180.5, p-value = 0.01102\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n  0.9999634 15.9999727\nsample estimates:\ndifference in location \n              4.999996 \n\n\nHere, we have a small p-value which suggests the two groups are significantly different. Our confidence interval says that we think, with 95% confidence, the true difference in tolerant taxa abundance between the Upstream and shopping plaza point is between ~1 and ~16 individuals per kick-net haul. This lines up well with what we visually can deduce from our bar chart.\nRepeat this for Sensitive and Moderate taxa.\n\n\nHere, I’m keeping with the dplyr approach to filter, but I’m storing all three dataframes in one list. I can make them in a loop. Then I can run the mann-whitney U test in a loop for each taxa and see all output at once! You can see all I’m really changing is rather than typing “Tolerant” as my filter, I loop through the unique levels of Tolerance column in macro_by_group dataframe. Then I can filter on the ‘level’ iterator. I then run the test in the same loop but print out my level at the top so I can make sense of the data:\n\ntolerance_levels <- list()\nfor(level in unique(macro_by_group$Tolerance)) {\n  tolerance_levels[[level]] <- macro_by_group |> \n  filter(Tolerance == level)\n  \n  print(level)\n  wilcox.test(count ~ Site, data = tolerance_levels[[level]],\n            exact = FALSE, conf.int = TRUE) |> \n    print()\n}\n\n[1] \"Moderate\"\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  count by Site\nW = 82, p-value = 0.05878\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -9.999325e-01  1.336044e-05\nsample estimates:\ndifference in location \n          -5.32433e-05 \n\n[1] \"Sensitive\"\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  count by Site\nW = 99, p-value = 0.1\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -6.571325e-05  0.000000e+00\nsample estimates:\ndifference in location \n                     0 \n\n[1] \"Tolerant\"\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  count by Site\nW = 180.5, p-value = 0.01102\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n  0.9999634 15.9999727\nsample estimates:\ndifference in location \n              4.999996 \n\n\nLooking at all the data, we don’t have a significant difference in abundance of the sensitive or moderate groups. This is somewhat surprising and something to think about in your discussion. A key thing to think about is that sensitive taxa are completely absent from the Shopping Plaza site. The lack of significance from the test comes from the fact we don’t have enough data to be confident that the population abundance is significantly different than 0 at the Upstream site. This is where stats and biology can differ a bit. While in a statistics world you are forced to say somthing like “we fail to reject the null hypothesis that there is a significant difference in sensitive taxa abundance between the plaza and upstream sites”. In the biology world we want to evaluate if stream quality effects who lives there. While the sensitive taxa were sparse at the upstream site, they were totally absent from the plaza site. This alone provides weak support for the hypothesis. We also would want to note that we sampled directly following a large rain event at the upstream sight. This could have reduced the population sizes with large flusing and our results may be deflated compared to the average stream value. We might want to consider extending this study to sample at various time points following large disturbances.\nThat is something to address in your discussion.\n\n\n\n\n\nBonus: Diversity Assessment\nIn our original approach, we just compared if there is a significant difference in abundance between the two sites.\nHowever, we could look at a metric like diversity rather than abundance. Let’s do that here!\nFor this analyses, we don’t want to use the processed data. We want to use the raw data. You can access that here\nSince this is bonus, I’m just going to read in from my website directly. You should know how to locally import if that’s what you’re doing\n\nraw_macro <- read.csv('https://raw.githubusercontent.com/USC-Ecology-Lab/Lab_4_Urban-Stream-Ecology/main/data/raw_macro_survey.csv')\n\nWe’ll use code similar to lab 2 here. We don’t necessarily care about the species, just what are the differences in diversity from each kicknet haul?\n\n\n# We can write a function to do this repeatedly!\ndiversity_calculator <- function(count) {\n  p <- count / sum(count) # create counts\n  lnp <- log(p)\n  H <-  -sum(p * lnp)\n  return(H)\n}\n\ndiversity_df <- raw_macro |> \n  group_by(Site, Survey) |> \n  summarise(H = diversity_calculator(Count))\n\nOne issue here is that in sites where there were no taxa, the calculation messes up and reports NaN (not a number). However, we can say that having 0 taxa is the same Shannon Wiener value as all of one taxa (essentially no diversity). So let’s change to NaNs to zeros:\n\ndiversity_df$H[is.nan(diversity_df$H)] <- 0\n\nNow we can compare H between sites with a mann-whitney U test:\n\nwilcox.test(H ~ Site, data = diversity_df,\n            exact = FALSE, conf.int = T)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  H by Site\nW = 100, p-value = 0.3024\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -2.170960e-05  6.642698e-05\nsample estimates:\ndifference in location \n         -9.031466e-06 \n\n\nSo this suggests there is not a significant difference in the H (shannon wiener diversity index) between the two sites. Overall diversity was very low at both locations. However, it is again likely driven by our small sampling size and the large flushing event which may have pushed out some of the sensitive macro invertebrates. However, this is a good example of when indicator species are valuable. If the diversity is low, but occupied by native, sensitive taxa the stream is likely healthier than one with massive invasive populations. This difference is not captured by the a taxonomic agnostic approach like the Shannon Wiener index"
  },
  {
    "objectID": "code_lab-05.html",
    "href": "code_lab-05.html",
    "title": "Phenology Analysis",
    "section": "",
    "text": "Initial Visualization\nFirst, let’s look the general cycles across the years. This is a good approach to get a general idea of bloom time patterns across the years\n\nData Processing\nHere we are going to use the MW_PhenoDat_2013_2019_anonymized.csv dataset. Make sure to set your working directory to wherever you’ve stored the file:\n\nLocal Import:Web Import:\n\n\n\npheno_dat <- pheno_dat <- read.csv('./data/MW_PhenoDat_2013_2019_anonymized.csv')\n\n\n\n\npheno_dat <- read.csv('https://raw.githubusercontent.com/USC-Ecology-Lab/Lab_5_Phenology/main/data/MW_PhenoDat_2013_2019_anonymized.csv')\n\n\n\n\nTake a look at the data frame - it’s fairly large. Its actually too big for the view function to work on this webpage. It might break your R!\n\nView(pheno_dat)\n\n\n\n\nThe main column we are interested in is Flower which is a binary variable of whether or not a given plant in a plot is flowering. However, using some dplyr language, we can easily calculate the proportion flowering in each site at each date:\n\npheno_sum <- pheno_dat |> \n  group_by(Date = as.Date(Date, format = '%m/%d/%y'),Site_Code, Transect) |> \n  summarize(prop_flowering = sum(Flower)/length(Flower))\n\nWe can safely take a look at pheno_sum.\n\nView(pheno_sum)\n\n\n\n\n\n\n\n\nWe can see that there are for each plot, at particular dates a proportion of the flower which have flowered. This can be used for a nice plot. But first, I want to make a figure which has day-of-year on the x-axis and then different years as different colors. So I need to make a new column for day of year. Here I can use the lubridate package:\n\npheno_sum$DOY <- pheno_sum$Date |> yday()\n\n\n\nPlotting\nNow let’s make the plot. I’ll have loess smoothing lines and points. One thing I want to show as well is the different transects of the study. Here, I can use a nifty ggplot feature called facet_wrap where I can specify how to break a figure into two panels.\n\nggplot(pheno_sum)+\n  geom_point(aes(x = DOY, y = prop_flowering,\n                 color = as.factor(year(Date))),\n             alpha = 0.25)+\n  geom_smooth(aes(x = DOY, y = prop_flowering,\n                  color = as.factor(year(Date))),\n              se = F)+\n  scale_y_continuous(limits = c(0,1))+\n  facet_wrap(~Transect)+\n  labs(x = 'Day of Year', y  = 'Proportion Flowering',\n       color = 'Year') +\n  theme_classic()\n\n\n\n\n\n\nThis figure is fairly helpful to get a general intuition of when flowers start flowering. For the most part, they are flowering in summer but there was one early year in 2015. This figure doesn’t get at our hypotheses but it does suggest there are some interesting mechanisms governing flowering patterns.\n\n\n\nStatistical Analysis\nNow we can build some models to help identify what the primary factors governing flowering patterns are. First, we’ll need a few more packages (which you probably haven’t encountered yet). I’m going to load lme4 to build linear mixed models (discussed below) and wiqid which has a function standardize2match which we’ll want to use later.\n\nlibrary(lme4)\nlibrary(wiqid)\n\n\nData Loading & Manipulation\nWe’ll want three new datasets and I’m going to do some fairly advanced processing here. This is less important to understand, just make sure that you copy everything correctly and it should go well.\nWe want the MW_SiteInfo_2013_2020 for elevation data, the MW_SDDall data for snow dis, and the MW_Phenocurves for peak flowering probability which is our main response variable of interest\n\nLocal ImportWeb-import\n\n\n\nsite_meta <- read.csv('MW_SiteInfo_2013_2020.csv')\npheno_calced <- read.csv('MW_Phenocurves.csv')\n\n\n\n\nsite_meta <- read.csv('https://raw.githubusercontent.com/USC-Ecology-Lab/Lab_5_Phenology/main/data/MW_SiteInfo_2013_2020.csv')\n\npheno_calced <- read.csv('https://raw.githubusercontent.com/USC-Ecology-Lab/Lab_5_Phenology/main/data/MW_Phenocurves.csv')\n\n\n\n\nIt’s always good to take a quick look at your data and make sure it is how you are expecting!\n\nView(site_meta)\n\n\n\n\n\n\n\n\n\nView(pheno_calced)\n\n\n\n\n\n\n\n\nWe need to match up elevation values to the pheno_calced values based on the Site_Loc and Year columns. Here we can use some fancy dplyr. However, a tricky issue is that the names don’t match (i.e. pheno_calced has a site_code column while site_meta has a Site_Loc colume). Don’t worry, dplyr makes it easy!\n\npheno_calced <- site_meta |> \n  select(Site_Loc, Elevation) |> \n  right_join(pheno_calced, by = c(\"Site_Loc\" = 'site_code'))\n\nGreat, now we can get into the actual model building!\n\n\nSimple Linear Modelling\nFor this analysis we will be using a variety of linear models. Fundamentally, these models follow the idea of fitting a line to the data. Think of a basic \\(Y = mx + b\\). However, in statistics, you’ll more often see the format:\n\\[\n\\hat{Y} = \\beta_0 + \\beta_1x\n\\]\nWhere \\(\\beta_0\\) is the intercept and \\(\\beta_1\\) is the slope of the line. To get accustomed to this idea, let’s just fit a simple model. Note I’ll be making plots as we make the models because I believe that helps the understanding the best.\nBefore we get started, let’s review the three hypotheses:\n(H1) Earlier snowmelts will facilitate earlier blooms as melts may act as a cue for initiating plant growth, starting the cycle earlier;\n(H2) Because higher elevations typically have harsher conditions, the peak bloom will be delayed at elevation, however;\n(H3) Consistent with climate change theory, higher-elevations will be more sensitive to change resulting in an increased impact of snowmelt changes at altitudes on bloom timing.\nThus, we would expect 1- A significant effect of snow disappearance date (SDD) delaying peak flowering probability date, 2- A significant effect of elevation delaying peak flowering probability date, 3- Increasing elevation to increase the effect that SDD has on peak flowering probability date.\nFirst let’s look at the simple linear model case. This is one variable and one response (one x, one y). In R, we will fit a linear model using base R, with the function lm(). This allows for the function formula notation y~x, data = data.frame. For this exercise, when I run linear model, I will save it to a new variable. On a technical note, this then saves that linear model as a model object which is a list with all sorts of features we can use later. Mainly we just want to get a summary of the model object.\nFor the next several tabs, I’ll explore simple linear models. I will provide most the explanation in the first tab. However, read through all tabs for maximum learning. Note, the actual analysis for this lab will be a more complicated model. However, this is the warm-up. I encourage you to run this all in R, but don’t overly stress on it.\n\nSnow Disappearance DateSDD PlotElevationElevation PlotChecking assumptions\n\n\nHere we can use SDD as our predictor variable and peak as the response.\n\nsimple_sdd_mod <- lm(peak ~ SDD, data = pheno_calced)\n\nsummary(simple_sdd_mod)\n\n\nCall:\nlm(formula = peak ~ SDD, data = pheno_calced)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.739  -7.568   0.484   8.965  48.257 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 136.71865    4.27519   31.98   <2e-16 ***\nSDD           0.44620    0.02482   17.98   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.39 on 600 degrees of freedom\nMultiple R-squared:   0.35, Adjusted R-squared:  0.3489 \nF-statistic: 323.1 on 1 and 600 DF,  p-value: < 2.2e-16\n\n\nThere’s a lot to look at here but some key pieces of information to extract are (1) The estimate and significance of the SDD Coefficient on the model. We can find this information in the Coefficients table on the second row. In statistical language, the intercept and the SDD are our \\(\\beta_0\\) and our \\(\\beta_1\\) respectively. For biology/interpretation, the SDD coefficient is the effect that increasing SDD has on our response variable. So looking at the table, we could say for every day later in the year snow disappears, we’d expect the peak flowering probability date to delay 0.44 days. Many times in a linear model the intercept is difficult to understand in context but here it makes a lot of sense: if the SDD was 0, or if there was no snow in the year, we’d expect peak flowering probability to occur at day 136.72, on average.\nNow, the coefficient reported for SDD is the effect determined in our sample of data, however we are interested in making an inference to the population level. So we need to rely on the statistical test which is performed while the model is fit and we have a p-value at the end of our coefficients table. This test evaluates whether or nor the slope of the line is significantly different from 0 (can we be 95% confident that our sample effect reflects the true-population level effect). Our p-value is less than 2 x 10-16. So we can conclude that if there really is no effect of SDD on peak flowering probability date, the probability of us observing the effect in our sample that we did is effectively impossible.\nOne other key pieces of information from our regression summary is the R2 value. This is essentially the proportion of variance in the y variable that can be explained by the x. In our case, it is 0.35. While only explaining 35% of the variation may seem small, in ecology this is actually considered a fairly good fit. When you think about all the things occurring in nature, the fact we can attribute 35% of the variability to one single factor is fairly impressive.\nThis is all more information that we’d typically write up however. In reality, we’d be more likely to write “There is a significant effect of SDD on peak flowering probability date (\\(\\beta_{SDD}\\) = 0.446, p < 0.001, R2 = 0.35).” Alternatively: “On average, there was a significant delay of peak flowering date by 0.446 days for each day delay of SDD (p < 0.001, R2 = 0.35)”.\nAnother way to interpret the effect of SDD on peak flowering probability date is to use a confidence interval. A confidence interval reports (usually 95%) the range of possible values where we believe the true population level effect is. If a confidence interval includes 0, then the p-value is greater than 0.05 and we would conclude no significant effect (e.g. based on the data collected we can’t conclude there is a real effect). Alternatively, if the effect does not include 0, we can conclude there is some significant effect. Confidence intervals are a nice way to present the data because it gives a more intuitive range of the data than just a p-value.\n\nconfint(simple_sdd_mod)\n\n                 2.5 %      97.5 %\n(Intercept) 128.322491 145.1148179\nSDD           0.397447   0.4949492\n\n\nHere, I can look at the confidence intervals for the coefficients and report “There was a significant effect of SDD delaying peak flowering probability (0.397-0.494 days, 95% CI for \\(\\beta_{SDD}\\)).\n\n\nLet’s make a nice plot to visualize our relationship we described above.\n\nggplot(pheno_calced) +\n  geom_point(aes(x = SDD, y = peak)) +\n  geom_smooth(aes(x = SDD, y = peak),\n              method = 'lm') +\n  labs(x = \"Snow Diss. Date [Day]\", y = 'Peak Flowering Probability [Day]') +\n  theme_bw()\n\n\n\n\nHere, we can visualize a pretty clear relationship! One thing to note is that there is increased variation at early snowmelt dates compared to later melt dates (Fig. 1).\n\n\nNow let’s make a simple linear regression for elevation’s effect on peak date. From an R perspective, I only need to change one piece of the code. However, I’m also going to save the model to a new object name which makes a little more sense:\n\nsimple_ele_mod <- lm(peak ~ Elevation, data = pheno_calced)\n\nsummary(simple_ele_mod)\n\n\nCall:\nlm(formula = peak ~ Elevation, data = pheno_calced)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-59.715  -8.326   3.951  11.617  40.804 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 1.754e+02  1.096e+01  15.992  < 2e-16 ***\nElevation   2.200e-02  6.421e-03   3.426 0.000654 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.67 on 600 degrees of freedom\nMultiple R-squared:  0.01919,   Adjusted R-squared:  0.01755 \nF-statistic: 11.74 on 1 and 600 DF,  p-value: 0.000654\n\n\nBy looking at the data above, we can conclude that there is a significant effect of elevation delaying peak day on average 0.02 days per meter elevation (p < 0.001). However the effect is very weak (R2 = 0.02). When we look at the plot in the next tab,\n\n\n\nggplot(pheno_calced) +\n  geom_point(aes(x = Elevation, y = peak)) +\n  geom_smooth(aes(x = Elevation, y = peak),\n              method = 'lm') +\n  labs(x = \"Elevation [m]\", y = 'Peak Flowering Probability [Day]') +\n  theme_bw()\n\n\n\n\nSo again, we can see that our\n\n\nWhen constructing linear models, we are making several implicit assumptions. There are referred to as the LINE assumptions: Linearity - the data have a linear relationship, Independence of observations - one observation doesn’t influence the next, Normality of errors - the distance away from the regression line are normally distributed across the data (residuals are normal), and equality of variance - there isn’t high variance at one level versus another. These assumptions can all have a variety of influences if broken, but the main ones to look out for are the normality of errors and to see if there are severe outliers (this is a feature of lack of equal variance. In R, we can explore assumptions very quickly by plot -ing our linear model object:\n\nplot(simple_ele_mod)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese are confusing plots with no context. But focus on the first two. The first one is essentially our figure from the plots rotated to be horizontal around the regression line. What we look for here is that the data don’t have a strong funnel shape (residuals are even and homogeneous across the x-axis). It looks pretty good but there’s some weird values at low levels. The second plot is the q-q plot and it tells us how normal our errors are. We want a pretty straight line. We pretty much have that but we can see at the lower tail there’s some leverage. This suggests that at lower elevations, the data get a little non-linear and it might be pulling down the slope of our model.\n\n\n\n\n\nSLR: Takeaways:\nThe simple linear models address the first and second part of our hypotheses fairly well. However, it doesn’t account for the third part of our hypotheses. The simple models also leave some questions lingering. Notably, in the SDD model, there is high variance at early SDD values. This suggests there might be some effect of certain species of plant flowering earlier when snow melt facilitates it. However, in cold years, the snow stays longer preventing all species from blooming. Also in our elevation model, the fit was very poor, so while there was a significant effect, is it really meaningful? We can build more comprehensive models to explore all these questions.\n\n\nBuilding More Comprehensive Models:\nThis dataset and our hypothesis provide the ideal scenario to explore the use of linear mixed models. These models incorporate what are called both random and fixed effects. I’ll provide a lay description here with the next few tabs.\n\nMultiple Linear RegressionFixed vs Random EffectsPutting it togetherModel BuildingFurther reading\n\n\nIn the previous example, we explored singular linear regression. These models fit just a single effect to one y. However, we can include multiple x’s fairly easily. This allows us to take in account how important one factor is while accounting for all others. In statistics language this looks like:\n\\[\n\\hat{Y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n\n\\]\nThus, we can fit as many predictor variables to try and explain a single response variable. We can also fit “interactions” to see how the value of one predictor influences the effect of another:\n\\[\n\\hat{Y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3 x_1x_2\n\\]\nIn the above model, we create a third coefficient for the multiplicative interaction between a continuous \\(x_1\\) and \\(x_2\\). If you’re thinking ahead, you can realize how this will be useful for testing our third hypothesis, we can evaluate how the value of elevation influences the effect of sdd:\n\\[\n\\hat{peak} = \\beta_0 + \\beta_{sdd}SDD + \\beta_{ele}Elevation + \\beta_3 SDD * Elevation\n\\]\n\n\nSo far, we discussed everything in a classic model framework. These are all fixed effects: we are interested in the fixed effect of SDD and elevation on peak date. However, there are other things influencing peak date which we haven’t yet thought about. Specifically the species of flower\nA random effect can be added to a model to account for some source of variation we know might be there but aren’t particularly interested in. This is typically used when we have a bunch of different levels of some categorical factor but don’t care about the impact of each of those levels. In our case, we are interested in a broad scale pattern, but we know there might be some effect that species has on peak date. A random effect assumes that there is some group-wide average (an average flowering peak date), and all the different levels (different flowers) are pulled from a normal distribution around that average. In our case, this is a pretty intuitive application. Another area of variation could be the observation plot itself. Since there were different plots observed at different locations, there might be some mid-scale patchiness effects which we aren’t particularly interested in but want to account for.\nAt a statistical level: we are effectively fitting a separate intercept for each instance of a different flower species/ observation plot. But the intercepts themselves are fit to some normal distribution, constraining their effects.\n\n\nWe can build a mixed effect model to incorporate both the fixed effects we are interested and the random effects we want to account for. In our case, we will model SDD and elevation as fixed effect while accounting for flower species and plot as random effects.\nA key difference between a mixed effects model and a multiple linear model is the algorithm used to fit the model. A typical regression (linear model) uses Least Squares (called ordinary least squares or OLS regression) to fit the line to the data. However, because in a mixed effect model, we assume the random effects come from some normal distribution. It will use a Restricted Maximum Likelihood fit (REML). (Super technical: OLS is a maximum likelihood method, just not restricted on the effects and allows each effect’s its own normal distribution).\n\n\nWhen building a model, the goal is parsimony: how can we provide the most simple example while accounting for the maximal variation. There are statistical tests to compare between the models to compare metrics for how well a model describes the data in the fewest parameters possible. Some common metrics to compare are AIC, BIC, and R2 adjusted. We’ll use AIC, arguable the most common. The value itself is not necessarily meaningful to use but we are interested in a lower value. We can compare models using an ANOVA to see if it is a significantly better fit.\n\n\nhttps://stats.oarc.ucla.edu/other/mult-pkg/introduction-to-linear-mixed-models/\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC5970551/\n\n\n\n\nBuilding the model in R:\nOk, that was a lot of background. Now, let’s get into the model building process in. I will create four models with increasing complexity, starting with just a single random effect and fixed effect then adding them in.\nOn a technical note: when constructing a multiple-effect model, it is recommended to center the variables. This allows for when data have different magnitudes to compare their effects directly. In our case, this is very necessary as elevation is on the order of 1000’s while sdd is on the order of 10’s. So if we didn’t center the variables, it can introduce some problems for model fitting and evaluation. In R we can do this with scale()\nIn R, we will need to use a package called lme4. There’s a few packages for mixed effects models but this is a common one.\n\nlibrary(lme4)\n\nNow we’ll use the lmer function rather than lm.\n\npeak_mod1 <- lmer(peak ~ scale(SDD) + (1 | species), data = pheno_calced) \npeak_mod2 <- lmer(peak ~ scale(Elevation) + scale(SDD) + (1 | species), data = pheno_calced)\npeak_mod3 <- lmer(peak ~ scale(Elevation) + scale(SDD) + scale(Elevation) * scale(SDD) + (1 | species), data = pheno_calced)\npeak_mod4 <- lmer(peak ~ scale(Elevation) + scale(SDD) + scale(Elevation) * scale(SDD) + (1 | species) + (1|Site_Loc), data = pheno_calced)\n\nGreat, now we’ve made our models, let’s evaluate them.\n\nanova(peak_mod1, peak_mod2, peak_mod3, peak_mod4)\n\nData: pheno_calced\nModels:\npeak_mod1: peak ~ scale(SDD) + (1 | species)\npeak_mod2: peak ~ scale(Elevation) + scale(SDD) + (1 | species)\npeak_mod3: peak ~ scale(Elevation) + scale(SDD) + scale(Elevation) * scale(SDD) + (1 | species)\npeak_mod4: peak ~ scale(Elevation) + scale(SDD) + scale(Elevation) * scale(SDD) + (1 | species) + (1 | Site_Loc)\n          npar    AIC    BIC  logLik deviance   Chisq Df Pr(>Chisq)    \npeak_mod1    4 4499.4 4517.0 -2245.7   4491.4                          \npeak_mod2    5 4497.4 4519.4 -2243.7   4487.4  4.0742  1  0.0435429 *  \npeak_mod3    6 4487.7 4514.1 -2237.8   4475.7 11.7049  1  0.0006234 ***\npeak_mod4    7 4425.1 4455.9 -2205.6   4411.1 64.5428  1  9.446e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOk, looking at our ANOVA results, each model is significantly different, indicating that our final model is the best one. So let’s look at the summary:\n\nsummary(peak_mod4)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: \npeak ~ scale(Elevation) + scale(SDD) + scale(Elevation) * scale(SDD) +  \n    (1 | species) + (1 | Site_Loc)\n   Data: pheno_calced\n\nREML criterion at convergence: 4403.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.7583 -0.6136 -0.0759  0.5242  4.7144 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Site_Loc (Intercept)  41.94    6.476  \n species  (Intercept) 233.19   15.271  \n Residual              70.02    8.368  \nNumber of obs: 602, groups:  Site_Loc, 33; species, 17\n\nFixed effects:\n                            Estimate Std. Error t value\n(Intercept)                 213.4703     3.9274  54.355\nscale(Elevation)             -1.6423     1.1665  -1.408\nscale(SDD)                   17.4380     0.5134  33.963\nscale(Elevation):scale(SDD)   1.6052     0.5089   3.154\n\nCorrelation of Fixed Effects:\n            (Intr) scl(E) s(SDD)\nscal(Elvtn)  0.066              \nscale(SDD)   0.024 -0.165       \nsc(E):(SDD) -0.065  0.236  0.010\n\n\nThe fixed effect table is good as it provides an estimate for our coefficients, but it doesn’t provide a p-value, we’ll just use confidence intervals instead:\n\nconfint(peak_mod4)\n\n                                  2.5 %      97.5 %\n.sig01                        4.5833985   8.7173791\n.sig02                       10.8213011  21.8755592\n.sigma                        7.8841047   8.8769593\n(Intercept)                 205.6003468 221.3394586\nscale(Elevation)             -4.0068515   0.6321201\nscale(SDD)                   16.3457915  18.4666094\nscale(Elevation):scale(SDD)   0.6130762   2.6058664\n\n\nGreat, we can see there is a significant effect of SDD but not elevation (once accounting for the variation in sdd and species/plot effects). However, we do have a significant effect of elevation on the effect of sdd –> there is a significant interaction! This means that as we increase elevation , the effect of sdd increases as well (95% CI: 0.61-2.61). Another thing to note is that the scaled data are not directly interpretative for the effect. Technically we can say: for each unit increase of scaled SDD, there is a 1.6 day delay in average flower peak. But that isn’t very meaningful, we need to unscale the data to interpret it. This creates a challenge for plotting:\n\n\n\nWhat should I takeaway?\nWhen constructing multiple models, it was found the ideal model included fixed effects as Elevation, SDD, and the interaction of those terms and random effects to account for flowering species and observation plot (AVONA, p-value < 0.001). In this model it was found that there was a significant effect of SDD and the interaction term. However elevation itself was not significant.\nThis supports our first and third hypotheses, but did not support the second one! Some possible explanations for that might be that the high-elevation plants are adapted to flower at harsh conditions but are senstive to changes as they might be specialists.\n\n\nPlotting the model results!\nNow we want to make a plot to display the effect of SDD on peak. WE ALREADY MADE THAT PLOT ABOVE!. However, the line fit by ggplot uses a basic linear model fit. We want the slope of the line to reflect the more complicated model.\nAs mentioned above, we need to plot the effect using scaled data. Do to this I’m going to use the standardize2match function from the wiqid. This function takes two vectors and will standardize the first vector to match the variation of the second one. For our purposes, I’m going to match a range of values which reflect the range of SDD:\n\nsdd_scaled <- standardize2match(c(104:204), pheno_calced$SDD)\n\nI saved my scaled-data as a new variable. Now I can generate predicted values from the model. Note I’m getting these values from the summary of fixed effects above.\n\nmod_predicted_peak <- 213.47 + 17.438 * sdd_scaled\n\nNow we have enough data for our plot:\n\nggplot() +\n  geom_point(aes(x = SDD, y = peak), data = pheno_calced) +\n  geom_smooth(aes(x = c(104:204), y = mod_predicted_peak),\n              method = 'lm') +\n  labs(x = \"Snow Diss. Date [Day]\", y = 'Peak Flowering Probability [Day]') +\n  theme_classic()\n\n\n\n\n\n\nThis is the plot you should include in your worksheet. For comparison, this is the same plot with the original line added in black:\n\nggplot() +\n  geom_point(aes(x = SDD, y = peak), data = pheno_calced) +\n  geom_smooth(aes(x = c(104:204), y = mod_predicted_peak),\n              method = 'lm') +\n  geom_smooth(aes(x = SDD, y = peak), data = pheno_calced,\n              color = 'black', method = 'lm') +\n  theme_classic()\n\n\n\n\nWe can see when we account for the random effects and elevation, the average slope of SDD on peak is steeper. We know that this is driven in part by the interaction of elevation and SDD, when elevation is higher, the impact of SDD is higher on peak.\nFor your worksheets, I don’t expect you to run all the models we ran in the tutorial. I mainly am interested in you reporting the best model and its associated plots. If you were to do this type of analysis for a full paper, it is pretty common to include a table comparing the model effects which we described above."
  },
  {
    "objectID": "code_main.html",
    "href": "code_main.html",
    "title": "Programming in Ecology",
    "section": "",
    "text": "Broadly speaking, ecology is a field focused studying organisms and how they interact with others and their surroundings. This often involves identifying, describing, and hypothesizing about ecological patterns. In the pursuit of studying such patterns, ecologists in the modern world utilize a vast array of tools to collect and analyze data. Ecological data spans a wide range of formats and sampling distributions. As ecologists, we must be able to properly interrogate our data so that we can identify meaningful trends. That is where the application of statistics comes into the picture. Simply collecting data and making general conclusions cannot inform thorough conclusions. Analyzing data through the proper application of statistical tests will help us as scientists.\nWith the advancement of technology and statistical computing, ecological data analysis has progressed beyond the point of simple calculations. The large amounts of data acquired in ecology require us to be proficient programmers and statisticians. Luckily, there are several, free tools which are growing in popularity for people to use, making data analysis extreme accessible. Arguably the most common tool used in ecology, academia, and data science is R. R is an open-source programming environment and language. R was developed primarily by statisticians which makes it extreme versatile. Additionally, because so many ecologists use it, there are many add-ons (called packages) which are taylor-made for ecological applications.\nIn this course, I will provide examples and support for processing lab data in R. The goal is that someone who has never used R will be able to still be successful. This means at times I may over-explain somethings. Alternatively, it is difficult to remember what it was like to first learn something, so if I am not explaining anything well, please come ask me! Finally, when it comes to completing your assignments, I don’t care what software you use to get it done. For example, if you have to make a bar graph, you might know how to complete something quicker in excel than in R. So, go ahead and use what is most comfortable. However, time invested in learning R will only better equip you to expand your skills.\n\n\nGoogle, chatGPT, and Stack Overflow are all extremely useful resources for solving any issues you may encounter while working with R. Often times if you get an error, simply copy and paste it in a search and there’s likely someone else who experienced a similar issue. This is a fundamental piece of the process. Working with R (or any programming language) is a rarely a smooth process. Don’t be dissuaded by needing to search and solve issues. It’s all just part of the game."
  },
  {
    "objectID": "code_main.html#downloading-r-studio-r",
    "href": "code_main.html#downloading-r-studio-r",
    "title": "Programming in Ecology",
    "section": "Downloading R studio & R",
    "text": "Downloading R studio & R\nGo to the posit website at https://posit.co/download/rstudio-desktop/. This website has instruction for downloading both R and RStudio.\nWhen downloading R, select the right one for your computer at the top panel. If on the initial installation it asks you to select a mirror, it doesn’t matter. Just select whatever.\nOnce you have both R and Rstudio downloaded, you can just open Rstudio."
  },
  {
    "objectID": "code_main.html#rstudio-layout",
    "href": "code_main.html#rstudio-layout",
    "title": "Programming in Ecology",
    "section": "RStudio Layout",
    "text": "RStudio Layout\nWhen you first open RStudio, there will be three main tabs, the largest of which is the console. On the right are two windows with multiple tabs. The preselected tab on the bottom is a file browser and the one of top shows what is active in your R environment. At the start this is empty but as you create variables or functions they are visible in this window.\nMy setup is a bit different but you can see a good orientation at this website."
  },
  {
    "objectID": "code_main.html#understanding-r-code",
    "href": "code_main.html#understanding-r-code",
    "title": "Programming in Ecology",
    "section": "Understanding R code",
    "text": "Understanding R code\nThere are several great resources for learning R in detail which I’ll link at the bottom. Very briefly, I want people to understand some very basic items:\nComments are great for understanding code. You should include it for your future self to refer back to. I will include comments in all our class code.\n\n# comments are written on lines starting with hastags. These will appear different.\n# Comments can also be written after code\n2+2 == 4 # running this line will return TRUE\n\nWe can assign values to objects using an “assignment operator”. Traditionally, in R this is an arrow <-. You can also use an equal sign =. I prefer the arrow for a number of reasons but it is also intuitive. You take all the values on the left and put them into the storage variable on the left.\n\nx <- 5 # take 5 and put it into x\ny = 5 # take 5 and put it into y\n\nprint(x == y) # will print TRUE\n\nz <- x * y # take the product of x and y and put it into z\nprint(z) # print z\n\nFinally, functions are operations which can be preformed on values/objects. Functions are executed by feeding arguments into a call. This generally looks like function(arg1, arg2, ...). This is very similar to excel where you would write =function(arg1, arg2) in a cell.\nNote in the examples above, print is a function. You can learn more about any function through the documentation. Simply write into the r console ?function. where the function is listed.\n\n?print\n\nFunctions can also be wrapped around one another and are preformed inside-out. Standard order of operations. For example:\n\n# c() is a function short for concatenate or combine. it chains together values\nc(5,5,3,4)\n\n# the mean function takes the mean of a range of values.\n# so you could do this two ways:\n\n# option 1:\nx <- c(5,5,3,4)\nmean(x)\n\n# option 2:\nmean(c(5,5,3,4))\n\nA unique aspect of R, which has recently been added to the base functionality\nThis is an extremely simplistic overview. I encourage everyone to look at some of the additional resources for help with learning R."
  },
  {
    "objectID": "code_main.html#organizing-your-computer-files-importing-to-r",
    "href": "code_main.html#organizing-your-computer-files-importing-to-r",
    "title": "Programming in Ecology",
    "section": "Organizing your computer files & importing to R",
    "text": "Organizing your computer files & importing to R\nMany people do not maintain a clear organization of files in their computer’s storage. This problem is compounded by .\nA common problem that many students using the university’s office licence run into is that their word/excel documents are saved into their University cloud storage (onedrive). Then it can be tricky to find those files through a programming approach. If you have a good file organization system, please keep to it and you should be fine. However, if you haven’t put much thought into organizing your files, I encourage you to create a new folder for this course. You can store all your files in that folder. Then when trying to work with R, you can load your files from that path.\nSo let’s talk about file paths. In windows, you can use the file explorer to look up files. Then you can access the path by clicking on the top bar. Typically, it will look something like this: ‘C:\\Users\\yourname\\Documents\\EcologyLab’. In Mac, it will look a little different. Notably, the slashes will be front-slash instead of back-slash. If you try to read a file from R, mac users can just copy-paste their file paths. However, windows users need to write it out with a front-slash. Alternatively, you can use double-backslashes but this is a bad habit to develop.\nIf you want to be an advanced R user, you should learn about organizing your files in a package format and taking advantage of RStudio’s project feature. You should also use relative file paths and set-up projects using git. However, this is beyond the scope of the course and likely most applications for ecology projects.\nRead more about file organization in the additional resources page.\n\nReading in files\nThe most common US-based data format is a comma separated value file (*.csv). However, most people are more familiar with working in excel which has its own file format (.xlsx or .xls). There are several R packages to help you read in excel files. But for the sake of keeping things simple this course, we will stick to .csv files.\nYou can save any excel file as a .csv. Just make sure you keep your headers simple.\nTo read in a file, you can call read.csv(). This is where file paths become important. In R, you have a working directory. This is where R is currently looking for files. You can look at your working directory with the function getwd(). Also RStudio displays the working directory at the top of the console window. Note that the tilde (~) often will refer to your Documents folder. At least for Windows machines.\n\nYou can change your working directory with setwd(). You simply put the file path to the folder where you want to access data from.\nIn short, loading files might look like this:\n\nmy_path <- '~/BIOL570L Docs' #replace this with your path file\nsetwd(my_path) #reassign working directory. This is only necessary if not already there\nmy_data <- read.csv('mydata.csv') #load data\n\n#altarntively\nmy_data <- read.csv('~/BIOL570L Docs/mydata.csv')"
  },
  {
    "objectID": "code_main.html#packages",
    "href": "code_main.html#packages",
    "title": "Programming in Ecology",
    "section": "Packages",
    "text": "Packages\nA great benefit of R is that there are user-made packages which contain functions and data for particular purposes. Most functions I’ve used in this example are ‘base’ R, meaning they exist by default. Other functions can be loaded by packages. To keep R simple, most packages are not loaded by default. To load a package, you must first install it. Luckily, R does a great job maintaining all packages in R, through the Comprehensive R Archive Network (CRAN). So you can download CRAN packages directly though R.\nTo install a package, you can use the install.packages() function.\n\n# we will use ggplot2 in our first assingment so you'll need to download it.\ninstall.packages(\"ggplot2\")\n\nThen each time you need to utilize a package in an R script, you will need to load the package. Typically this is done at the top of a script. You can load a package with the library() function.\n\nlibrary(ggplot2)\n\nNote that when installing we use quotes and when loading we don’t. It’s tricky that way. You should only need to install a package once unless you need to update it. Often times beginners in R get stuck on trying to use functions they haven’t loaded. So make sure you called library() if you get an error about not finding a function."
  },
  {
    "objectID": "gen_expectations.html",
    "href": "gen_expectations.html",
    "title": "General Expectations for Scientific Writing",
    "section": "",
    "text": "All these elements are generally based on the core concepts of a scientific paper. Broadly speaking, scientific papers follow a funnel structure. The introduction takes big-scale ideas about the field of knowledge, identifies what missing knowledge there is (what are your core questions), then introduces the specific study and hypotheses. The methods and results are narrowly focused on the exact details of the study. Then the discussion takes those results, evaluates them in a narrow sense and then places them in the context of larger ideas / other studies.\n\n\nBackground / Introduction Statements\nThe background of your assignments should focus broadly, starting with general ecological ideas, then focus into the specifics of your investigation. Often times, this means introducing the broad theoretical or hypothetical underpinnings of your topic. Then you introduce the specifics of your study system and how it relates to those broad scale system. Then finally, you introduce the specific aims of the investigation (hypotheses, goals, questions).\nTo accomplish writing these sections, it is highly necessary to utilize primary literature and/or review papers to introduce the background. These should be written in a way that someone with a basic ecological background can learn and understand this study.\n\n\nHypotheses:\nIn a full paper, the main questions, hypotheses and predictions are listed in the Introduction portion (Typically in the last paragraph). For the project plans, worksheets, and grading, I’ve listed them separately. That is because of how critically important a good understanding of a hypothesis is.\nFar too many people do not adequately distinguish between hypotheses, predictions, and theories. While this is in part a societal problem - people often refer to a hypothesis they have as a theory - we will not fall for it in this class. A hypothesis is a statement about how things work. It is a proposed explanation for an observed pattern. However, what makes a good hypothesis is that it is explicit in mentioning a mechanism which can be tests. Hypotheses should have a clear prediction which can be derived from them, that we then seek to test in our observations.\n\n\nFigures:\nAs stated in the syllabus, you are welcome to create figures however you wish. I am teaching the course in R and encouraging you to give it a try. Regardless of how you created your figure, there are some elements which should be universal. I’ll discuss them in detail below. However, one thing that every figure should do is represent a summary of the data in some quick way. Really, the purpose of a figure is to show your results and key messages in an easy-to-digest way. If you were really excited about some scientific finding, you could put all the data into a google-sheet, share the link and send it out to the masses. However, who would actually engage with your data? Probably no-one unless they were extremely interested in whatever sub-discipline you were researching. Figures should be readily accessible ways to share information. People are extremely visual creatures. We are exceptionally good at recognizing patterns… even if they are not there. Think about how often you looked at a cloud and recognized some familiar entity.\nThus, make a good figure follows the classic saying: with great power comes great responsibility. We always want to present our data in a clear figure. But we also want to make sure that figure accurately reflects our data.\nThere are plenty of great resources out there about creative ways to summarize your data. One of my favorites is the r graph gallery.\n\nAxes titles:\nAll axis on a figure should have a title. Only sometimes, does a categorical axis have enough detail in the labels to not warrant an axis title. Units, when needed, should be included as well. If there is a legend, you can include a title for it if necessary but sometimes it should be evident.\n\n\nFigure Color:\nTraditionally, figures simply relied on black, white, and grey due to the cost of printing color articles. Now that there are web-based publications primarily, color is increasingly common. However use color with caution. You don’t want it to look unprofessional. Additionally, keep in mind that many people are color blind, so it is important to make the figure accessible to all types of vision. Check out some great resources on color-blind friendly, palettes here.\n\n\nError bars & sources of variation\nBecause figures often have summary data shown, we want to make sure that our figure has some measure of variation shown. Many times we show the standard error, hence the term error bars. However, I prefer to show the more raw data with standard deviation shown. Either way, make sure when creating a figure, you evaluate what variation needs to be shown in the figure and you communicate what is shown in the figure caption. If this is not possible with the figure you’ve created, make sure you are confident in your figure choice.\n\n\nFigure Captions\nEvery figure you submit should have a figure caption. We are striving to communicate in a way which is consistent with general conventions in science. You’ll notice as you read papers, very, very, very, very, very, very, very rarely would a figure ever have a title at the top. Rather figures have captions which provide a little more detail about the image above.\nThis is not to say that figure titles are completely obsolete. While academic articles often avoid titles, they are fairly common in popular writing. You might notice in a book you are reading or a news article, graphs typically come with a quick title. Like on this page, I used a title because it is a web-article. Note the key difference is that titles are generally going to be short, while figure captions contain a lot of detail.\nA figure caption should follow the general format:\nFigure 1. Statement describing the figure briefly. Important stats for the figure. Another important detail.\nCaptions should be terse and informative. Every figure should be “stand-alone”. This means that you should include enough detail where someone who hasn’t read the paper could glance at the figure and understand what is going on. This doesn’t mean you need to write an entire methods and results, but provide some context. If you are looking for a good example, just go to google scholar and search for some topic you are interested in. Take a look at how they format their figures in published, professional work. Also note figure captions go below the figure.\n\n\nMaking tables:\nYou can also provide a table to summarize data in a quick way. Tables are dangerous as people often want to fill them with content which is unnecessary. That said, a well-made table can really improve a manuscript. When making your tables, a key difference is by convention, table captions go ontop, while figure captions go on the bottom. They are also counted separately, (Figure 1)…(Figure 2)…(Table 1)…(Figure 3), etc.\n\n\nSome Figure Don’ts:\n\nDo not include a title unless appropriate\nDo not start your figure captions by stating “Figure 1 shows….” or “As seen in this figure….”.\n\n\n\n\nResults Statements:\nWhen writing your results, you are describing your data and key findings in words. Note you shouldn’t simply regurgitate all your information in a lengthy list. You want to present the data in a narrative form. The results should include references to your figures and statistics when appropriate. When reporting statistics, make sure to include not just the p-values, but also effect sizes as that is what we are typically interested. I will even accept reporting of confidence intervals rather than p-values.\nHowever, a common pitfall many beginning scientific writers fall into is just stating their stats/ figures.\nFor example:\n\nFigure 1 shows a clear pattern between the observed groups. The first group was larger than the second. A t-test revealed a significant difference (p < 0.001).\n\nThat’s not very easy to read and does not clearly show that you, the writer, understand what you are saying. Think about it as describing what happened (past tense). For example:\n\nThe first group’s average body weight (##kg) was significantly larger than the second group’s average body weight (##kg) (Figure 1, p < 0.001, t-test).\n\nNote in this second case demonstrates a lot more information. We narrate the findings and put supporting details in parentheses. The results should narrate both significant (from a statistical test) results, and interesting trends. For example, if you plotted a regression, with a ton of variation around the linear model, you might say “While there was a significant relationship between x and y (Figure 1, linear model, p-value = 0.002, \\(R^2\\) = 0.24), there was extreme variation in the y variable at high values of x (Figure 1).”\nAnother commonly discussed theme when writing results is to “not interpret the results in the discussion.” This phrase, if you’ve heard it before, is suggesting that you shouldn’t evaluate the hypotheses in the results section. That means, don’t say “Clearly, the hypothesis was not supported” in the results, save that for the discussion.\n\n\nDiscussion Statement:\nThe discussion should evaluate results in context of your original hypotheses and other general ecological ideas. You should suggest alternative hypotheses to explain your data or unexpected trends. Note that this should always be in the context of your results.\nFor example, you should never just state “Human error counting may have skewed results”. Such a statement doesn’t add any information.\nInstead, you might write “While there was a large difference between the two forested regions, there was also huge variation within the sub-plots. Such variation might be natural, however differences between student’s accuracy in counting may have contributed. Yet, Smith et al. (2021) observed similar levels of variation between closely-located soil sites. Thus, it is likely the variation is a real signal.” That text is all made-up but should convey a general idea as to how these things should be written.\nIn a full paper, the discussion should utilize prior research as well, citing relevant sources. For our worksheets, I am mainly expecting a discussion of the current research. Thus, citations are not necessary (for worksheets), just a thoughtful consideration of the sampling design, data, and alternative explanations.\n\n\nGeneral Writing Style:\nWhen writing scientifically, we typically take a slightly different tack than when writing for other purposes. Scientific writing often utilizes the passive voice. This is where the direct object takes the focus of the sentence and the subject (the one doing the action) is less emphasized. For example:\n“The samples were collected in a randomized grid format”\nRather than”\n“We counted using a randomized grid format”\nThat said, many traditionalists will argue that you should never use the first person when writing scientifically. I completely disagree. There are appropriate times to use difference tenses, voices, and persons when writing scientifically. Generally, while methods are written passively, an active voice is useful when describing a subjective decision made by the researchers. For example:\n“While samples were collected along quadrats placed systematically in intervals along a transect, we excluded plots which had clearly been altered by evil fire-monsters as they were outside the scope of this investigation. In such a case, the plot was omitted from the dataset.”\nFinally, there are levels to writing well. And largely, it comes with practice and revision. I typically write things very poorly on my first pass, then have to self-revise for clarity. When you are writing however, I encourage you to think about how to write briefly, yet communicate all needed information. Also, while there are certain expectations in the rubric, you should not need to write them explicitly (all the time). For example:\n“An alternative hypothesis would be that squirrels just a lazy losers.”\nThat type of sentence does not actually enhance the writing, it just shows you attempted to meet the requirement. Instead:\n“While the squirrels observed in this study were largely resting, it may have been that this is a diel pattern. Notably, it was extremely hot outside. Possibly, squirrel activity varies throughout the day and this variation was not captured in our sampling design. To fully investigate the hypothesis, squirrels ought to be observed at multiple points throughout the day.”\n\n\nGeneralized Rubric for Common Elements:\n\n\n\n\n\n\n\n\n\n\n\nCategory\nInsufficient\n(<60)\nMajor Revisions\n(60-75)\nMinor Revisions\n(75-85)\nMeets Expectations\n(85-90)\nExemplary\n(90+)\n\n\n\n\nIntroduction/\nBackground\nBackground is missing literature or not appropriately cited. Content is confusing and does not follow a logical progression. Writing displays a lack of direction or understanding.\nBackground is present but cited studies are not relevant. Some information may be slightly inaccurate or not well described. Detail is lacking overall.\nThe introduction is lacking in some small aspect. Literature may be present yet not well placed or in context. Funnel structure is wobbly. Hypotheses do not clearly come from background.\nThe introduction utilizes 3+ published studies to have background. Major ecological concepts are briefly discussed then transitioned to study specifics. Some inconsistencies may be present.\nThe introduction incorporates a comprehensive review of major hypotheses utilizing literature to establish state of knowledge on topic. Introduces study system and narrowly focuses into hypotheses\n\n\nHypotheses\nSeveral potential issues:\nOnly predictions are listed,\nhypotheses are incoherent,\nWriting is unclear to the point of complete confusion\nHypotheses may be present but predictions are lacking or unclear. Hypotheses are clearly untestable. other major issues with study ideas.\nHypotheses and predictions are listed, although they may be lacking a small component. There may be some mismatch between hypotheses & predictions\nHypotheses are clearly stated with predictions. Predictions are testable and match study design\nHypotheses are clearly stated and logical extensions from background. Hypotheses have direct predictions which can be logically derived from them into a testable study.\n\n\nMethods\nMethods are not well thought-out and clearly do not relate to hypotheses or context of study.\nMethods describe collection and analyses but have a fundamental flaw which compromises the study design. Or massively unclear how data may be connected to the study.\nMethods have data collection and analysis described. There may be some unclear sections. Study design may have some potential flaws with analyses.\nMethods are described with data collection and analysis well described. Some detail may be missing about exactly why/how a procedure was/will be done.\nThe methods are well thought-out. Described data collection and analyses methods are consistent with a goal of directly testing the hypotheses. A clear, comprehensive understanding of data is displayed\n\n\nFigures\nFigures do not accurately show the data. There is a clear issue with how information is presented.\nFigures are missing major elements. Or figure does not have an appropriate use of showing data. Confusing or unrelated to the project\nData are well summarized however some minor element may be missing. Figures are still coherent\nData are well summarized, all captions, axes titles, etc are present. Figures are able to be interpreted without context\nFigures are clear, creative, and aesthetically thoughtful. Data are well summarized, all captions, axes titles, etc are present.\n\n\nResults Statements\nResults statement is incorrect in its description of data\nSeveral potential issues may have occurred:\nResults do not utilize correct analyses\nResults do not correctly reference figures\nResults are not detailed enough in the description of data\nResults have too much or irrelevant detail which distracts from an overall message\nResults list key findings and statistics however, may be lacking in a complete description or missing minor elements.\nResults correctly utilize statistics and figures. Statistics are reported using appropriate metrics and effect sizes. Statements are clear and correct.\nResults are brief, yet informative. Statistics are correctly utilized and described well in statements. Results is an easy to follow narrative format and highlights key findings accurately.\n\n\nDiscussion Statements\nresults are incorrectly interpretted. Outside studies are not utilized or utilized incorrectly. Overall discussion is severely limited.\nHypotheses are evaluated, yet there is a major disconnect between results and discussion or lacking in key areas.\nDiscussion meets minimum requirements of hypothesis evaluation and connection to other studies. Yet is limited in the overall discussion of ideas.\nDiscussion evaluates results & original hypotheses. Makes some connections to other studies and other potential ideas.\nThe discussion is a comprehensive evaluation of the results from this study. Makes clear connections to other studies both the place results in context but also evaluate alternative trends/ideas.\n\n\nGeneral Quality\nDemonstrates a lack of effort, or confusing at multiple points to the stage of difficult to understand.\nConfusing in some sections. Writing is choppy or does not follow scientific standards\nWriting is clear generally but in some sections lacking or inconsistent.\nWriting is clear, may be constrained at points but consistently accurate throughout.\nWriting is quality and scientific. It is natural to read, clear, and demonstrates a thorough understanding of ideas.\n\n\n\n\n\nSubmitting Assignments for Revision:\nNow that this is clearly available, I am going to increase the harshness with which I grade. However, I don’t want people getting caught up on assignments as we progress through the semester. Ultimately the goal of the worksheets and the project proposal is for you to learn how to go about thinking, working, and writing like a professional ecologist. Over the next several worksheets, I will grade with detailed comments and feedback, according to the general rubric above. However, I really encourage you to learn from these assignments, not just get penalized for not having done it.\nSo for the worksheets, I am going to allow you to submit revisions.\nThe project plan will also be eligible for revisions.\nRevised submissions will be eligible to receive 66.66% of the lost points on regrading. For example, if you received a 38/50, you lost 12 points. If you fully addressed all concerns in your revision, you can get 8 points back. I settled on this exact percentage as it allows most people to improve your grades\nA few things to note:\n\nAssignment 1 was not graded to the new standards described here. Mainly the figure captions were absent and I did not grade for those. However, revised submissions will be expected to be completed according to details described above. Yet, I will allow assignment 1 to have 100% revision credit because it will require a higher-bar than the initial expectations.\nAssignment 2 while technically not due until after this page was written, it was assigned before. So while I will grade it following these standards, assignment 2 is eligible for 100% revision credit as well.\nAll worksheet revisions are due by the end of fall break (10/19). Please send me an email if you submitted a revision you would like to have graded.\nRevisions are only eligible for a singular submission. No multiple re-submits.\nProject plan revisions will be more flexible depending on individual situations"
  },
  {
    "objectID": "gen_R-guide-plotting.html",
    "href": "gen_R-guide-plotting.html",
    "title": "A guide to making basic plots in R",
    "section": "",
    "text": "Making good plots requires researchers to be well-informed about ways to best communicate their data. Below, I’ll provide a quick guide for making plots for different types of data. Like most of this course, this information is far from exhaustive. Yet, it might provide a good launching pad to explore ideas. Other good resources for making nice plots are the R graph gallery, and asking ChatGPT.\nIn this example, I’ll use ggplot2, so if you are interested in recreating these figures in your own machine, make sure to load the package. However, I’ll also display how the plot() function in R allows for versitile and quick plotting. This can be used to get a quick idea about how to best plot your data. We’ll also use dplyr"
  },
  {
    "objectID": "gen_R-guide-plotting.html#the-simplest-form",
    "href": "gen_R-guide-plotting.html#the-simplest-form",
    "title": "A guide to making basic plots in R",
    "section": "The simplest form:",
    "text": "The simplest form:\n\nBase R plotting\nPlotting in base R allows for a similar format to many of the functions you’ll see in the stats package where you can use what I call “formula” structure. That is, rather than specifying the x and y axis, you can specify a relationship with ~.\n\nplot(Petal.Width ~ Petal.Length, data = iris,\n     xlab = 'Petal Length [mm]',\n     ylab = 'Petal Width [mm]')\n\n\n\n# The same plot could be achieved with\n# plot(iris$Petal.Width ~ iris$Petal.Length)\n# plot(y = iris$Petal.Width, x = iris$Petal.Length)\n\n\n\nggplot approach\nplotting in ggplot may seem a little more complicated. However, in the long-run it facilitates better looking graphics with easier-to-read code. The fundamental idea behind ggplot is using geoms, which are plotting objects (called by functions) to make a particular type of plot. In a geom function, there is a similar layout where plotting objects can be called using aes() where users specify x and y values. Similar to base R plotting, the data argument can be utilized to avoid specifying each value with $. Learning ggplot can be particularly tricky because of the range of ways to format the code and produce the same plot. I have very particular reasons for my preferred method but I won’t divulge them all here.\nAnother great feature of ggplot is the built in theme functions. These can allow you to quickly clean up the plot an make the features all look the same! In this guide, I’ll rely on theme_bw(). However, in most my work, I use theme_pubr() from the ggpubr package, with customization in theme(). Explore around with the themes for your own purposes!\n\nggplot(data = iris)+\n  geom_point(aes(x = Petal.Length, y=Petal.Width))+\n  labs(x = 'Petal Length [mm]', y = 'Petal Width [mm]') +\n  theme_bw()\n\n\n\n# same as:\n# ggplot(data = iris, aes(x = Petal.Length, y = Petal.Width))"
  },
  {
    "objectID": "gen_R-guide-plotting.html#adding-to-the-scatterplot",
    "href": "gen_R-guide-plotting.html#adding-to-the-scatterplot",
    "title": "A guide to making basic plots in R",
    "section": "Adding to the scatterplot",
    "text": "Adding to the scatterplot\n\nTrendlines:\nScatterplots can be greatly improved by adding features. For example, we might want to add a trendline to the figure. These can be useful for demonstrating a linear replationship between the variables.\nIn baseR, we need to use the abline() function. This function requires you to specify the linear relationship. Fortunately, we can just feed it a model object. Here, I use lm() inside the abline function. However, if you defined your linear model elsewhere, you can put that object in place.\n\nplot(Petal.Width ~ Petal.Length, data = iris,\n     xlab = 'Petal Length [mm]',\n     ylab = 'Petal Width [mm]')\nabline(lm(Petal.Width ~ Petal.Length, data = iris))\n\n\n\n\nPersonally I find ggplot a bit more flexible for making trend lines. We can use geom_smooth() or stat_smooth(). By default, this function will create a smoothed trend line (the exact smoothing default varies based on the data):\n\nggplot(data = iris)+\n  geom_point(aes(x = Petal.Length, y=Petal.Width))+\n  geom_smooth(aes(x = Petal.Length, y=Petal.Width))+\n  labs(x = 'Petal Length [mm]', y = 'Petal Width [mm]') +\n  theme_bw()\n\n\n\n\nHowever, we can change the smoothing method to fit whatever trendline we want. Here, we can use ‘lm’. I also usually like to get rid of the error line with se = F:\n\nggplot(data = iris)+\n  geom_point(aes(x = Petal.Length, y=Petal.Width))+\n  geom_smooth(aes(x = Petal.Length, y=Petal.Width),\n              method = 'lm', se = F)+\n  labs(x = 'Petal Length [mm]', y = 'Petal Width [mm]') +\n  theme_bw()\n\n\n\n\n\n\nColoring by group:\nColor can be a great tool to add to scatterplots to give context to the data or display a third dimension.\nWe can first look at grouping by species:\nIn base r we can use the col argument. I’m also going to add a legend but it is a bit trick to do this in base R:\n\nplot(Petal.Width ~ Petal.Length, \n     col = Species,\n     data = iris,\n     xlab = 'Petal Length [mm]',\n     ylab = 'Petal Width [mm]')\nlegend('bottomright', legend = unique(iris$Species),\n       col = c('black','red', 'green'), pch = c(20))\nabline(lm(Petal.Width ~ Petal.Length, data = iris))\n\n\n\n\nThe above plot is OK, but we’re starting to hit the wall of base R graphics. In ggplot, we can make the same figure but we can add some better features. For example, group-specific trend lines!\n\nggplot(data = iris,\n       aes(x = Petal.Length, y=Petal.Width,\n                 color = Species))+\n  geom_point()+\n  geom_smooth(method = 'lm', se = F)+\n  labs(x = 'Petal Length [mm]', y = 'Petal Width [mm]') +\n  theme_bw()\n\n\n\n\n\n\nColor by a continuous factor:\nWe could also color by a continuous variable. In the iris dataset, this is not the most useful approach. However, I’ll demonstrate here with coloring by Sepal.Width. I’ll also change the color scale to make it more visible using the scale_color_diverge() function.\n\nggplot(data = iris,\n       aes(x = Petal.Length, y=Petal.Width))+\n  geom_point(aes(color = Sepal.Width))+\n  geom_smooth(method = 'lm', se = F)+\n  scale_color_gradient(low = 'grey', high = 'black')+\n  labs(x = 'Petal Length [mm]', y = 'Petal Width [mm]') +\n  theme_bw()\n\n\n\n\n\n\nMore factors!\nWe could also use size to communicate a fourth dimension. Again, the iris dataset may not really require this feature. However, I’ll use the Sepal.Length as a bubbling element for demonstration purposes. If you are paying attention, you’ll notice I’m moving around where I assign the aesthetic mappings (aes()). I also added a transparency value to the points (alpha).\n\nggplot(data = iris,\n       aes(x = Petal.Length, y=Petal.Width))+\n  geom_point(alpha = 0.7, aes(size = Sepal.Length,\n                              color = Sepal.Width))+\n  geom_smooth(method = 'lm', se = F)+\n  scale_color_gradient(low = 'grey', high = 'black')+\n  labs(x = 'Petal Length [mm]', y = 'Petal Width [mm]') +\n  theme_bw()\n\n\n\n\n\n\nCorrelogram\nCorrelograms are great ways to assess multiple relationships at once. There’s some great packages to make nice figures for theses. However, base R offers a quick way to assess multiple relationships at once. Here we can use the first four columns of the iris dataset to see how each individual variable is related to one another.\n\nplot(iris[,1:4])\n\n\n\n\nWe can calculate the correlation matrix for each of those values using cor()\n\ncor(iris[,1:4])\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\nSepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\nPetal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\nPetal.Width     0.8179411  -0.3661259    0.9628654   1.0000000"
  },
  {
    "objectID": "gen_R-guide-plotting.html#summarizing-the-data",
    "href": "gen_R-guide-plotting.html#summarizing-the-data",
    "title": "A guide to making basic plots in R",
    "section": "Summarizing the data",
    "text": "Summarizing the data\nAs discussed in the general expectations, a good plot should offer a summary of the data. So here, we can average the chick weights to show a better summary of the data.\n\nSummary by all chicks\n\nall_chicks <- ChickWeight |>\n  group_by(Time = Time) |> \n  summarize(mean_weight = mean(weight),\n            se_weight = sd(weight)/sqrt(nrow(ChickWeight)))\n\nNow we can use that data to make a plot of the average chick growth across time, regardless of diet:\nIn Base R:\nfor the base r case, I’m going to use both points with lines that way we can show the standard error\n\nplot(mean_weight ~ Time, all_chicks,\n     type = 'b',\n     xlab = 'Days Since Hatching',\n     ylab = 'Mean Weight [g]')\narrows(x0 = all_chicks$Time, \n       x = all_chicks$Time, \n       y0 = all_chicks$mean_weight - all_chicks$se_weight,\n       y = all_chicks$mean_weight + all_chicks$se_weight,\n       angle = 90, length = 0.075, code = 3)\n\n\n\n\nIn ggplot:\nThere’s two nice ways to make this figure in ggplot. We can use the point-and-line method as shown above or we can use ribbons for standard error.\n\nggplot(all_chicks,\n       aes(x = Time, y = mean_weight))+\n  geom_point()+\n  geom_errorbar(aes(ymin = mean_weight - se_weight,\n                    ymax = mean_weight + se_weight))+\n  geom_line()+\n  labs(x = \"Days Since Hatching\",\n       y = 'Mean Weight [g]')+\n  theme_bw()\n\n\n\n\n\nggplot(all_chicks,\n       aes(x = Time, y = mean_weight))+\n  geom_line(size = 1)+\n  geom_ribbon(aes(ymin = mean_weight - se_weight,\n                  ymax = mean_weight + se_weight),\n              size = 1,\n              fill = 'grey', alpha = 0.7)+\n  labs(x = \"Days Since Hatching\",\n       y = 'Mean Weight [g]')+\n  theme_bw()\n\n\n\n\n\n\nSummary by diet\nThe nature of this dataset really implies we should be investigating how growth varied based on diet. So let’s look at the data grouped by diet. I’m only going to do this in ggplot at this point and jump to the final figure, but we build on the figures above!\n\nchicks_by_diet <- ChickWeight |>\n  group_by(Time = Time, diet = Diet) |> \n  summarize(mean_weight = mean(weight),\n            se_weight = sd(weight)/sqrt(nrow(ChickWeight)))\n\nFor this ggplot, I’ll just use the ribbon approach. Pay attention to the details here. Note that lines require color arguments while ribbons require fill.\n\nggplot(chicks_by_diet,\n       aes(x = Time, y = mean_weight))+\n  geom_line(size = 1, aes(color = diet))+\n  geom_ribbon(aes(ymin = mean_weight - se_weight,\n                  ymax = mean_weight + se_weight,\n                  fill = diet),\n              size = 1, alpha = 0.25)+\n  labs(x = \"Days Since Hatching\",\n       y = 'Mean Weight [g]',\n       fill = 'Diet', color = \"Diet\")+\n  theme_bw()"
  },
  {
    "objectID": "gen_R-guide-plotting.html#a-complex-example",
    "href": "gen_R-guide-plotting.html#a-complex-example",
    "title": "A guide to making basic plots in R",
    "section": "A complex example:",
    "text": "A complex example:\nLet’s take this all one step further. While it may be nice to show summary data, it can be fun to have each individual line shown as well. This is effectively the “raw” approach but we can clean it up a bit by playing around with transparency values:\n\nggplot()+\n  geom_line(data = chicks_by_diet,\n            aes(x = Time, y = mean_weight,\n                color = diet),\n            size = 1) +\n  geom_line(data = ChickWeight,\n            aes(x = Time, y = weight,\n                color = Diet, group = Chick),\n            alpha = 0.15)+\n  labs(x = \"Days Since Hatching\",\n       y = 'Mean Weight [g]',\n       fill = 'Diet', color = \"Diet\")+\n  theme_bw()\n\n\n\n\nI don’t think this plot is as effective as the summary ones, however, it is fun to make. This type of figure might be useful to display simulation results."
  },
  {
    "objectID": "gen_R-guide-stats.html",
    "href": "gen_R-guide-stats.html",
    "title": "A starter guide to analyzing data in R",
    "section": "",
    "text": "There is a wide, wide, wide, range of ways to analyze data. Here, I hope to provide a starting tool kit for you to explore your data with. However, keep in mind you should always take extended precaution when using an analysis to ensure that you are checking your assumptiosn"
  },
  {
    "objectID": "gen_R-guide-stats.html#anova",
    "href": "gen_R-guide-stats.html#anova",
    "title": "A starter guide to analyzing data in R",
    "section": "ANOVA",
    "text": "ANOVA"
  },
  {
    "objectID": "gen_R-guide-stats.html#simple-linear-regression",
    "href": "gen_R-guide-stats.html#simple-linear-regression",
    "title": "A starter guide to analyzing data in R",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression"
  },
  {
    "objectID": "gen_R-guide-stats.html#multiple-linear-regression",
    "href": "gen_R-guide-stats.html#multiple-linear-regression",
    "title": "A starter guide to analyzing data in R",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression"
  },
  {
    "objectID": "gen_R-guide-stats.html#logistic-regression",
    "href": "gen_R-guide-stats.html#logistic-regression",
    "title": "A starter guide to analyzing data in R",
    "section": "Logistic Regression",
    "text": "Logistic Regression"
  },
  {
    "objectID": "gen_R-guide-stats.html#nmds",
    "href": "gen_R-guide-stats.html#nmds",
    "title": "A starter guide to analyzing data in R",
    "section": "NMDS",
    "text": "NMDS"
  },
  {
    "objectID": "gen_R-guide-stats.html#clustering",
    "href": "gen_R-guide-stats.html#clustering",
    "title": "A starter guide to analyzing data in R",
    "section": "Clustering",
    "text": "Clustering"
  },
  {
    "objectID": "gen_R-guide-stats.html#pca",
    "href": "gen_R-guide-stats.html#pca",
    "title": "A starter guide to analyzing data in R",
    "section": "PCA",
    "text": "PCA"
  },
  {
    "objectID": "gen_stats-guide.html",
    "href": "gen_stats-guide.html",
    "title": "570L Fall 2023",
    "section": "",
    "text": "As ecologists, we often are interested in testing hypotheses about some ecological relationship, prediction, or theory. Generally, our hypotheses may relate to some wild population, large ecosystem, or a broad-scale relationship (e.g. a predicted relationship from a theory; like allometric scaling or survival under different conditions, etc). However, it is typically impossible to survey the entire group of interest. In statistics, we define the entire group of interest as our “population” or the “population-level”. Note that this may be slightly different than the ecological definition of population as a group of same-species organisms in an ecosystem. For the purposes of this article, I’ll use “population” in the statistical sense.\nLet’s say we were interested in whether or not campus squirrels were fatter than squirrels from Harbison forest. To test this prediction, we’d need to collect some campus squirrels and some forest squirrels, get a body-fat composition analysis and compare them. Well, in this example, our “population-level” interest is all the squirrels in the forest and all the squirrels on campus. What specifically we are interested in is the difference between mean campus body fat and mean forest body fat. In statistical notation, this would be written as: \\(\\mu_{c} - \\mu_{f}\\). Where c and f denote campus and forest. The term \\(\\mu\\) is common for population-level mean. Any metric of interest at the population-level, we can call a parameter. However, it would be impossible to capture every single squirrel. So we can’t actually measure our parameter of interest. Instead we have to take a sample from the population. Let’s say we collect 5 squirrels each from the forest and campus. Well now we could calculate the mean of our sample. The metric calculated at the sample-level, is referred to as a statistic. In our case that would be the sample mean of campus and forest squirrels (in statistics notation this is either \\(\\bar{x}_c - \\bar{x}_f\\) or \\(\\hat{\\mu}_c - \\hat{\\mu}_f\\)). Well, how can we know if our statistics are actually representative of the true population parameter of interest? That is where inferential statistical tests enter the picture. At the most broad level, statistical analyses give us the ability to discuss how confident we are that our samples are representative of reality (the population-level parameters).\n\nIn this document, I’ll briefly summarize some basics of data types, how to plot them, and a basic tool kit for statistically testing your data. This is by no means an exhaustive list, but hopefully a good starting point. Additionally, in this article I’ll try to provide a little bit more detail for those who are interested. Don’t feel the need to deeply understand everything. However, please read through the whole article. A common issue in applying statistics to ecological data, is that ecologists fall into traps of how to correctly understand and interpret the results of a test. To avoid these traps, we need to understand the basics of common approaches, then why they might be misleading when interpreting them.\nLike most things, learning how and when to apply a given test, comes with experience. So I hope you are able to refer back while working on independent projects.\nLet’s recap some basic terms:\nPopulation - in a statistical sense, this is the entire group of interest (could be multiple, or less, biological populations)\nParameter - a metric which describes a feature at the population level\nSample - a sub-group of the population\nStatistic - a metric describing the sample-level, which can be used to make inferences about the population."
  },
  {
    "objectID": "gen_stats-guide.html#caveats-with-common-approaches",
    "href": "gen_stats-guide.html#caveats-with-common-approaches",
    "title": "570L Fall 2023",
    "section": "Caveats with common approaches:",
    "text": "Caveats with common approaches:\nMany statistical tests have a set of assumptions. You should evaluate those assumptions prior to applying a test. Plenty of papers get published without evaluation of the assumptions yet it is best to know thel limitations of different data analysis methods."
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "Fall 2023 570L - Ecology Lab",
    "section": "About this course:",
    "text": "About this course:\nThis is an independent, one-credit lab course for upper division students interested in ecology. The primary goal of this course is for students to develop independent ecological investigations which they will complete over the course of the semester. However, there will also be several assigned labs at the beginning of the course to introduce students to basic ecology field techniques."
  },
  {
    "objectID": "lab_0_intro_assignment.html",
    "href": "lab_0_intro_assignment.html",
    "title": "Introductory Assignment",
    "section": "",
    "text": "Part 1: Background about you (5 points):\n\n1.a What made you interested in this course?\n\n\n1.b What skills do you hope to gain through this lab?\n\n\n1.c What are your career goals?\n\n\n\nPart 2: Intro to R (5 points):\nTo get some basic familiarity with using R, I want you to make a graph.\nGraph fiddler crab carapace width by two sites. The graph should be appropriate for the type of data and display the mean crab carapace width by site. Standard error bars should be available as well.\nYou can download fiddler crab data which I formatted here. The data originally comes from the LTER’s educational resources page. Just click the link, right-click (or command/control S) and save the data as a .csv file. Note you should be able to open the .csv file using excel.\n\nBONUS (5 points):\nRemake the plot but this time include all sites, not just the two. hint: this can be achieved by removing one line of dplyr code."
  },
  {
    "objectID": "lab_1_behavioral-ecology.html",
    "href": "lab_1_behavioral-ecology.html",
    "title": "Behavioral & Urban Ecology",
    "section": "",
    "text": "Fun Squirrel Information:\n\nhttps://sc.edu/uofsc/posts/2021/10/remembering_the_days_natural_history_stroll_ep_35.php\nhttps://www.instagram.com/usc.squirrels/?hl=en\nhttps://www.facebook.com/watch/?v=499139211340770\nhttps://www.tiktok.com/@uofsc/video/7191269652230720814 (not available on campus wifi due to tiktok restrictions despite this is university’s account)\n\nIn this lab, we will study movement and behavior of squirrels on the USC campus. The primary objective will be to introduce students to conducting basic ecological fieldwork in a convenient location. However, don’t take this lab for granted! There are plenty of opportunities to develop an independent project utilizing nearby spaces on campus. Try to be curious of your surroundings!\nStudying organism behavior may seem fairly straightforward. However, it can be extremely challenging to do so with robust scientific methods. A basic tool in behavioral ecology research is to use an ethogram. This is a table which clearly defines possible actions to observe in an individual’s behavior.\nIn this lab we will construct ethograms to test a hypothesis about squirrel activity.\nDownload the full project plan here\nDownload the worksheet here"
  },
  {
    "objectID": "lab_2_forest-biodiversity.html",
    "href": "lab_2_forest-biodiversity.html",
    "title": "Forest Biodiversity",
    "section": "",
    "text": "Download the worksheet here\n\nLab Context\nFor this lab, we will be evaluating how biodiversity varies based on different disturbance regimes. A classic concept in ecology is the intermediate disturbance hypothesis. This concept was formulated by Connell in 1978. The hypothesis is an extension of the concept of ecological succession. Succession theory suggests that communities, in a new environment, will develop following a predictable pattern in which the first organisms are colonizing ones (fast-life histories, r-selected individuals). These taxa are quick to reproduce and establish a population. Then, as resources develop in the environment, longer living yet better competing individuals will move into the area. Thus the community will slowly become dominated by organisms which have slow-life history strategies (K-selected). This concept is a persistent idea in ecology and was developed in coastal dunes [1], freshwater streams [2], and carrion studies [3]. Yet is has been extended into many other areas of ecology, including plankton (see Margalef’s Mandala).\nSo, how does this lead to the intermediate disturbance hypothesis? Well, what “resets” an environment is disturbances (wildfires, storms, waves, etc.). So the intermediate disturbance hypothesis posits that if disturbances are too frequent, then only early successional species will be present. However, if undisturbed, the community will move towards a late-stage successional community equilibrium. If there are “intermediate” levels of disturbance. Then we might expect higher levels of biodiversity as it sustains a mixture of community types.\n\nIn this lab we will test the intermediate disturbance hypothesis by sampling thre regions in Harbison forest which correspond to different levels of disturbance. We’ll have a low-disturbance area which has been essentially undisturbed. Then we’ll have a high disturbance region which is consistently mowed. Finally, we’ll have a mid-disturbance area which was subjected to a burn 1 year ago.\n\n\nMeasuring biodiversity\nThere are several metrics to evaluate biodiversity. In this lab we will calculate a few of the most common ones. For your worksheets you will only be required to select one of them.\nA straightforward metric is Species Richness (\\(R\\)). This is the number of unique species in a region. \\[\nR = \\# of\\_unique\\_taxa\n\\] However, this is not the most informative metric as it does not account for the number of taxa there are of each species. For example, if there are 100 different species, yet 99% of them were one type, the area wouldn’t be very biodiverse. Species richness alone is not very informative in such cases.\nAlternatively species diversity can be measured through a variety of indices. Arguable the most common is the Shannon-Wiener Index (\\(H\\)). This ranges from 0 (a low diversity) to larger numbers. The larger the \\(H\\), the more diverse the community is.\n\\[\nH = -\\sum_{i = 1}^{R}{p_i lnp_i}\n\\] Here, for each unique taxa, we add up the proportion (\\(p\\)) of each (\\(i^{th}\\)) taxa, multiplied by the natural log of that taxa. We do that for all unique taxa (\\(R\\)) For count data, we can extend the formula:\n\\[\nH = -\\sum_{i = 1}^{R}{\\frac{n_i}{N}ln\\frac{n_i}{N}}\n\\]\nHere, \\(n_i\\) is the count of the specific taxa while \\(N\\) is the total count of taxa.\nWhile \\(H\\) provides a nice measure of biodiversiy, it is still related to the overall richness. An alternative metric is to measure evenness. This will report the how spread across the different taxa the relative abundace is. For example, if we had 5 individuals each of two taxa, it would be more “even” than 1 of taxa A and 9 of taxa B. This metric is independent of the overall richness and give a perspective to the relative success of the taxa which are present. The more common metric for evenness is Pileou’s (\\(D\\)):\n\\[\nD = 1 - \\sum_{i=1}^{R}{\\frac{n_i(n_i-1)}{N(N - 1)}}\n\\]\n\n\nQuantitative Sampling\nA large challenge for ecologists who conduct field studies is how to collect quantitative, unbiased estimates of population abundance, community structure, or other ecologically relevant metrics. Here we want to collect biodiversity metrics in four forest regions.\nThere are several common field sampling techniques:\nSome basic tools:\n\nQuadrat (basically a square, typically of PVC)\n\nThese are useful for defining a set sampling area. They give us the ability to quantify density (count per area) which then standardizes our metrics across regions\n\nTransect\n\nA fixed line which we go and sample along. Sampling along a transect can be done with point counts (whatever is touching the transect), line-quadrat methods (placing quadrats systematically or randomly along the transect), or band-transect methods (counting everything within a fixed width of the transect).\n\n\nWhen deploying these tools, there are several ways we can try to reduce bias. We want to have a fixed way to sample. Some common methods are:\n\nHaphazard sampling\n\nThis is just sampling when we encounter our study target. This is inherently biased although it is sometimes necessary when we are interested in highly mobile or elusive organisms. It also could be that we are interested in particular taxa so we just observe those. This is what we did for our Behavioral Ecology Lab last week\n\nSystematic Sampling\n\nHere, samples are collected at a fixed interval (say every 2m along a transect or in set regions of a grid). This allows us to sub-sample a region. This can be particularly useful for measuring change along a gradient.\n\nRandom Sampling\n\nHere, random coordinates are selected in a study area (grid or transect). This ensures a non-bias sample of our study region. However, we should be cautious if there is confounding variables in the area. This is what we will do this week for the intermediate disturbance hypothesis lab.\n\nStratified-Random Sampling\n\nIf we are sampling an area with known variation (differences in moisture, elevation, etc), we can create strata. These are set sub-regions to sample. Here we can then generate random coordinates within each strata for a stratified, but still random approach.\n\n\nWhen we construct our sampling design some key considerations are: How can we be unbias our samples? What are we measuring? If it is just the overall area, random sampling may be sufficient. However if we believe there is variation within our study area we might want to use a transect with systematic sampling, or a stratified random design. These are all things you’ll want to consider when designing your sampling schemes.\n\n\nHarbison Forest Directions\nGo to the main entrance to Harbison Forest off broad river road. Note that some people turn early and go to the South Carolina Forestry Commission. Drive forward (past the first parking lot on the left) and keep left towards the gate (marked in blue). Drive straight down the gravel road. Shortly after the intersection for the Eagle Trail lot (you’ll see signs about a gazeebo, just go straight), you’ll see a parking lot on the left (marked by yellow X).\n\nIf you have problems getting there, you can call my cell 440-668-8376.\nDress for being outside! It will be warm but there are a lot of bugs and sharp plants. I always opt for covering up rather than being cool but it’s a personal trade-off. Also wear appropriate shoes!"
  },
  {
    "objectID": "lab_3_community-assemblages.html",
    "href": "lab_3_community-assemblages.html",
    "title": "Community Assemblages",
    "section": "",
    "text": "Download worksheet here\n\nLab Context\nLast week we got the opportunity to look at forest communities in different stages of successional communities. However, we just looked at the different plants present in the area. Yet, the community structure of one portion of the environment can influence the entire ecosystem. A fairly long standing concept in ecology is the idea of foundational species. These are organisms whose presence defines the overall structure of an ecosystem. In forests, trees often can play a foundational role in an ecosystem. When thinking of trees, many people probably think of the animals that are associated with them. Birds, squirrels, insects all live in the upper portion of trees. However, trees also can influence the ecosystem by altering soil properties. Trees have root networks which support sub-surface communities of fungal networks, invertebrates, and bacteria. Additionally, their roots can create physical structure to the soil, altering the hydrological characteristics. Trees also can alter the soil chemistry through their leaves. As their leaves fall, they can change soil chemistry as they decompose.\nWe refer to these fallen leaves as the “leaf litter”. In the litter, there can be a diverse community of arthropods which thrive in the leaf litter and upper layer of the top soil. This week, we’ll be investigating which factor is the major factor in determining the soil invertebrate community: trees or abiotic factors.\nWe’ll be sampling up a hill, with a strong moisture gradient. We’ll start in a wetland and move to the top of a dry hill. Along this gradient, we’ll expect to see a difference difference in the soil characteristics as well as the primary trees which occupy this area.\n\n\nSampling Plan\nIn last weeks lab background I provided a brief overview different field sampling techniques. Last week we did a random-grid approach. That was the appropriate approach because we were interested in the average biodiversity in a given plot of land. However, this week, we’re interested in how things change along a gradient.\nSo we’ll want to use a transect-based approach. Because this week we’re looking at trees, which are relatively sparse, we’ll use a band transect approach. This allows us to characterize every tree and it’s exact position along the hill. We’ll identify and measure each within 1-m of either side of the transect (2-m bands). The starting position of the transects will be randomly assigned. These data will allow us to characterize changes in abundance of major trees, as well as the relative importance of each tree in regions of the hill.\nThen, to characterize the soil invertebrate community, we’ll want to collect samples along the hill. However, this is a little more tricky. Notably, we are interested in testing the hypothesis that trees alter the invertebrate community, more so than abiotic factors. However, the dominant tree species will likely change as we go up the hill. So we need account for the fact that species are confounded with the abiotic environment. To do this, we’ll take multiple invertebrate samples in distinct regions along the transect. The exact position of the samples will be collected haphazardly within distinct 10-m sections of the transect. One sample will be collected next to representative species of trees in that region of the transect. The sample will be collected right at the base. Then to account for soil-moisture variation, we’ll collect a sample which is located far-away from any tree. Thus, within each region we can compare litter community and account for variation due to the soil invertebrate community.\n\n\nMeeting location:\nWe’ll meet at the Gordon Belser Arboretum: see details here.\nThe address for the gate is 4080 Bloomwood Road. We it takes about 15 minutes to get there from campus. So let’s plan on getting there and starting lab at 3:15pm."
  },
  {
    "objectID": "lab_4_urban-stream.html",
    "href": "lab_4_urban-stream.html",
    "title": "Urban Stream Ecology",
    "section": "",
    "text": "Access the full WS4 here\nIn this lab we will compare two stream locations with differing levels of urbanization to assess the overall ecosystem health. Often times when assessing ecosystem health, ecologists, conservationists, and ecosystem managers will utilize “indicator” species. These are taxa who might respond more quickly to environmental perturbations. Generally, there are some species which will respond extremely quickly to environmental perturbations. These tend to be specialist species which cannot survive a range of conditions. Urban streams are subjected to increased variability in water flow, increased nutrient and contaminant inputs, and decreases in oxygen. These all will likely reduce the number of sensitive and moderately sensitive species. Alternatively there are pollution tolerant species who can survive these suboptimal conditions. At times these tolerant species can even thrive.\nAs we learned last week, assessing differences in community composition can be a decent challenge to quantify. By relying on indicator species, we can easily compare abundance of different groups of organisms to compare the overall ecological health of an ecosystem.\n\nLab This Week:\nFor lab this week we will travel to two stream locations with anticipated differences in urban alterations. We will assess the stream’s overall health using a generalized stream health assessment survey. Then we will quantify the overall abundance of common stream macroinvertebrates. We can then pool these to compare the overall abundance of sensitive, moderately sensitive, and pollution tolerant taxa. By comparing these taxa we can assess overall stream health between the two locations.\n\n\nQuantification & Sampling Strategy\nThis week we will employ a systematic, time-based survey of macro-invertebrates. In previous labs, we employed systematic, haphazard, and random sampling based on spatially defined units. However, in our stream ecosystems the overall area to work with is relatively small. Yet we want to ensure that we sample a consistent way that way we are able to compare the two stream locations without bases of sampling effort, area, etc. As such, we will sample the streams for a fixed interval of time to ensure that we survey the areas with equal effort.\nWill will deploy six, 5-minute all out surveys of non-overlapping 2-m stretch of stream. During our all-out survey efforts the stream will be surveyed using seine nets to sample smaller macro invertebrates, dip nets for collection of medium-sized organisms, and visual observers to count flying insects, large benthic organisms, etc.\n\n\nLogistics:\nFor time sake, we will have one section survey one stream location. Both locations are approximately 20-minutes away from campus, however, they are about 20-minutes from each other. The pristine area will be selected by Monday The urbanized stream will be a branch of Gills Creek. This is the same watershed, however by the time the water reaches this point, it has passed through several artificial lakes in residential areas. Also the stretch of creek itself is altered in a heavily trafficked, commercial area.\nWe will meet at 25-minutes after the start of class so please plan to arrive by 3:15 at the meeting location.\nHere is our general schedule:\n3:15-3:30: Arrival, set-up, discuss\n3:30-3:45: Initial Stream Health Assessment Form\n3:45-4:30: Surveys, with buffer time\n4:30-4:45: Clean-up discuss\n4:45: Depart\n5:15: You get where you need to be.\n\nAddresses:\nTuesday: 723 Crowson Rd, Columbia, SC 29205\nThursday: TBD\nStudents are encouraged to wear shoes which they can get wet, long sleeves, bug-spray, and pants/shorts with belt loops. I will provide a selection of waders but we’re limited in the number and sizes."
  },
  {
    "objectID": "lab_5_phenology.html",
    "href": "lab_5_phenology.html",
    "title": "570L Fall 2023",
    "section": "",
    "text": "This Week\nPhenology is the study of cyclical dynamics, typically over annual cycles in natural phenomena. Seasonal variation is a fundamental component of many natural populations and can structure ecological interactions, population growth, and many other phenomena. Read the project plan for a full background. To well characterize phenological phenomena, you need to have long-term datasets. Then you can evaluate how such patterns shift over time.\nThere majority of the lab will be dicussed on the code page."
  },
  {
    "objectID": "proj_main.html",
    "href": "proj_main.html",
    "title": "Independent Projects",
    "section": "",
    "text": "The primary objective of this lab course will be for students to pursue a self-led investigation of an ecological hypothesis. Students must propose their own projects which they will pursue over the course of the semester."
  },
  {
    "objectID": "proj_main.html#presentations",
    "href": "proj_main.html#presentations",
    "title": "Independent Projects",
    "section": "Presentations",
    "text": "Presentations\nStudents will develop a presentation to communicate their findings to their peers. These presentations will follow a typical format of 8-10 minutes for presentations with 2 minutes for question. Developing clear presentations are a critical skill for any ecologist, whether presenting results at a scientific conference or to a general audience. Keep in mind who will be in the audience when presenting your work!\n\nGeneral Presentation expectations\nAccess the rubric here\nTake a good look through the rubric to know what to expect when formatting your presentation. The grading generally corresponds to how much time you should allocate for each section. In class we will discuss exact elements of how to clearly format your presentation and deliver it well.\nSome specific considerations for this class:\n\nPresentations will be 8 - 10 minutes long\nYou will present your core results. This may not be all your results (if you did a lot)\nFigures should be large, with clear axes titles\n\nWhenever you show a new figure, introduce the audience to the axis, including scale and units\n\nMost people are not familiar at all with your study. Give lots of background! Often times your actual results may be a fairly small part of the overall presentation when presenting to a broad audience.\nDiscuss statistics, but primarily in the context of the figures and how much weigh should be given to any visual trends\nDiscuss future ways to further the research or possible limitation on the data.\n\nDO NOT JUST STATE “Well there’s human error so I may have counted wrong”. If discussing limitations, specifically how it could influence the data. For example maybe the study was done in October but the flowering status of a plant is a key feature for identification so you may have overestimated a certain plant, etc."
  },
  {
    "objectID": "proj_main.html#final-papers",
    "href": "proj_main.html#final-papers",
    "title": "Independent Projects",
    "section": "Final Papers",
    "text": "Final Papers\nStudents will write a mini-manuscript to communicate their findings. These should be formatted as a small version of work which would be submitted to an ecological peer-reviewed journal. Writing is a critical skill in aiding your thinking and processing of information. Being able to communicate scientific results clearly in writing is important!\n\nGeneral Presentation Guidelines\nAccess the rubric here\nGenerally, the grading follows the generalized rubric we’ve used over the course of the class. However, some specifics which might be useful. Think of these as “minimum” requirements. A good paper should exceed these. For example, you should have tried to address your hypotheses which multiple analyses. You may not find it necessary to share every bit of exploratory data but you should include anything that is useful to advance the story you are trying to tell with your data.\n\n5+ pages\n\n\n\nFormatted as introduction, methods, results, discussion, references. Include an abstract as well!\n\nAbstracts are a 200 word statement which summarizes everything\n\n3+ references\n\nPeer-reviewed literature is utilized to place your study in context both in the introduction and the discussion. And methods if citing a particular method.\n\n2 figures and/or tables. There should be at least one figure however, more will make the paper much, much better.\nAt least one statistical analysis (it should be appropriate).\n\nMany of you will have found null results - no effect even if you expected one. That is OK! just explain in the discussion why you think you observed those results.\n\nWritten in a general scientific format\n\nLike what is in the papers you read!"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Course Website:\nAll course documents will be shared via the course website: https://USC-Ecology-Lab.github.io\n\n\nCourse Description:\nThis is a 1 credit lab based course for ecological investigation. Students will be trained in basic field techniques and analyses common in ecology. A core portion of this course will be an independent investigation on an ecological topic selected by students.\n\n\nLearning Outcomes:\n\nUnderstand fundamental ecology sampling schemes\nUtilize appropriate statistical tools for analyzing ecological data\nAbility to design quality investigations to test hypotheses\nGain experience with communicating scientific results to peers\n\n\n\nCourse Technology:\nLab activities will included data analysis in the R programming language. While this course will rely heavily on R, prior knowledge is not expected. Students should have access to both R and Rstudio. See here for download information. Data, documents, and code for this course will all be hosted on github. Git and GitHub will not be taught yet it is a great tool for any scientist to be familiar with. Interested people should look more here for a friendly introduction.\nFor independent projects, students are welcome to use whichever software they are most productive in. However R is encouraged.\n\n\nOutdoor activity & safety policy:\nEcology research often requires outdoor data collections in variable conditions and environments. This course is no exception. There will be multiple field trips to nearby areas. Students are expected to consult the pre-lab content and arrive prepared for weather and lab conditions. It is expected that students, to the best of their ability, will engage in data collection during labs. However, please contact be if you feel uncomfortable or unsafe with any lab activity.\n\n\nTransportation:\nFor labs which are not initially meeting at the lab space (Week 2,3,4). You can either meet at the lab space, or at the specified location. Meeting time will vary based on location. Transportation will be available to any students who don’t have access. Carpooling is encouraged.\nAll field trips will end with enough time for students to return to campus by the specified end time of lab.\n\n\nGrades:\nThis is a one credit lab based course. The majority of student grades will be centered on their independent project and lab-based worksheets. There is a total of 1000 points available. Point breakdowns are as follows:\n\nIntroduction Worksheet (10 points) : Prior to the start of lab, students must complete the introduction worksheet.\nLab-based worksheets (50 points each, 200 total): In the first weeks of lab students will complete field labs to test assigned hypotheses. These assignments will be worth 50 points each. Note that there is a tentative 5th worksheet. If we are able to complete the 5th worksheet, then the lowest of the 5 worksheet grades will be dropped. If a lab is cancelled due to weather or other unforeseen circumstance, we will adjust the value of all worksheets to scale to a total of 200 points.\nProject Plan (100 points): Groups/Individual students will propose their independent project following the assigned project plan worksheet. This assignment may be subject to a round of revisions. Early submissions are encouraged.\nData Collection Progress Report (15 points each, 45 total): During the data collection period, students must attend the beginning of lab regardless of their planned collection activities. Students will share with the class their project status and challenges. Active discussion between groups is expected.\nData Analysis Progress Report (15 points each, 45 total): During the data analysis period, students must attend the beginning of lab regardless of their planned activities. Students will share with the class their project status and challenges. Active discussion between groups is expected.\nFinal Presentation (250 Points): Final independent projects will be shared in a standard 15 minute research presentation format. Group members may allocate presentation responsibilities independently, however all group members are expected to present an equal portion. See rubric for more details.\nFinal Paper (300 Points): Final independent projects will be written in a full paper format. See rubric for grading details.\nAuthor contribution statement (50 Points): Evaluation of group members contributions to the final project. Group members will independently report the activity of all group members and assign points. Severe discrepancies will may warrant point deductions on individual project grades.\n\n\n\nLate work & make-up policy:\nAny assignments turned in late will be subject to a 10% grade penalty starting immediately following the due date. For each additional 24-hour period late, a 10% penalty will be added. Some assignments are ineligible for late submission, including the progress reports, final presentation, and author contribution statement.\n\n\nAttendance Policy:\nAttendance is expected for all lab sections. Any expected lab absences should be discussed prior to the start of that lab period.\n\n\nAcademic Integrity:\nAny cases of plagiarism will result in a minimum failure of assignment and may result in further penalties, including automatic failure of the course.\n\n\nGenerative AI & ChatGPT:\nGenerative AI is an extremely powerful tool for anyone who wishes to be a productive scientist. It can be particularly helpful for organizing and writing code and providing the blueprint for text. However, in its current state, ChatGPT and other generative AI applications do not produce work at the level expected of an upper-division undergraduate. If you want to receive a good grade in the course you should not rely exclusively on ChatGPT. If you are not outworking the machine, you are falling behind. Be skeptical of its responses as it cannot be trusted.\n\n\nCo-enrollment with 570 Lecture:\nThis lab course is a fully independent credit. Co-enrollment in the lecture section of 570 is not required nor expected. While content will overlap and it will be beneficial to take both courses, the content will not be synchronous. Note that the major assignment in the lecture section is the Research Proposal. You are welcome to use your independent project as the basis for that assignment. However note that these are separate assignments with different grading structure.\n\n\nClass Schedule:\n\n\n\n\n\n\n\n\n\n\nWeek (Dates)\nLocation\nLab\nAssignments Given\nAssignments Due\n\n\n\n\n0 (8/29 & 8/31)\nVirtual\nIntroduction & R set-up\nIntro Worksheet\n\n\n\n1 (9/5 & 9/7)\nLab room and near-by campus areas\nIntroduction & Mammal Behavior\nWS1\nProject Plan\nIntro Worksheet\n\n\n2 (9/12 & 9/14)\nHarbison State Forest\nIntermediate Disturbance & Succession\nWS2\nWS1\n\n\n3 (9/19 & 9/21)\nUSC Arboretum\nCommunity Assemblages\nWS3\nWS2\n\n\n4 (9/26 & 9/28)\n\nAlex Gets Concussed\n\n\n\n\n5 (10/3 & 10/5)\nLab Room\nAnalyzing Community Data & Review of Projects\n\n\n\n\n6 (10/10 & 10/12)\nVarious Stream Locations\nUrban Aquatic Ecology\nPhenology Lab (Virtually available)\nWS4\nWS5*\nWS3\n\n\n7 (10/17 & 10/19)\nFall Break Modified Class!\nLab Room (TUESDAY ONLY)\nTuesday (Phenology lab)\nProject Planning\n\nWS4 (Friday)\nProject Plan (Friday)\n\n\n8 (10/24 & 10/26)\nLab Room & TBD\nProject Planning & In-Class sharing\n\nWS5*\nData Collection Progress Report\n\n\n9 (10/31 & 11/2)\nLab Room & TBD\nData Collection\n\nData Collection Progress Report\n\n\n10 (11/7 & 11/9)\nLab Room & TBD\nData Collection\n\nData Collection Progress Report\n\n\n11 (11/14 & 11/16)\nLab Room\nData Analysis\n\nData Analysis Progress Report\n\n\n12 (11/21 & 11/23)\nThanksgiving Break\nIndividual Meetings\nData Analysis\n\nData Analysis Progress Report\n\n\n13 (11/28 & 11/31)\nLab Room\nData Analysis\n\nData Analysis Progress Report\n\n\n14 (12/5 & 12/7)\nTBD\nFinal Presentations\n\nFinal Presentation\n\n\n15 (12/12)\n\nFinal Paper Submission\nNote I encourage you to submit before, this is just latest possible since grades are due 12/15\n\nFinal Project Paper\nAuthor Contribution Statement"
  }
]