[
  {
    "objectID": "addtl_resources.html",
    "href": "addtl_resources.html",
    "title": "Additional Resources",
    "section": "",
    "text": "Advanced Concepts\n\nGit and version control are great ways to organize computational projects. This is increasingly a useful skill to understand:\n\nHappy Git with R\n\nIf you want to learn more about organizing files in a meaningful way, I found this workshop to be extremely helpful:\n\nR for Reproducible Research\n\nThis website was made with quarto, a new markdown format and interactive programming environment. Quarto offers a great way to make pretty documents and presentations with incorporate code:\n\nQuarto"
  },
  {
    "objectID": "code_lab-00.html",
    "href": "code_lab-00.html",
    "title": "Introductory Lab Guide",
    "section": "",
    "text": "When starting a new analysis in R, it is best to create a new R script. You can save this script in your course folder. At the start of each script, it is good form to load any needed packages. In this exercise we’ll need ggplot. If you haven’t already installed ggplot2, enter into the console install.packages(\"ggplot2\").\nAt the top of your script, load the package:\n\nlibrary(ggplot2)"
  },
  {
    "objectID": "code_lab-00.html#reading-the-data-into-r.",
    "href": "code_lab-00.html#reading-the-data-into-r.",
    "title": "Introductory Lab Guide",
    "section": "Reading the data into R.",
    "text": "Reading the data into R.\nIf you downloaded the data to your folder, you can read it in with read.csv(). First make sure your working directory is focused on your file.\n\nsetwd('~/BIOL570L_Docs') # set this to your path in your computer's folder\ncrab_width <- read.csv('LTER_CrabCarapaces.csv')"
  },
  {
    "objectID": "code_lab-00.html#base-r-option",
    "href": "code_lab-00.html#base-r-option",
    "title": "Introductory Lab Guide",
    "section": "Base R Option:",
    "text": "Base R Option:\nFirst, I’ll create individual variables for each mean value and sd value by site:\n\nmean_BC <- mean(crab_width$carapace_width[which(crab_width$Site == 'BC')]) # take mean of BC\nsd_BC <- sd(crab_width$carapace_width[which(crab_width$Site == 'BC')]) # take sd of BC\n\n# same for GTM\nmean_GTM <- mean(crab_width$carapace_width[which(crab_width$Site == 'GTM')])\nsd_GTM <- sd(crab_width$carapace_width[which(crab_width$Site == 'GTM')])\n\nWe can look at those values individually:\n\nprint(mean_GTM)\n\n[1] 12.40321\n\n\nThen we’ll need to pull all those values into a data frame\n\ncrab_means <- data.frame(Site = c('BC','GTM'),\n                        mean = c(mean_BC, mean_GTM),\n                        sd = c(sd_BC, sd_GTM))\n\nWe now can take a look at those values\n\ncrab_means\n\n  Site     mean       sd\n1   BC 16.19730 4.814464\n2  GTM 12.40321 1.804449"
  },
  {
    "objectID": "code_lab-00.html#dplyr-option",
    "href": "code_lab-00.html#dplyr-option",
    "title": "Introductory Lab Guide",
    "section": "dplyr option:",
    "text": "dplyr option:\nThis is the cleaner approach but it does abstract a few steps away. First you want to make sure you have installed dplyr. Also you should load the library (do this at the top of the script).\n\nlibrary(dplyr)\n\nThe tidyverse syntax is really big on piping code together. So we’ll use a number of functions here and chain them all together. Piping takes the value on the left of the pipe operator (|>) and pushes it to the first argument of the function on the right. In tidyverse/dplyr functions, the first argument is often a data.frame. This makes it easy to chain together these functions. In this code, I will chain it all together, but if you want to learn more, you can run one pipe at a time and see what happens in each step.\n\ncrab_means <- crab_width |> \n  filter(Site %in% c('BC', 'GTM')) |> #filter only to these sites\n  group_by(Site) |> #group by site\n  summarize(mean = mean(carapace_width),\n            sd = sd(carapace_width)) # apply mean and sd functions\n\nNow we can look at the data. Note that tidyverse functions will create tibbles rather than data.frames. For most purposes this is a very minor detail that will not matter until you are working on high-level problems or developing software in R.\n\ncrab_means\n\n# A tibble: 2 × 3\n  Site   mean    sd\n  <chr> <dbl> <dbl>\n1 BC     16.2  4.81\n2 GTM    12.4  1.80"
  },
  {
    "objectID": "code_lab-01.html",
    "href": "code_lab-01.html",
    "title": "Lab 1 Code",
    "section": "",
    "text": "The first major challenge (and likely only one) will be organizing your data so that it works well with the provided code. Take the data and format it in an excel sheet. For this analysis, you should total the time spent on each action and put that as rows. You should also count the number of times each action occurred.\nHere’s how my data look. Note that I made this prior to lab so I am using imaginary data:\n\nIf you want to use the code provided, you must exactly match the layout that I used here. While your data might be different R is very picky about a few things. Here’s some potential issues students might run into:\n\nMake sure column names match exactly. R is case sensitive\nGenerally spaces are challenges in character vectors so use a _ instead.\nWhile our data collection sheet on paper had merged cells for behavior, in R, they must be individually represented in each row. So make sure you don’t format the excel sheet in a fancy way\nMake sure to save the files as a .csv file. This is not the excel default. You must select ‘save as’.\nSave the file as a name you can find and remember!"
  },
  {
    "objectID": "code_lab-01.html#check-out-the-data",
    "href": "code_lab-01.html#check-out-the-data",
    "title": "Lab 1 Code",
    "section": "Check out the data",
    "text": "Check out the data\nIf you succesfully loaded the data you should take a look at it to make sure the layout is how you want.\n\nhead(sqdf)\n\n   category           action num_events total_mins\n1  conflict          chasing         19         37\n2  conflict running_squirrel         24         35\n3  conflict         taunting         10          3\n4 avoidance    running_other          4          2\n5 avoidance running_predator          1          1\n6    forage        searching         13         45\n\n\nNote that here I used head(). But in reality, I typically use the View() function to take a peek at data.\nSome possible issues at this step would be that your column headers are wrong or your num_events column or total_mins column are characters (they should be ‘int’). If this is the case, there is something wrong with how you formatted your excel sheet."
  },
  {
    "objectID": "code_lab-01.html#figures",
    "href": "code_lab-01.html#figures",
    "title": "Lab 1 Code",
    "section": "Figures",
    "text": "Figures\nThe original approach can be improved on in a few ways. First, let’s think about that figure. While it was easy to compile the data by behavior category, we smoothed over the details we recorded for individual actions. Below is code to make a similar figure but with a little more detail.\nThis one is a bit trickier since I have to specify the colors I want to use. I’m still making bars by behavior category but now I’m filling the colors with stacked actions. Here, I specify some colors for each action:\n\ncolors = c(\n  `chasing` = '#D81B60',\n  `running_squirrel` = '#C75780',\n  `taunting` = \"#BF7993\",\n  `running_other` = \"#1E88E5\",\n  `running_predator` = \"#66A8E2\",\n  `searching` = \"#FFC107\",\n  `collecting_food` = \"#FDD458\",\n  `storing_food` = \"#FFE597\",\n  `jumping` = \"#FFEFBF\",\n  `eating` = '#004D40',\n  `resting` = '#4D7F77'\n)\n\nNow I can make the same figure but with this enhanced detail. Note that for making your figures, your actions and behavior categories will be different than this example dataset. You’ll need to modify the above color scale for different categories. In R, you can make a different color either with names ('black'). Or you could use HEX codes like I did. Just google any color pallete mixer. As a note, it’s often beneficial to consider colorblind friendly palettes to make your figures accessible to everyone!\n\nggplot(sqdf) + \n  geom_bar(aes(x = category, y = total_mins,\n               fill = action),\n           stat = 'identity', position = 'stack') +\n  scale_fill_manual(values = colors) +\n  labs(x = 'Behavior Category', y = 'Total Minutes', fill = 'Action')+\n  theme_classic()\n\n\n\n\n\n\nTime spent during different behavior categories of Eastern Grey Squirrels observed on USC’s Horshoe. Actions corresponding to each behavior category are shown.\n\n\n\n\nWhile this figure isn’t perfect, I’d probably want to change the colors a bit more. It provides a good idea of how to improve this figure. Another idea, is that while time spent on a category is a great metric, we might also be interested in the number of events that each item happened.\n\nggplot(sqdf) + \n  geom_bar(aes(x = category, y = num_events,\n               fill = action),\n           stat = 'identity', position = 'stack') +\n  scale_fill_manual(values = colors)+\n  labs(x = 'Behavior Category', y = 'Number of Events', fill = 'Action')+\n  theme_classic()\n\n\n\n\n\n\nNumber of events for different actions of squirrels observed on USC’s Horseshoe. Actions are shown grouped by major behavior category.\n\n\n\n\nThis figure provides a different perspective than the first one. We can see that avoidance events occur more often than previously suggested while rest events are less common.\nWe could also look at the average duration of each event:\n\nggplot(sqdf) +\n  geom_bar(aes(x = total_mins/num_events, y = action, fill = category),\n           stat = 'identity')+\n  labs(x = \"Average Duration of Action [mins]\", y = \"Action\", fill = \"Behavior Category\")+\n  theme_classic()\n\n\n\n\n\n\nAverage action direction of Eastern Grey squirrels observed on USC’s Horseshoe campus.\n\n\n\n\nAs you can see, even with a fairly rudimentary dataset, we can display the information in a number of ways. Making a variety of plots can be extremely useful. First, as a researcher this is a critical step of exploratory data analysis (EDA). EDA allows us to notice major trends in our data, sometimes surprising ourselves. Additionally, we can try to think about what is the best way to communicate our findings. We want our figures to be clear to a reader who has no familiarity with our research project. The captions should be brief yet informative. When making your figures, ask your self: “What is the main message I want a reader to take away from this figure?” and then you can think about how effectively you communicate that message through the figure."
  },
  {
    "objectID": "code_lab-01.html#other-data-analyses",
    "href": "code_lab-01.html#other-data-analyses",
    "title": "Lab 1 Code",
    "section": "Other data analyses",
    "text": "Other data analyses\nIn our initial analysis, we simply tested if the distribution of time allocated on one behavior category was statistically significantly different from a uniform time allocation. However, that isn’t the best test of our original hypothesis. The project plan hypothesis suggested that conflict would receive more time allocation than other categories. So let’s run the analysis but with a different “expected” distribution for our squirrel’s time allotment. In this case I’ll define a vector of an expected_model which has the proportions of time spent in my different behavior categories.\nHere, I’m going to make an expected model to match my hypothsis, where a squirrel spends 5% of its time in avoidance, 40% in confilct, 30% foraging, and 25% resting.\nTo make these proportions, I’m matching the order to the order of my observed categories:\n\nsq_summary$category\n\n[1] \"avoidance\" \"conflict\"  \"forage\"    \"rest\"     \n\n\nYou’ll have to create your own expected model to match your unique categories!\n\n# let's say you are interested in comparing \n# for unique categories\n\n# total_observation_time <- sum(sqdf$total_mins) #total up all time observed\nexpected_proportions <- c(0.05,0.4,0.3,0.25)\n\nchisq.test(x = sq_summary$total_time, p = expected_proportions)\n\n\n    Chi-squared test for given probabilities\n\ndata:  sq_summary$total_time\nX-squared = 4.2917, df = 3, p-value = 0.2316\n\n\nFrom this result, we have a p-value > 0.05. This would lead me to write a statement similar to this as a result:\n“There was no significant difference between the time allocation of our observed squirrels and the expected allocations under a conflict-heavy time allocation (chi-squared test, p-value = 0.23).”\nClearly this is fabricated data but it gives a great idea of how we could extend this analysis."
  },
  {
    "objectID": "code_lab-02.html",
    "href": "code_lab-02.html",
    "title": "Lab 1 Code",
    "section": "",
    "text": "Accessing the data\nYou can download the data from the course github org: link\nDownload and save the data in your files/folder\nThen load it into R. I’m going to name it raw_data\n\nsetwd('C:/Users/abart/OneDrive/Documents/UofSC/Classes/BIOL570L')\nraw_data <- read.csv('2023_WS2_Data-Share.csv') # whatever you named i\n\n\n\n\nNow let’s take a look and make sure the structure is as we expect:\n\nstr(raw_data)\n\n'data.frame':   105 obs. of  4 variables:\n $ Region : chr  \"High-Disturbance\" \"High-Disturbance\" \"High-Disturbance\" \"High-Disturbance\" ...\n $ Quadrat: int  1 1 1 1 2 2 2 3 3 3 ...\n $ Taxa   : chr  \"A\" \"B\" \"C\" \"D\" ...\n $ Count  : int  1 2 7 2 2 9 4 2 3 7 ...\n\n\nLook’s good to me!\n\n\nAnalyzing the data\nThis week the question is somewhat straightforward but the code will get quite complex. This guide walks through some of the process of building out the code. If you just want to get to plots and core statistical analyses, you don’t need all this code. I will clearly distinguish between the required and the “bonus” portions. However, I encourage you to skim through all sections as it will inform the process.\nThe main object is to test the intermediate disturbance hypothesis. For this, we have three forest regions: High-Disturbance, Mid-Disturbance, Low-Disturbance. Thus, our predictive variable will be “Region” (which is categorical with three levels). Our response variable should test if the three regions are significantly different from one another in some metric of biodiversity. What is expected for this week’s worksheet, is to select ONE metric to test the hypothesis. Our three possible response variables are (1) Species richness (integer/count data), (2) Shannon’s H (continuous), and (3) Pileou’s D (continuous).\nSo in all cases, we’ll have a categorical predictor variable with a continuous response. Because our categorical variable has three levels, we’ll need to use an ANOVA test, or its non-parametric equivalent. In all cases, we will be comparing are the regions on average significantly different from one another. So we’ll need to calculate the metric for each quadrat, then average by region. Thankfully, much of the dplyr syntax (code) we’ve seen is very useful for these operations.\nRegardless of metric, we’ll make a simple barplot and run a statistical analysis to assess if the observed differences in the barplot are significantly different.\n\n\nA. Species Richness\nFirst, let’s run the analysis for species richness. This is the most straightforward from a coding perspective. We just need to count the number of unique species in each quadrat.\nThis could feasibly be done by hand but it is easier in R. Also, it would not be feasible if we had more regions and we’re less likely to make a mistake!\nThe first step will be to make a data.frame with just counts the number of unique species in each quadrat. We can do that with dplyr I’ll name it richness_df.\nInside this complex chunk of code the key operation comes in the summarize function, where I create the new column richness. Note that here, I’m saying “take the raw data, group it by region and quadrat, then for each group, calculate richness in a new column”. We can calculate richness by finding the unique taxa and then counting them (length).\n\nrichness_df <- raw_data |> \n  group_by(Region, Quadrat) |> #group these\n  summarise(richness = length(unique(Taxa))) # make a column for the # of unique taxa\n\nYou can compare the new richness data.frame to your raw data. Take a look with these functions:\n\nView(richness_df)\nView(raw_data)\n\nThis is a good opportunity to spot-check some of the math. Does the high-density, quadrat 1 have the correct number?\n\nA.1 Plotting Species Richness\nTo make our plot, we’ll need to make a summary data frame which has the mean richness per region with standard deviation. I’ll call it richness_plot_df\nWe can use dplyr:\n\nrichness_plot_df <- richness_df |> \n  group_by(Region) |> \n  summarize(mean_richness = mean(richness),\n            sd_richness = sd(richness))\n\nNow we can make our plot:\n\nggplot(richness_plot_df) +\n  geom_bar(aes(x = Region, y = mean_richness),\n           stat = 'identity', fill = 'black')+\n  geom_errorbar(aes(x = Region, ymin = mean_richness,\n                    ymax = mean_richness + sd_richness),\n                width = 0.5, color = 'black')+\n  labs(x = \"Region\",y = \"Mean Richness\")+\n  theme_bw()\n\n\np1 = ggplot(richness_plot_df) +\n  geom_bar(aes(x = Region, y = mean_richness),\n           stat = 'identity', fill = 'black')+\n  geom_errorbar(aes(x = Region, ymin = mean_richness,\n                    ymax = mean_richness + sd_richness),\n                width = 0.5, color = 'black')+\n  labs(x = \"Region\",y = \"Mean Richness\")+\n  theme_bw()\nwatermark_plot(p1)\n\n\n\n\nGreat! It looks pretty good!\n\n\nA.1b Plotting Bonus\nOne issue with the plot above is I don’t like the order of the categories. When we look back to the lab reading, Connell’s figure has the categories as a gradient from high-to-mid-to-low.\nSo let’s reorganize the plot to match. Here we need to reassign factor levels in the data frame then it will work in ggplot:\n\n# What if I want my figures in a different order?\nrichness_plot_df$Region <- factor(richness_plot_df$Region, levels = c(\"High-Disturbance\",\n                                                                      \"Mid-Disturbance\",\n                                                                      \"Low-Disturbance\"))\n\nNow we actually can just run the same plot code:\n\nggplot(richness_plot_df) +\n  geom_bar(aes(x = Region, y = mean_richness),\n           stat = 'identity', fill = 'black')+\n  geom_errorbar(aes(x = Region, ymin = mean_richness,\n                    ymax = mean_richness + sd_richness),\n                width = 0.5, color = 'black')+\n  labs(x = \"Region\",y = \"Mean Richness\")+\n  theme_bw()\n\n\np1 = ggplot(richness_plot_df) +\n  geom_bar(aes(x = Region, y = mean_richness),\n           stat = 'identity', fill = 'black')+\n  geom_errorbar(aes(x = Region, ymin = mean_richness,\n                    ymax = mean_richness + sd_richness),\n                width = 0.5, color = 'black')+\n  labs(x = \"Region\",y = \"Mean Richness\")+\n  theme_bw()\nwatermark_plot(p1)\n\n\n\n\n\n\nA.2 Richness Analysis\nIn the project plan we suggested running an ANOVA to compare between the categories. However, ANOVAs assume that our (response) data are normally distributed.\nTechnical note: ANOVAs are just extensions of linear models. There are several assumptions but normality of residuals is the main one. This is not normality of the data, but are the data within a group normally distributed around the mean. However, typically if the data are non-normal then the residuals will be non-normal at small sample sizes. At larger sample sizes this is less of a concern due to the Central Limit Theorem.\nSo first we need to see if the species richness are normally distributed within each group. We can do this a number of ways but the easiest is to see if the density distribution looks normal. Note these are not plots we’d share with anyone, or put in your worksheet but it is a quick way for us to see:\nHere, I’m using base R graphics and some advanced approaches just to keep it short. Don’t worry about all these details. Here, we’re primary concerned about the question of normality:\n\npar(mfrow = c(2,2)) # set up plotting window in 2x2 grid\nfor(group in unique(richness_df$Region)) {\n  density(richness_df$richness[richness_df$Region == group]) |> \n  plot(main = group)\n}\n\n\n\n\nLooking at the figures, it seems that non-normality might be a concern. So we’ll need to use a non-parametric test. The alternative to an ANOVA in this case is a Kruskal-Wallace test.\nThe kruskal-wallace test is similar to our chi-squared test in that it asks: “Are there any differences between the distributions of the groups”.\nLet’s run this test in R:\n\nkruskal.test(richness_df$richness ~ richness_df$Region) # Tell's us it is significantly different\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  richness_df$richness by richness_df$Region\nKruskal-Wallis chi-squared = 6.9755, df = 2, p-value = 0.03057\n\n\nHere, the p-value tells us if the groups are signficantly different, however it doesn’t tell us which groups are different.\nNow we can use the dunn.test as a post-hoc test. This compares values between each individual groups:\n\ndunn.test(x = richness_df$richness, g = richness_df$Region)\n\n  Kruskal-Wallis rank sum test\n\ndata: x and group\nKruskal-Wallis chi-squared = 6.9755, df = 2, p-value = 0.03\n\n                           Comparison of x by group                            \n                                (No adjustment)                                \nCol Mean-|\nRow Mean |   High-Dis   Low-Dist\n---------+----------------------\nLow-Dist |  -2.309266\n         |    0.0105*\n         |\nMid-Dist |  -2.278936   0.231662\n         |    0.0113*     0.4084\n\nalpha = 0.05\nReject Ho if p <= alpha/2\n\n\nNote that in this output, this gives us the kruskall test up above but it also gives us a p-value for each group-specific comparison of means. This is where we can write our results from!\n\n\n\nB. Shannon’s H Diversity Metric\nCalculating the Shannon-Wiener index is a little more complicated from an R perspective. Look at the formula:\n\\[\nH = -\\sum_{i = 1}^{R}{\\frac{n_i}{N}ln\\frac{n_i}{N}}\n\\]\nWe need to caculate for each quadrat, the total number of species (\\(N\\)) and the proportion of each \\(i^{th}\\) species in that quadrat, the natural log of that proportion and then sum it up for all species in that quadrat!\nAgain you could brute-force this calculation or do it in Excel. For this lab it might be feasible, but what if you had years of data! This is where R is useful.\n\nB.0 Bonus\nLet’s first calculate H for just one region, one quadrat by “hand”. I’ll create a data frame of just one quadrat\n\n# just for one case:\nquadrat_1 <-  raw_data |> \n  filter(Region == \"Low-Disturbance\", Quadrat == '1')\n\nquadrat_1\n\n           Region Quadrat Taxa Count\n1 Low-Disturbance       1    A     1\n2 Low-Disturbance       1    B     1\n3 Low-Disturbance       1    C     3\n\n\nNow we can calculate all those values. Note here, I’m using p as the term for \\(\\frac{n_i}{N_i}\\)\n\np <- quadrat_1$Count / sum(quadrat_1$Count) # create counts\nlnp <- log(p)\n-sum(p * lnp)\n\n[1] 0.9502705\n\n\n\n\nB.0 Calculating H for all the data\nHere, we will create our own function for calculating H. Honestly, this is more advanced than your typical intro-to-R but it makes the analysis easier on the whole. So let’s roll with it.\n\n# We can write a function to do this repeatedly!\ndiversity_calculator <- function(count) {\n  p <- count / sum(count) # create counts\n  lnp <- log(p)\n  H <-  -sum(p * lnp)\n  return(H)\n}\n\nIf you did the bonus above, you can check the function works but just running on the quadrat 1 data!\nNow we can use same code were familiar with to calculate H, just with our own function inside summarize. I’ll create a new data.frame called diversity_df\n\ndiversity_df <- raw_data |> \n  group_by(Region, Quadrat) |> \n  summarise(H = diversity_calculator(Count))\n\n\n\nB.1 Plotting Diversity\nLet’s make our summary data frame to plot with. I’ll call it div_plot\n\ndiv_plot <- diversity_df |> \n  group_by(Region) |> \n  summarize(mean_H = mean(H),\n            sd_H = sd(H))\n\nNow let’s plot it. Note that I’m included the code in here to reorganize the order of the categories from the A.1b section:\n\n# What if I want my figures in a different order?\ndiv_plot$Region <- factor(div_plot$Region, levels = c(\"High-Disturbance\",\n                                                      \"Mid-Disturbance\",\n                                                      \"Low-Disturbance\"))\n\nggplot(div_plot) +\n  geom_bar(aes(x = Region, y = mean_H),\n           stat = 'identity', fill = 'black')+\n  geom_errorbar(aes(x = Region, ymin = mean_H,\n                    ymax = mean_H + sd_H),\n                width = 0.5, color = 'black')+\n  labs(x = \"Region\",y = \"Shannon's H\")+\n  theme_bw()\n\n\n\n\n\n\n\n\nB.2 Analyzing H\nLet’s take a look at if normality might be a concern for this dataset:\n\npar(mfrow = c(2,2)) # set up plotting window in 2x2 grid\nfor(group in unique(diversity_df$Region)) {\n  density(diversity_df$H[richness_df$Region == group]) |> \n  plot(main = group)\n}\n\n\n\n\nAgain, non-normality might be a considerable concern. So let’s use the kruskal-wallace test.\n\nkruskal.test(diversity_df$H~diversity_df$Region)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  diversity_df$H by diversity_df$Region\nKruskal-Wallis chi-squared = 4.3288, df = 2, p-value = 0.1148\n\n\nHere, this test suggests that there is no significant difference between the regions. So we can write that as our result. (This might change with added data).\n\n\n\nC. Evenness\nFinally let’s calculate pileou’s evenness. Again, this might create some new code challenges but it will make it easier in the long-run.\nRemember the formula:\n\\[ D = 1 - \\sum_{i=1}^{R}{\\frac{n_i(n_i-1)}{N(N - 1)}} \\]\n\nC.0 Bonus\nHere, we can calculate the value for just one quadrat. Here, I’m using the quadrat_1 data frame I created in B.0 bonus. If you skipped that go back and make sure you have quadrat_1 in your environment.\n\nn = quadrat_1$Count\nN = sum(quadrat_1$Count)\n# Solve for D\n1 - sum((n*(n-1))/(N*(N-1)))\n\n[1] 0.7\n\n\n\n\nC.0 Calculating Pileou’s evenness\nHere’s the self-defined function which will calculate pileou’s evenness:\n\npileou_calculator <- function(count) {\n  n = count\n  N = sum(count)\n  D = 1 - sum((n*(n-1))/(N*(N-1)))\n  return(D)\n}\n\nNow we can calculate it for all quadrats. I’ll call it evenness_df\n\n# calculate for all!\nevenness_df <- raw_data |> \n  group_by(Region, Quadrat) |> \n  summarize(D = pileou_calculator(Count))\n\n\n\nC.1 Plotting Evenness\nWe’ll need to make a summary dataframe first. I’ll call it evenness_plot\n\nevenness_plot <- evenness_df |> \n  group_by(Region) |> \n  summarize(mean_D = mean(D),\n            sd_D = sd(D))\n\nNow we can plot that dataframe.\n\n# What if I want my figures in a different order?\nevenness_plot$Region <- factor(evenness_plot$Region, levels = c(\"High-Disturbance\",\n                                                      \"Mid-Disturbance\",\n                                                      \"Low-Disturbance\"))\n\nggplot(evenness_plot) +\n  geom_bar(aes(x = Region, y = mean_D),\n           stat = 'identity', fill = 'black')+\n  geom_errorbar(aes(x = Region, ymin = mean_D,\n                    ymax = mean_D + sd_D),\n                width = 0.5, color = 'black')+\n  labs(x = \"Region\",y = \"Pileou's D\")+\n  theme_bw()\n\n\n\n\n\n\n\n\nC.2 Analyzing D\nFirst, let’s check the normality of our groups:\n\npar(mfrow = c(2,2)) # set up plotting window in 2x2 grid\nfor(group in unique(evenness_df$Region)) {\n  density(evenness_df$D[evenness_df$Region == group]) |> \n  plot(main = group)\n}\n\n\n\n\nAgain, non-normality might be a concern so let’s use the kruskall.wallace test\n\nkruskal.test(evenness_df$D~evenness_df$Region)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  evenness_df$D by evenness_df$Region\nKruskal-Wallis chi-squared = 6.9544, df = 2, p-value = 0.03089\n\n\nIt suggests that there is some significant difference in average evennnes between the groups. Let’s look at that by group specific comparisons with the dunn.test\n\ndunn.test(x = evenness_df$D, g = evenness_df$Region)\n\n  Kruskal-Wallis rank sum test\n\ndata: x and group\nKruskal-Wallis chi-squared = 6.9544, df = 2, p-value = 0.03\n\n                           Comparison of x by group                            \n                                (No adjustment)                                \nCol Mean-|\nRow Mean |   High-Dis   Low-Dist\n---------+----------------------\nLow-Dist |  -2.544257\n         |    0.0055*\n         |\nMid-Dist |  -0.698480   1.999694\n         |     0.2424    0.0228*\n\nalpha = 0.05\nReject Ho if p <= alpha/2"
  },
  {
    "objectID": "code_main.html",
    "href": "code_main.html",
    "title": "Programming in Ecology",
    "section": "",
    "text": "Broadly speaking, ecology is a field focused studying organisms and how they interact with others and their surroundings. This often involves identifying, describing, and hypothesizing about ecological patterns. In the pursuit of studying such patterns, ecologists in the modern world utilize a vast array of tools to collect and analyze data. Ecological data spans a wide range of formats and sampling distributions. As ecologists, we must be able to properly interrogate our data so that we can identify meaningful trends. That is where the application of statistics comes into the picture. Simply collecting data and making general conclusions cannot inform thorough conclusions. Analyzing data through the proper application of statistical tests will help us as scientists.\nWith the advancement of technology and statistical computing, ecological data analysis has progressed beyond the point of simple calculations. The large amounts of data acquired in ecology require us to be proficient programmers and statisticians. Luckily, there are several, free tools which are growing in popularity for people to use, making data analysis extreme accessible. Arguably the most common tool used in ecology, academia, and data science is R. R is an open-source programming environment and language. R was developed primarily by statisticians which makes it extreme versatile. Additionally, because so many ecologists use it, there are many add-ons (called packages) which are taylor-made for ecological applications.\nIn this course, I will provide examples and support for processing lab data in R. The goal is that someone who has never used R will be able to still be successful. This means at times I may over-explain somethings. Alternatively, it is difficult to remember what it was like to first learn something, so if I am not explaining anything well, please come ask me! Finally, when it comes to completing your assignments, I don’t care what software you use to get it done. For example, if you have to make a bar graph, you might know how to complete something quicker in excel than in R. So, go ahead and use what is most comfortable. However, time invested in learning R will only better equip you to expand your skills.\n\n\nGoogle, chatGPT, and Stack Overflow are all extremely useful resources for solving any issues you may encounter while working with R. Often times if you get an error, simply copy and paste it in a search and there’s likely someone else who experienced a similar issue. This is a fundamental piece of the process. Working with R (or any programming language) is a rarely a smooth process. Don’t be dissuaded by needing to search and solve issues. It’s all just part of the game."
  },
  {
    "objectID": "code_main.html#downloading-r-studio-r",
    "href": "code_main.html#downloading-r-studio-r",
    "title": "Programming in Ecology",
    "section": "Downloading R studio & R",
    "text": "Downloading R studio & R\nGo to the posit website at https://posit.co/download/rstudio-desktop/. This website has instruction for downloading both R and RStudio.\nWhen downloading R, select the right one for your computer at the top panel. If on the initial installation it asks you to select a mirror, it doesn’t matter. Just select whatever.\nOnce you have both R and Rstudio downloaded, you can just open Rstudio."
  },
  {
    "objectID": "code_main.html#rstudio-layout",
    "href": "code_main.html#rstudio-layout",
    "title": "Programming in Ecology",
    "section": "RStudio Layout",
    "text": "RStudio Layout\nWhen you first open RStudio, there will be three main tabs, the largest of which is the console. On the right are two windows with multiple tabs. The preselected tab on the bottom is a file browser and the one of top shows what is active in your R environment. At the start this is empty but as you create variables or functions they are visible in this window.\nMy setup is a bit different but you can see a good orientation at this website."
  },
  {
    "objectID": "code_main.html#understanding-r-code",
    "href": "code_main.html#understanding-r-code",
    "title": "Programming in Ecology",
    "section": "Understanding R code",
    "text": "Understanding R code\nThere are several great resources for learning R in detail which I’ll link at the bottom. Very briefly, I want people to understand some very basic items:\nComments are great for understanding code. You should include it for your future self to refer back to. I will include comments in all our class code.\n\n# comments are written on lines starting with hastags. These will appear different.\n# Comments can also be written after code\n2+2 == 4 # running this line will return TRUE\n\nWe can assign values to objects using an “assignment operator”. Traditionally, in R this is an arrow <-. You can also use an equal sign =. I prefer the arrow for a number of reasons but it is also intuitive. You take all the values on the left and put them into the storage variable on the left.\n\nx <- 5 # take 5 and put it into x\ny = 5 # take 5 and put it into y\n\nprint(x == y) # will print TRUE\n\nz <- x * y # take the product of x and y and put it into z\nprint(z) # print z\n\nFinally, functions are operations which can be preformed on values/objects. Functions are executed by feeding arguments into a call. This generally looks like function(arg1, arg2, ...). This is very similar to excel where you would write =function(arg1, arg2) in a cell.\nNote in the examples above, print is a function. You can learn more about any function through the documentation. Simply write into the r console ?function. where the function is listed.\n\n?print\n\nFunctions can also be wrapped around one another and are preformed inside-out. Standard order of operations. For example:\n\n# c() is a function short for concatenate or combine. it chains together values\nc(5,5,3,4)\n\n# the mean function takes the mean of a range of values.\n# so you could do this two ways:\n\n# option 1:\nx <- c(5,5,3,4)\nmean(x)\n\n# option 2:\nmean(c(5,5,3,4))\n\nA unique aspect of R, which has recently been added to the base functionality\nThis is an extremely simplistic overview. I encourage everyone to look at some of the additional resources for help with learning R."
  },
  {
    "objectID": "code_main.html#organizing-your-computer-files-importing-to-r",
    "href": "code_main.html#organizing-your-computer-files-importing-to-r",
    "title": "Programming in Ecology",
    "section": "Organizing your computer files & importing to R",
    "text": "Organizing your computer files & importing to R\nMany people do not maintain a clear organization of files in their computer’s storage. This problem is compounded by .\nA common problem that many students using the university’s office licence run into is that their word/excel documents are saved into their University cloud storage (onedrive). Then it can be tricky to find those files through a programming approach. If you have a good file organization system, please keep to it and you should be fine. However, if you haven’t put much thought into organizing your files, I encourage you to create a new folder for this course. You can store all your files in that folder. Then when trying to work with R, you can load your files from that path.\nSo let’s talk about file paths. In windows, you can use the file explorer to look up files. Then you can access the path by clicking on the top bar. Typically, it will look something like this: ‘C:\\Users\\yourname\\Documents\\EcologyLab’. In Mac, it will look a little different. Notably, the slashes will be front-slash instead of back-slash. If you try to read a file from R, mac users can just copy-paste their file paths. However, windows users need to write it out with a front-slash. Alternatively, you can use double-backslashes but this is a bad habit to develop.\nIf you want to be an advanced R user, you should learn about organizing your files in a package format and taking advantage of RStudio’s project feature. You should also use relative file paths and set-up projects using git. However, this is beyond the scope of the course and likely most applications for ecology projects.\nRead more about file organization in the additional resources page.\n\nReading in files\nThe most common US-based data format is a comma separated value file (*.csv). However, most people are more familiar with working in excel which has its own file format (.xlsx or .xls). There are several R packages to help you read in excel files. But for the sake of keeping things simple this course, we will stick to .csv files.\nYou can save any excel file as a .csv. Just make sure you keep your headers simple.\nTo read in a file, you can call read.csv(). This is where file paths become important. In R, you have a working directory. This is where R is currently looking for files. You can look at your working directory with the function getwd(). Also RStudio displays the working directory at the top of the console window. Note that the tilde (~) often will refer to your Documents folder. At least for Windows machines.\n\nYou can change your working directory with setwd(). You simply put the file path to the folder where you want to access data from.\nIn short, loading files might look like this:\n\nmy_path <- '~/BIOL570L Docs' #replace this with your path file\nsetwd(my_path) #reassign working directory. This is only necessary if not already there\nmy_data <- read.csv('mydata.csv') #load data\n\n#altarntively\nmy_data <- read.csv('~/BIOL570L Docs/mydata.csv')"
  },
  {
    "objectID": "code_main.html#packages",
    "href": "code_main.html#packages",
    "title": "Programming in Ecology",
    "section": "Packages",
    "text": "Packages\nA great benefit of R is that there are user-made packages which contain functions and data for particular purposes. Most functions I’ve used in this example are ‘base’ R, meaning they exist by default. Other functions can be loaded by packages. To keep R simple, most packages are not loaded by default. To load a package, you must first install it. Luckily, R does a great job maintaining all packages in R, through the Comprehensive R Archive Network (CRAN). So you can download CRAN packages directly though R.\nTo install a package, you can use the install.packages() function.\n\n# we will use ggplot2 in our first assingment so you'll need to download it.\ninstall.packages(\"ggplot2\")\n\nThen each time you need to utilize a package in an R script, you will need to load the package. Typically this is done at the top of a script. You can load a package with the library() function.\n\nlibrary(ggplot2)\n\nNote that when installing we use quotes and when loading we don’t. It’s tricky that way. You should only need to install a package once unless you need to update it. Often times beginners in R get stuck on trying to use functions they haven’t loaded. So make sure you called library() if you get an error about not finding a function."
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "Fall 2023 570L - Ecology Lab",
    "section": "About this course:",
    "text": "About this course:\nThis is an independent, one-credit lab course for upper division students interested in ecology. The primary goal of this course is for students to develop independent ecological investigations which they will complete over the course of the semester. However, there will also be several assigned labs at the beginning of the course to introduce students to basic ecology field techniques."
  },
  {
    "objectID": "lab_0_intro_assignment.html",
    "href": "lab_0_intro_assignment.html",
    "title": "Introductory Assignment",
    "section": "",
    "text": "Part 1: Background about you (5 points):\n\n1.a What made you interested in this course?\n\n\n1.b What skills do you hope to gain through this lab?\n\n\n1.c What are your career goals?\n\n\n\nPart 2: Intro to R (5 points):\nTo get some basic familiarity with using R, I want you to make a graph.\nGraph fiddler crab carapace width by two sites. The graph should be appropriate for the type of data and display the mean crab carapace width by site. Standard error bars should be available as well.\nYou can download fiddler crab data which I formatted here. The data originally comes from the LTER’s educational resources page. Just click the link, right-click (or command/control S) and save the data as a .csv file. Note you should be able to open the .csv file using excel.\n\nBONUS (5 points):\nRemake the plot but this time include all sites, not just the two. hint: this can be achieved by removing one line of dplyr code."
  },
  {
    "objectID": "lab_1_behavioral-ecology.html",
    "href": "lab_1_behavioral-ecology.html",
    "title": "Behavioral & Urban Ecology",
    "section": "",
    "text": "Fun Squirrel Information:\n\nhttps://sc.edu/uofsc/posts/2021/10/remembering_the_days_natural_history_stroll_ep_35.php\nhttps://www.instagram.com/usc.squirrels/?hl=en\nhttps://www.facebook.com/watch/?v=499139211340770\nhttps://www.tiktok.com/@uofsc/video/7191269652230720814 (not available on campus wifi due to tiktok restrictions despite this is university’s account)\n\nIn this lab, we will study movement and behavior of squirrels on the USC campus. The primary objective will be to introduce students to conducting basic ecological fieldwork in a convenient location. However, don’t take this lab for granted! There are plenty of opportunities to develop an independent project utilizing nearby spaces on campus. Try to be curious of your surroundings!\nStudying organism behavior may seem fairly straightforward. However, it can be extremely challenging to do so with robust scientific methods. A basic tool in behavioral ecology research is to use an ethogram. This is a table which clearly defines possible actions to observe in an individual’s behavior.\nIn this lab we will construct ethograms to test a hypothesis about squirrel activity.\nDownload the full project plan here\nDownload the worksheet here"
  },
  {
    "objectID": "lab_2_forest-biodiversity.html",
    "href": "lab_2_forest-biodiversity.html",
    "title": "Forest Biodiversity",
    "section": "",
    "text": "Download the worksheet here\n\nLab Context\nFor this lab, we will be evaluating how biodiversity varies based on different disturbance regimes. A classic concept in ecology is the intermediate disturbance hypothesis. This concept was formulated by Connell in 1978. The hypothesis is an extension of the concept of ecological succession. Succession theory suggests that communities, in a new environment, will develop following a predictable pattern in which the first organisms are colonizing ones (fast-life histories, r-selected individuals). These taxa are quick to reproduce and establish a population. Then, as resources develop in the environment, longer living yet better competing individuals will move into the area. Thus the community will slowly become dominated by organisms which have slow-life history strategies (K-selected). This concept is a persistent idea in ecology and was developed in coastal dunes [1], freshwater streams [2], and carrion studies [3]. Yet is has been extended into many other areas of ecology, including plankton (see Margalef’s Mandala).\nSo, how does this lead to the intermediate disturbance hypothesis? Well, what “resets” an environment is disturbances (wildfiles, storms, waves, etc.). So the intermediate disturbance hypothesis posits that if a disturbances are too frequent, then only early succesional species will be present. However, if undisturbed, the community will move towards a late-stage succesional community equilibrium. If there are “intermediate” levels of disturbance. Then we might expect higher levels of biodiversity as it sustains a mixture of community types.\n\nIn this lab we will test the intermediate disturbance hypothesis by sampling thre regions in Harbison forest which correspond to different levels of disturbance. We’ll have a low-disturbance area which has been essentially undisturbed. Then we’ll have a high disturbance region which is consistently mowed. Finally, we’ll have a mid-disturbance area which was subjected to a burn 1 year ago.\n\n\nMeasuring biodiversity\nThere are several metrics to evaluate biodiversity. In this lab we will calculate a few of the most common ones. For your worksheets you will only be required to select one of them.\nA straightforward metric is Species Richness (\\(R\\)). This is the number of unique species in a region. \\[\nR = \\# of\\_unique\\_taxa\n\\] However, this is not the most informative metric as it does not account for the number of taxa there are of each species. For example, if there are 100 different species, yet 99% of them were one type, the area wouldn’t be very biodiverse. Species richness alone is not very informative in such cases.\nAlternatively species diversity can be measured through a variety of indices. Arguable the most common is the Shannon-Wiener Index (\\(H\\)). This ranges from 0 (a low diversity) to larger numbers. The larger the \\(H\\), the more diverse the community is.\n\\[\nH = -\\sum_{i = 1}^{R}{p_i lnp_i}\n\\] Here, for each unique taxa, we add up the proportion (\\(p\\)) of each (\\(i^{th}\\)) taxa, multiplied by the natural log of that taxa. We do that for all unique taxa (\\(R\\)) For count data, we can extend the formula:\n\\[\nH = -\\sum_{i = 1}^{R}{\\frac{n_i}{N}ln\\frac{n_i}{N}}\n\\]\nHere, \\(n_i\\) is the count of the specific taxa while \\(N\\) is the total count of taxa.\nWhile \\(H\\) provides a nice measure of biodiversiy, it is still related to the overall richness. An alternative metric is to measure evenness. This will report the how spread across the different taxa the relative abundace is. For example, if we had 5 individuals each of two taxa, it would be more “even” than 1 of taxa A and 9 of taxa B. This metric is independent of the overall richness and give a perspective to the relative success of the taxa which are present. The more common metric for evenness is Pileou’s (\\(D\\)):\n\\[\nD = 1 - \\sum_{i=1}^{R}{\\frac{n_i(n_i-1)}{N(N - 1)}}\n\\]\n\n\nQuantitative Sampling\nA large challenge for ecologists who conduct field studies is how to collect quantitative, unbiased estimates of population abundance, community structure, or other ecologically relevant metrics. Here we want to collect biodiversity metrics in four forest regions.\nThere are several common field sampling techniques:\nSome basic tools:\n\nQuadrat (basically a square, typically of PVC)\n\nThese are useful for defining a set sampling area. They give us the ability to quantify density (count per area) which then standardizes our metrics across regions\n\nTransect\n\nA fixed line which we go and sample along. Sampling along a transect can be done with point counts (whatever is touching the transect), line-quadrat methods (placing quadrats systematically or randomly along the transect), or band-transect methods (counting everything within a fixed width of the transect).\n\n\nWhen deploying these tools, there are several ways we can try to reduce bias. We want to have a fixed way to sample. Some common methods are:\n\nHaphazard sampling\n\nThis is just sampling when we encounter our study target. This is inherently biased although it is sometimes necessary when we are interested in highly mobile or elusive organisms. It also could be that we are interested in particular taxa so we just observe those. This is what we did for our Behavioral Ecology Lab last week\n\nSystematic Sampling\n\nHere, samples are collected at a fixed interval (say every 2m along a transect or in set regions of a grid). This allows us to sub-sample a region. This can be particularly useful for measuring change along a gradient.\n\nRandom Sampling\n\nHere, random coordinates are selected in a study area (grid or transect). This ensures a non-bias sample of our study region. However, we should be cautious if there is confounding variables in the area. This is what we will do this week for the intermediate disturbance hypothesis lab.\n\nStratified-Random Sampling\n\nIf we are sampling an area with known variation (differences in moisture, elevation, etc), we can create strata. These are set sub-regions to sample. Here we can then generate random coordinates within each strata for a stratified, but still random approach.\n\n\nWhen we construct our sampling design some key considerations are: How can we be unbias our samples? What are we measuring? If it is just the overall area, random sampling may be sufficient. However if we believe there is variation within our study area we might want to use a transect with systematic sampling, or a stratified random design. These are all things you’ll want to consider when designing your sampling schemes.\n\n\nHarbison Forest Directions\nGo to the main entrance to Harbison Forest off broad river road. Note that some people turn early and go to the South Carolina Forestry Commission. Drive forward (past the first parking lot on the left) and keep left towards the gate (marked in blue). Drive straight down the gravel road. Shortly after the intersection for the Eagle Trail lot (you’ll see signs about a gazeebo, just go straight), you’ll see a parking lot on the left (marked by yellow X).\n\nIf you have problems getting there, you can call my cell 440-668-8376.\nDress for being outside! It will be warm but there are a lot of bugs and sharp plants. I always opt for covering up rather than being cool but it’s a personal trade-off. Also wear appropriate shoes!"
  },
  {
    "objectID": "proj_main.html",
    "href": "proj_main.html",
    "title": "Independent Projects",
    "section": "",
    "text": "General Guidelines:\n\nStudents may work in groups up to 3 members or individually\nProjects should be feasible, clearly organized, and interesting. A common pitfall is to create a project that is too complicated and unable to complete in the limited time of lab. A good experiment should be elegant!\nProjects will take the bulk of the semester, however there will be several milestones along the way to ensure progress.\nThe projects will incorporate ecological concepts learned throughout the semester.\n\n\n\nPhase 1: Idea generation\nThe first few weeks of lab will involve individual one-week labs. These will introduce students to field work and working in nearby ecosystems. During these labs students should take some time to think about the lab topic and their surroundings. Ask questions about the ecosystem and think of possible projects on these activities.\nAccess the project plan guide here\nAccess the project plan rubric here"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Course Website:\nAll course documents will be shared via the course website: https://USC-Ecology-Lab.github.io\n\n\nCourse Description:\nThis is a 1 credit lab based course for ecological investigation. Students will be trained in basic field techniques and analyses common in ecology. A core portion of this course will be an independent investigation on an ecological topic selected by students.\n\n\nLearning Outcomes:\n\nUnderstand fundamental ecology sampling schemes\nUtilize appropriate statistical tools for analyzing ecological data\nAbility to design quality investigations to test hypotheses\nGain experience with communicating scientific results to peers\n\n\n\nCourse Technology:\nLab activities will included data analysis in the R programming language. While this course will rely heavily on R, prior knowledge is not expected. Students should have access to both R and Rstudio. See here for download information. Data, documents, and code for this course will all be hosted on github. Git and GitHub will not be taught yet it is a great tool for any scientist to be familiar with. Interested people should look more here for a friendly introduction.\nFor independent projects, students are welcome to use whichever software they are most productive in. However R is encouraged.\n\n\nOutdoor activity & safety policy:\nEcology research often requires outdoor data collections in variable conditions and environments. This course is no exception. There will be multiple field trips to nearby areas. Students are expected to consult the pre-lab content and arrive prepared for weather and lab conditions. It is expected that students, to the best of their ability, will engage in data collection during labs. However, please contact be if you feel uncomfortable or unsafe with any lab activity.\n\n\nTransportation:\nFor labs which are not initially meeting at the lab space (Week 2,3,4). You can either meet at the lab space, or at the specified location. Meeting time will vary based on location. Transportation will be available to any students who don’t have access. Carpooling is encouraged.\nAll field trips will end with enough time for students to return to campus by the specified end time of lab.\n\n\nGrades:\nThis is a one credit lab based course. The majority of student grades will be centered on their independent project and lab-based worksheets. There is a total of 1000 points available. Point breakdowns are as follows:\n\nIntroduction Worksheet (10 points) : Prior to the start of lab, students must complete the introduction worksheet.\nLab-based worksheets (50 points each, 200 total): In the first weeks of lab students will complete field labs to test assigned hypotheses. These assignments will be worth 50 points each. Note that there is a tentative 5th worksheet. If we are able to complete the 5th worksheet, then the lowest of the 5 worksheet grades will be dropped. If a lab is cancelled due to weather or other unforeseen circumstance, we will adjust the value of all worksheets to scale to a total of 200 points.\nProject Plan (100 points): Groups/Individual students will propose their independent project following the assigned project plan worksheet. This assignment may be subject to a round of revisions. Early submissions are encouraged.\nData Collection Progress Report (15 points each, 45 total): During the data collection period, students must attend the beginning of lab regardless of their planned collection activities. Students will share with the class their project status and challenges. Active discussion between groups is expected.\nData Analysis Progress Report (15 points each, 45 total): During the data analysis period, students must attend the beginning of lab regardless of their planned activities. Students will share with the class their project status and challenges. Active discussion between groups is expected.\nFinal Presentation (250 Points): Final independent projects will be shared in a standard 15 minute research presentation format. Group members may allocate presentation responsibilities independently, however all group members are expected to present an equal portion. See rubric for more details.\nFinal Paper (300 Points): Final independent projects will be written in a full paper format. See rubric for grading details.\nAuthor contribution statement (50 Points): Evaluation of group members contributions to the final project. Group members will independently report the activity of all group members and assign points. Severe discrepancies will may warrant point deductions on individual project grades.\n\n\n\nLate work & make-up policy:\nAny assignments turned in late will be subject to a 10% grade penalty starting immediately following the due date. For each additional 24-hour period late, a 10% penalty will be added. Some assignments are ineligible for late submission, including the progress reports, final presentation, and author contribution statement.\n\n\nAttendance Policy:\nAttendance is expected for all lab sections. Any expected lab absences should be discussed prior to the start of that lab period.\n\n\nAcademic Integrity:\nAny cases of plagiarism will result in a minimum failure of assignment and may result in further penalties, including automatic failure of the course.\n\n\nGenerative AI & ChatGPT:\nGenerative AI is an extremely powerful tool for anyone who wishes to be a productive scientist. It can be particularly helpful for organizing and writing code and providing the blueprint for text. However, in its current state, ChatGPT and other generative AI applications do not produce work at the level expected of an upper-division undergraduate. If you want to receive a good grade in the course you should not rely exclusively on ChatGPT. If you are not outworking the machine, you are falling behind. Be skeptical of its responses as it cannot be trusted.\n\n\nCo-enrollment with 570 Lecture:\nThis lab course is a fully independent credit. Co-enrollment in the lecture section of 570 is not required nor expected. While content will overlap and it will be beneficial to take both courses, the content will not be synchronous. Note that the major assignment in the lecture section is the Research Proposal. You are welcome to use your independent project as the basis for that assignment. However note that these are separate assignments with different grading structure.\n\n\nClass Schedule:\n\n\n\n\n\n\n\n\n\n\nWeek (Dates)\nLocation\nLab\nAssignments Given\nAssignments Due\n\n\n\n\n0 (8/29 & 8/31)\nVirtual\nIntroduction & R set-up\nIntro Worksheet\n\n\n\n1 (9/5 & 9/7)\nLab room and near-by campus areas\nIntroduction & Mammal Behavior\nWS1\nProject Plan\nIntro Worksheet\n\n\n2 (9/12 & 9/14)\nHarbison State Forest\nIntermediate Disturbance & Succession\nWS2\nWS1\n\n\n3 (9/19 & 9/21)\nUSC Arboretum\nCommunity Assemblages\nWS3\nWS2\n\n\n4 (9/26 & 9/28)\nVarious Stream Locations\nUrban Aquatic Systems\nWS4\nWS3\n\n\n5 (10/3 & 10/5)\nLab Room\nPhenology Lab\nWS5*\nWS4\nProject Plan\n\n\n6 (10/10 & 10/12)\nLab Room & TBD\nProject Planning and Preliminary data collection\n\nWS5*\nProject Plan Revisions (If applicable)\n\n\n7 (10/17 & 10/19)\nFall Break Modified Class!\nLab Room & TBD\nData Collection\n\nData Collection Progress Report\n\n\n8 (10/24 & 10/26)\nLab Room & TBD\nData Collection\n\nData Collection Progress Report\n\n\n9 (10/31 & 11/2)\nLab Room & TBD\nData Collection\n\nData Collection Progress Report\n\n\n10 (11/7 & 11/9)\nLab Room\nData Analysis\n\nData Analysis Progress Report\n\n\n11 (11/14 & 11/16)\nLab Room\nData Analysis\n\nData Analysis Progress Report\n\n\n12 (11/21 & 11/23)\nThanksgiving Break\nLab Room\nData Analysis\n\nData Analysis Progress Report\n\n\n13 (11/28 & 11/31)\n\nFinal Presentations\n\nFinal Presentation\n\n\n14 (12/5 & 12/7)\n\nCourse Recap\n\nFinal Project Paper\nAuthor Contribution Statement"
  }
]